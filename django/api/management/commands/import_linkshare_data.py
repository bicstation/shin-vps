import os
import re
import ftplib
import time
import gzip
import csv
import traceback
import sys
import subprocess
from datetime import datetime, timezone
from typing import List, Tuple, Dict, Any, Optional, Set
from decimal import Decimal, InvalidOperation
import math 

# Djangoã®ã‚³ã‚¢æ©Ÿèƒ½ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, IntegrityError 
from django.utils import timezone
from django.conf import settings 

# ğŸš¨ ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ãƒŸãƒ¼å®šç¾© (handleå®Ÿè¡Œå‰ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§ã®NameErrorã‚’å›é¿ã™ã‚‹ãŸã‚)
class DummyModel:
    """handleå®Ÿè¡Œå‰ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§ã®NameErrorã‚’å›é¿ã™ã‚‹ãŸã‚ã®ãƒ€ãƒŸãƒ¼å®šç¾©"""
    objects = None
    def __init__(self, **kwargs): pass
    # LinkshareProduct ãŒæŒã¤ã“ã¨ãŒæƒ³å®šã•ã‚Œã‚‹å±æ€§ã‚’è¿½åŠ  (hasattrãƒã‚§ãƒƒã‚¯ç”¨)
    id = None
    merchant_id = None
    created_at = None
    updated_at = None
    link_id = None
    product_name = None
    sku = None
    primary_category = None
    sub_category = None
    product_url = None
    image_url = None
    buy_url = None
    short_description = None
    description = None
    discount_amount = None
    discount_type = None
    sale_price = None
    retail_price = None
    begin_date = None
    end_date = None
    brand_name = None
    shipping = None
    keywords = None
    manufacturer_part_number = None
    manufacturer_name = None
    shipping_information = None
    availability = None
    universal_product_code = None
    class_id = None
    currency = None
    m1 = None
    pixel_url = None
    attribute_1 = None
    attribute_2 = None
    attribute_3 = None
    attribute_4 = None
    attribute_5 = None
    attribute_6 = None
    attribute_7 = None
    attribute_8 = None
    attribute_9 = None
    attribute_10 = None
    # âŒ ä¿®æ­£: api_source ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãƒ¢ãƒ‡ãƒ«ã«ãªã„ãŸã‚å‰Šé™¤
    
LinkshareProduct = DummyModel

# ==============================================================================
# æ¥ç¶šãƒ»ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š (å®šæ•°)
# ==============================================================================
# ç’°å¢ƒå¤‰æ•°ãŒãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
FTP_HOST = os.getenv("LINKSHARE_FTP_HOST", "aftp.linksynergy.com")
FTP_USER = os.getenv("LINKSHARE_BS_USER", "rkp_3750988") 
FTP_PASS = os.getenv("LINKSHARE_BS_PASS", "u5NetPVZEAhABD7HuW2VRymP") 
FTP_PORT = 21
FTP_TIMEOUT = 180

MAX_SIZE_BYTES = 1073741824 # 1 GB ã®ãƒã‚¤ãƒˆå€¤
DOWNLOAD_DIR = "/tmp/ftp_downloads"

# LinkShareãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
FULL_DATA_PATTERN = re.compile(r"(\d+)_3750988_mp\.txt\.gz$")
DELTA_DATA_PATTERN = re.compile(r"(\d+)_3750988_delta\.txt\.gz$")

FIXED_DELIMITER = '|'
FIXED_DELIMITER_NAME = 'PIPE'
FIELD_MAPPING = {
    'C1': {'DB_FIELD': 'link_id', 'TYPE': 'str', 'DESCRIPTION': 'ãƒªãƒ³ã‚¯ID'},
    # LinkshareProductãƒ¢ãƒ‡ãƒ«ã« manufacturer_name ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ä¸€ã¤
    'C2': {'DB_FIELD': 'product_name', 'TYPE': 'str', 'DESCRIPTION': 'å•†å“å (æ—§ãƒ¡ãƒ¼ã‚«ãƒ¼å)'},
    'C3': {'DB_FIELD': 'sku', 'TYPE': 'str', 'PK': True, 'DESCRIPTION': 'å•†å“ã‚³ãƒ¼ãƒ‰ (SKU)'},
    'C4': {'DB_FIELD': 'sub_category', 'TYPE': 'str', 'DESCRIPTION': 'ã‚«ãƒ†ã‚´ãƒª2 (æ—§å•†å“å)'},
    'C5': {'DB_FIELD': 'primary_category', 'TYPE': 'str', 'DESCRIPTION': 'ã‚«ãƒ†ã‚´ãƒª1'},
    
    # ğŸš¨ ä¿®æ­£ã‚¾ãƒ¼ãƒ³ 1: URLã¨èª¬æ˜æ–‡ã®ã‚ºãƒ¬ã‚’è§£æ¶ˆ
    'C6': {'DB_FIELD': 'buy_url', 'TYPE': 'str', 'DESCRIPTION': 'è³¼å…¥URL (æ—§ã‚«ãƒ†ã‚´ãƒª2)'}, 
    'C7': {'DB_FIELD': 'image_url', 'TYPE': 'str', 'DESCRIPTION': 'ç”»åƒURL (æ—§å•†å“URL)'}, 
    'C8': {'DB_FIELD': 'product_url', 'TYPE': 'str', 'DESCRIPTION': 'å•†å“URL (æ—§ç”»åƒURL)'},
    'C9': {'DB_FIELD': 'short_description', 'TYPE': 'str', 'DESCRIPTION': 'çŸ­ã„å•†å“èª¬æ˜ (æ—§è³¼å…¥URL)'},
    'C10': {'DB_FIELD': 'description', 'TYPE': 'str', 'DESCRIPTION': 'è©³ç´°ãªå•†å“èª¬æ˜ (æ—§çŸ­ã„å•†å“èª¬æ˜)'},
    'C11': {'DB_FIELD': 'discount_amount', 'TYPE': 'str', 'DESCRIPTION': 'å€¤å¼•é‡‘é¡/ç‡'},
    
    # ğŸš¨ ä¿®æ­£ã‚¾ãƒ¼ãƒ³ 2: ä¾¡æ ¼ã¨å‰²å¼•æƒ…å ±ã®ã‚ºãƒ¬ã‚’è§£æ¶ˆ (C13ã¨C15ã«æ•°å€¤ãŒå…¥ã£ã¦ã„ãŸãŸã‚)
    'C12': {'DB_FIELD': 'discount_type', 'TYPE': 'str', 'DESCRIPTION': 'å‰²å¼•ã‚¿ã‚¤ãƒ— (æ—§å‰²å¼•é¡)'}, 
    'C13': {'DB_FIELD': 'retail_price', 'TYPE': 'Decimal', 'DESCRIPTION': 'å®šä¾¡ (æ—§å‰²å¼•ã‚¿ã‚¤ãƒ—)'}, 
    'C14': {'DB_FIELD': 'sale_price', 'TYPE': 'Decimal', 'DESCRIPTION': 'è²©å£²ä¾¡æ ¼ (æ—§è²©å£²ä¾¡æ ¼, ã‚ºãƒ¬ä¿®æ­£)'},
    # 'C15': {'DB_FIELD': 'discount_amount', 'TYPE': 'Decimal', 'DESCRIPTION': 'å‰²å¼•é¡ (æ—§å®šä¾¡)'},
    
    'C15': {'DB_FIELD': 'begin_date', 'TYPE': 'datetime', 'DESCRIPTION': 'è²©å£²é–‹å§‹æ—¥'},
    'C16': {'DB_FIELD': 'brand_name', 'TYPE': 'datetime', 'DESCRIPTION': 'ãƒ–ãƒ©ãƒ³ãƒ‰å'},
    'C17': {'DB_FIELD': 'brand_name', 'TYPE': 'str', 'DESCRIPTION': 'ãƒ–ãƒ©ãƒ³ãƒ‰å'},
    'C18': {'DB_FIELD': 'shipping', 'TYPE': 'Decimal', 'DESCRIPTION': 'é€æ–™'},
    'C19': {'DB_FIELD': 'keywords', 'TYPE': 'str', 'DESCRIPTION': 'æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'},
    'C20': {'DB_FIELD': 'manufacturer_part_number', 'TYPE': 'str', 'DESCRIPTION': 'è£½é€ å“ç•ª'}, 
    # C22: manufacturer_name (C2ã‚ˆã‚Šå„ªå…ˆ)
    'C21': {'DB_FIELD': 'manufacturer_name', 'TYPE': 'str', 'PRIMARY': True, 'DESCRIPTION': 'è£½é€ ãƒ¡ãƒ¼ã‚«ãƒ¼å'},
    'C22': {'DB_FIELD': 'shipping_information', 'TYPE': 'str', 'DESCRIPTION': 'é…é€è¿½åŠ æƒ…å ±'},
    'C23': {'DB_FIELD': 'availability', 'TYPE': 'str', 'DESCRIPTION': 'åœ¨åº«æƒ…å ±'},
    # ğŸš¨ ä¿®æ­£: C25ã‚’ dual purpose ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«
    'C24': {'DB_FIELD': 'common_product_code_dual', 'TYPE': 'str', 'DESCRIPTION': 'JAN/UPC (universal_product_code ã¨ class_id ã®ä¸¡æ–¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°)'}, 
    'C25': {'DB_FIELD': 'class_id', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§ã‚³ãƒ¼ãƒ‰'}, 
    'C26': {'DB_FIELD': 'currency', 'TYPE': 'str', 'DESCRIPTION': 'é€šè²¨å˜ä½ (JPY, USD, etc.)'}, 
    'C27': {'DB_FIELD': 'm1', 'TYPE': 'str', 'DESCRIPTION': 'M1 ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (ã‚«ã‚¹ã‚¿ãƒ å±æ€§)'},
    'C28': {'DB_FIELD': 'pixel_url', 'TYPE': 'str', 'DESCRIPTION': 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³è¨ˆæ¸¬ URL'},
    'C29': {'DB_FIELD': 'attribute_1', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§1 (æ—§M2)'}, 
    'C30': {'DB_FIELD': 'attribute_2', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§2 (æ—§M3)'},
    'C31': {'DB_FIELD': 'attribute_3', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§3 (æ—§M4)'},
    'C32': {'DB_FIELD': 'attribute_4', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§4 (æ—§M5)'},
    'C33': {'DB_FIELD': 'attribute_5', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§5 (æ—§M6)'},
    'C34': {'DB_FIELD': 'attribute_6', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§6 (æ—§M7)'},
    'C35': {'DB_FIELD': 'attribute_7', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§7 (æ—§M8)'},
    'C36': {'DB_FIELD': 'attribute_8', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§8 (æ—§M9)'},
    'C37': {'DB_FIELD': 'attribute_9', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§9 (æ—§M10)'},
    'C38': {'DB_FIELD': 'attribute_10', 'TYPE': 'str', 'DESCRIPTION': 'è¿½åŠ å±æ€§10 (æ—§M11)'},
}
EXPECTED_COLUMNS_COUNT = 38
DATE_FORMAT = '%Y%m%d %H:%M:%S'
DATE_REGEX = re.compile(r'^\d{8} \d{2}:\d{2}:\d{2}$')


# ==============================================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤
# ==============================================================================
def human_readable_size(size_bytes: int) -> str:
    """ãƒã‚¤ãƒˆæ•°ã‚’KB, MB, GBãªã©ã«å¤‰æ›ã—ã¦å¯èª­æ€§ã®é«˜ã„æ–‡å­—åˆ—ã‚’è¿”ã™"""
    size_name = ("B", "KB", "MB", "GB", "TB")
    if size_bytes == 0:
        return "0B"
    try:
        i = int(math.floor(math.log(size_bytes, 1024)))
        p = pow(1024, i) 
        s = round(size_bytes / p, 2)
        
        if i >= len(size_name):
            return f"{size_bytes} B"
            
        return f"{s:,.2f} {size_name[i]}"
    except ValueError:
        return f"{size_bytes} B"
    except Exception:
        return f"{size_bytes} B"

def _get_ftp_client() -> Optional[ftplib.FTP]:
    """FTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¥ç¶šãƒ»ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†"""
    try:
        ftp_client = ftplib.FTP()
        ftp_client.set_pasv(True) 
        ftp_client.set_debuglevel(0)
        
        ftp_client.connect(FTP_HOST, FTP_PORT, FTP_TIMEOUT)
        ftp_client.login(FTP_USER, FTP_PASS)
        
        return ftp_client
        
    except ftplib.all_errors as e:
        print(f"âŒ [ERROR] FTPæ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—: {e}", file=sys.stderr)
        return None
        
    except Exception as e:
        print(f"âŒ [ERROR] FTPæ¥ç¶šå‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return None

def get_ftp_mid_list(ftp_client: ftplib.FTP) -> List[Tuple[str, str, str, Optional[datetime], int]]:
    """
    FTPã‹ã‚‰å¯¾è±¡ã®ãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹ã€‚
    MLSDã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ï¼ˆã‚µã‚¤ã‚ºã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰ã‚’å–å¾—ã™ã‚‹ã€‚
    
    æˆ»ã‚Šå€¤ã®ã‚¿ãƒ—ãƒ«å½¢å¼: (mid, filename, file_type, mtime_dt, file_size)
    """
    file_list = []
    
    try:
        print("ğŸ“¡ [FTP] ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ (MLSD) ã‚’å–å¾—ä¸­...", file=sys.stdout, flush=True)
        # MLSDã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        for filename, facts in ftp_client.mlsd():
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ãŒ 'file' ã§ãªã„ã€ã¾ãŸã¯ã‚µã‚¤ã‚ºãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if facts.get('type') != 'file' or 'size' not in facts:
                continue
                
            is_full_data = FULL_DATA_PATTERN.match(filename)
            is_delta_data = DELTA_DATA_PATTERN.match(filename)
            
            if is_full_data or is_delta_data:
                mid = is_full_data.group(1) if is_full_data else is_delta_data.group(1)
                file_type = 'full' if is_full_data else 'delta'
                
                # ã‚µã‚¤ã‚ºã¨æ›´æ–°æ™‚åˆ»ã‚’å–å¾—
                try:
                    file_size = int(facts.get('size', 0))
                except ValueError:
                    file_size = 0
                
                mtime_str = facts.get('modify')
                mtime_dt = None
                if mtime_str:
                    try:
                        # MLSD timestamp format: YYYYMMDDhhmmss
                        mtime_dt = datetime.strptime(mtime_str, '%Y%m%d%H%M%S').replace(tzinfo=timezone.utc)
                    except ValueError:
                        pass
                        
                file_list.append((mid, filename, file_type, mtime_dt, file_size))
                
        print(f"âœ… [FTP] {len(file_list)} ä»¶ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
                
    except ftplib.all_errors as e:
        print(f"âŒ [ERROR] FTPãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {e}", file=sys.stderr)
        return []
        
    return file_list

def safe_cast(value: str, target_type: str, field_name: str) -> Optional[Any]:
    """LinkShareã®CSVå€¤ã‚’ã”å¸Œæœ›ã®å‹ã«å®‰å…¨ã«å¤‰æ›ã™ã‚‹ã€‚"""
    if not value or value.strip().lower() in ('null', 'none', 'n/a', ''):
        return None
        
    stripped_value = value.strip()
    
    if target_type == 'Decimal':
        try:
            temp_value = stripped_value.replace(',', '').replace('$', '').replace('Â¥', '')
            if not temp_value:
                return None
            return Decimal(temp_value)
        except InvalidOperation:
            return None
            
    elif target_type == 'datetime':
        if not DATE_REGEX.match(stripped_value):
            return None
        try:
            dt = datetime.strptime(stripped_value, DATE_FORMAT)
            return dt.replace(tzinfo=timezone.utc)
        except ValueError:
            return None
            
    return stripped_value


# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯
# ==============================================================================

def _display_mapping_for_first_row(row_list: List[str]):
    """æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ‘ãƒ¼ã‚¹çµæœã‚’è¡¨ç¤ºã—ã€ã‚«ãƒ©ãƒ ã‚ºãƒ¬ã®ç¢ºèªã‚’åŠ©ã‘ã‚‹ã€‚ï¼ˆãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼‰"""
    print("\n--- æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ‘ãƒ¼ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚° (ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: LinkShare Column -> DB Field -> Raw Value) ---", file=sys.stdout, flush=True)
    print(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(row_list)} / æœŸå¾…å€¤: {EXPECTED_COLUMNS_COUNT}", file=sys.stdout, flush=True)
    
    print(f"{'LS-COL':<7} | {'DB FIELD':<35} | {'RAW VALUE (å…ˆé ­50æ–‡å­—)':<50}", file=sys.stdout, flush=True)
    print("-" * 98, file=sys.stdout, flush=True)
    
    for i in range(EXPECTED_COLUMNS_COUNT):
        col_name = f'C{i+1}'
        mapping = FIELD_MAPPING.get(col_name, {'DB_FIELD': 'N/A', 'TYPE': 'str'})
        db_field = mapping.get('DB_FIELD', 'N/A')
        
        # manufacturer_name_fallback ã¯ DB ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã¯ãªã„ãŸã‚ã€å®Ÿéš›ã® manufacturer_name ã‚’è¡¨ç¤º
        if db_field == 'manufacturer_name_fallback':
            db_field = 'manufacturer_name (fallback)'
        elif db_field == 'common_product_code_dual': # ğŸ’¡ ä¿®æ­£: C25 ã®è¡¨ç¤ºã‚’æ˜ç¢ºã«
            db_field = 'universal_product_code & class_id'
        
        raw_value = row_list[i] if i < len(row_list) else ""
        
        display_value = raw_value.replace('\n', '\\n').replace('\r', '\\r')
        if len(display_value) > 50:
            display_value = display_value[:47] + "..."
            
        print(f"{col_name:<7} | {db_field:<35} | '{display_value}'", file=sys.stdout, flush=True)
    print("--------------------------------------------------------------------------------------------------", file=sys.stdout, flush=True)


def _parse_single_row(row_list: List[str], mid: str, advertiser_name: str) -> Optional[Dict[str, Any]]:
    """å˜ä¸€è¡Œã®CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€DBä¿å­˜ç”¨ã®è¾æ›¸å½¢å¼ã«å¤‰æ›ã™ã‚‹ã€‚"""
    if len(row_list) != EXPECTED_COLUMNS_COUNT:
        return None 

    data: Dict[str, Any] = {
        'merchant_id': mid, 
        'created_at': timezone.now(), 
        'updated_at': timezone.now(),
        # âŒ ä¿®æ­£: api_source ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ˜ç¤ºçš„ã«è¨­å®šã™ã‚‹è¡Œã‚’å‰Šé™¤
    }
    
    # manufacturer_name ã®åˆæœŸå€¤ã¯ C22 (PRIMARY) ãŒè¨­å®šã•ã‚Œã€C2 (FALLBACK) ã¯ãã®å¾Œã«è¨­å®šã•ã‚Œã‚‹

    for i, (col_name, mapping) in enumerate(FIELD_MAPPING.items()):
        raw_value = row_list[i]
        db_field = mapping.get('DB_FIELD')
        data_type = mapping.get('TYPE')

        if not db_field:
            continue
        
        # manufacturer_name ã®ç‰¹æ®Šå‡¦ç†
        if db_field == 'manufacturer_name' and 'PRIMARY' in mapping:
            # C22 (PRIMARY) ã®å€¤ãŒå­˜åœ¨ã™ã‚Œã°ãã‚Œã‚’æ¡ç”¨
            if raw_value.strip():
                data['manufacturer_name'] = raw_value.strip()
                continue
            
        # manufacturer_name_fallback ã®ç‰¹æ®Šå‡¦ç†
        elif db_field == 'manufacturer_name_fallback' and 'FALLBACK' in mapping:
            # C2 (FALLBACK) ã®å€¤ãŒã‚ã‚Šã€ã‹ã¤ PRIMARY (C22) ã®å€¤ãŒã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿æ¡ç”¨
            if raw_value.strip() and 'manufacturer_name' not in data:
                data['manufacturer_name'] = raw_value.strip()
                continue
            
        # ğŸ’¡ ä¿®æ­£: C25 ã®äºŒé‡ãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç† (universal_product_code ã¨ class_id)
        elif db_field == 'common_product_code_dual':
            casted_value = safe_cast(raw_value, data_type, db_field)
            if casted_value:
                data['universal_product_code'] = casted_value
                data['class_id'] = casted_value
            continue
            
        # é€šå¸¸ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å‡¦ç†
        elif db_field and db_field != 'manufacturer_name_fallback':
            data[db_field] = safe_cast(raw_value, data_type, db_field)

    return data


def _bulk_import_products(mid: str, product_data_list: List[Dict[str, Any]]) -> Tuple[int, int, int]:
    """
    åé›†ã•ã‚ŒãŸå•†å“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ«ã‚¯ã§DBã«ä¿å­˜/æ›´æ–°ã™ã‚‹ã€‚
    LinkshareProductãƒ¢ãƒ‡ãƒ«ã®ã¿ã«ä¿å­˜å‡¦ç†ã‚’è¡Œã†ã€‚
    """
    if not product_data_list or LinkshareProduct == DummyModel:
        # ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ãƒŸãƒ¼ã®å ´åˆã€å‡¦ç†ä»¶æ•°ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ã™ã‚‹ã®ã¿
        print(f"âš ï¸ [BULK] LinkshareProductãƒ¢ãƒ‡ãƒ«æœªå®šç¾©ã®ãŸã‚ã€DBä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚å‡¦ç†ä»¶æ•°: {len(product_data_list)}", file=sys.stderr, flush=True)
        return len(product_data_list), len(product_data_list), 0 

    incoming_sku_map = {data['sku']: data for data in product_data_list if data.get('sku')}
    skus_to_check = list(incoming_sku_map.keys())
    
    to_create_linkshare: List[LinkshareProduct] = []
    to_update_linkshare: List[LinkshareProduct] = []

    # 1. LinkshareProduct ã® Upsert æº–å‚™
    existing_products = LinkshareProduct.objects.filter(
        merchant_id=mid,
        sku__in=skus_to_check
    )
    
    # merchant_id ã¨ sku ã®è¤‡åˆãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã§æ¤œç´¢
    existing_sku_map = {p.sku: p for p in existing_products}
    
    # ğŸš¨ ä¿®æ­£: update_fields ãƒªã‚¹ãƒˆã‹ã‚‰ 'api_source' ã‚’å‰Šé™¤
    update_fields = [
        'link_id', 'product_name', 'primary_category', 'sub_category',
        'product_url', 'image_url', 'buy_url', 'short_description', 'description', 
        'discount_amount', 'discount_type', 'sale_price', 'retail_price', 'begin_date', 
        'end_date', 'brand_name', 'shipping', 'keywords', 
        'manufacturer_part_number', 'manufacturer_name',
        'shipping_information', 'availability', 
        'universal_product_code', 'class_id', 
        'currency', 'm1', 'pixel_url',
        'attribute_1', 'attribute_2', 'attribute_3', 'attribute_4', 'attribute_5', 
        'attribute_6', 'attribute_7', 'attribute_8', 'attribute_9', 'attribute_10',
        'updated_at' 
    ]

    for sku, data in incoming_sku_map.items():
        # LinkshareProductãŒæŒã¤å±æ€§ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        # ğŸš¨ manufacturer_name_fallback ã‚„ common_product_code_dual ã¯ãƒ¢ãƒ‡ãƒ«ã«ãªã„ã®ã§ã€é™¤å¤–
        clean_data = {
            k: v for k, v in data.items() 
            if hasattr(LinkshareProduct, k) and k not in ('manufacturer_name_fallback', 'common_product_code_dual')
        }
        clean_data['updated_at'] = timezone.now() 
        
        if sku in existing_sku_map:
            product_instance = existing_sku_map[sku]
            is_updated = False
            
            for key in update_fields:
                # clean_data ã«ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ›´æ–°ã®å¯¾è±¡å¤–
                if key not in clean_data:
                    continue
                
                current_value = getattr(product_instance, key)
                new_value = clean_data[key]
                
                # Decimalå‹ã¾ãŸã¯ datetimeå‹ã®æ¯”è¼ƒã§ã€å€¤ãŒç•°ãªã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                is_diff = True
                if current_value == new_value:
                    is_diff = False
                elif isinstance(current_value, Decimal) and isinstance(new_value, Decimal):
                     # Decimalå‹ã®æ¯”è¼ƒ: Decimal('0') ã¨ None ãŒç•°ãªã‚‹ã¨åˆ¤å®šã•ã‚Œãªã„ã‚ˆã†ã«
                     if current_value.compare(new_value) == 0:
                         is_diff = False
                elif current_value is None and new_value == '': # ç©ºæ–‡å­—åˆ—ã¨Noneã®åŒºåˆ¥ã‚’ç„¡ãã™
                     is_diff = False
                elif new_value is None and current_value == '':
                     is_diff = False
                     
                if is_diff:
                    setattr(product_instance, key, new_value)
                    is_updated = True
            
            if is_updated:
                to_update_linkshare.append(product_instance)
        else:
            to_create_linkshare.append(LinkshareProduct(**clean_data))
    
    updated_count = 0
    if to_update_linkshare:
        try:
            # `update_fields` ã«ãªã„ `created_at` ã‚„ `merchant_id`, `sku` ã¯æ›´æ–°ã•ã‚Œãªã„
            LinkshareProduct.objects.bulk_update(to_update_linkshare, update_fields, batch_size=5000)
            updated_count = len(to_update_linkshare)
        except Exception as e:
            print(f" âŒ [MID: {mid}] ãƒãƒ«ã‚¯æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
            
    created_count = 0
    if to_create_linkshare:
        try:
            LinkshareProduct.objects.bulk_create(to_create_linkshare, batch_size=5000)
            created_count = len(to_create_linkshare)
        except IntegrityError as e:
            # è¤‡åˆãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ (merchant_id, sku) ã®é‡è¤‡ãŒç™ºç”Ÿã—ãŸå ´åˆ
            print(f" âŒ [MID: {mid}] ãƒãƒ«ã‚¯ä½œæˆä¸­ã«IntegrityErrorãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
            
    return created_count + updated_count, created_count, updated_count


# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜ã‚’çµ±åˆã—ãŸãƒ¡ã‚¤ãƒ³å‡¦ç† 
# ==============================================================================

def parse_and_process_file(local_path: str, mid: str) -> Tuple[bool, int]:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€DBã«ä¿å­˜ã™ã‚‹ã€‚"""
    
    current_batch: List[Dict[str, Any]] = []
    parsed_count = 0
    total_saved_rows = 0
    advertiser_name: str = 'N/A'
    first_row_logged = False
    
    delimiter = FIXED_DELIMITER 
    
    try:
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ã«UTF-8ã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€'r' (UTF-8) ã§èª­ã¿è¾¼ã‚€
        with open(local_path, 'r', encoding='utf-8', newline='') as f:
            reader = csv.reader(f, delimiter=delimiter)
            
            # 1. HDRè¡Œã®å‡¦ç† (1è¡Œç›®)
            try:
                hdr_row = next(reader)
                if hdr_row[0].strip() == 'HDR':
                    advertiser_name = hdr_row[2].strip() if len(hdr_row) > 2 else 'N/A'
                    print(f"ğŸ’¡ [MID: {mid}] Advertiser Nameã‚’å–å¾—: '{advertiser_name}'ã€‚æ¬¡ã®è¡Œã‹ã‚‰ãƒ‡ãƒ¼ã‚¿é–‹å§‹ï¼ˆã‚«ãƒ©ãƒ åç„¡ã—ï¼‰ã€‚", file=sys.stdout, flush=True)
                else:
                    print(f"âš ï¸ [MID: {mid}] HDRè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚", file=sys.stderr, flush=True)
                    f.seek(0) # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                    reader = csv.reader(f, delimiter=delimiter)

            except StopIteration:
                print(f"âŒ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚", file=sys.stderr, flush=True)
                return False, 0
                
            # 2. ãƒ‡ãƒ¼ã‚¿è¡Œã®å‡¦ç†
            for row in reader:
                if len(row) != EXPECTED_COLUMNS_COUNT:
                    continue
                    
                parsed_count += 1

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›¸ãè¾¼ã¿ã¨ã¯åˆ¥ã«ã€ãƒ‘ãƒ¼ã‚¹ä»¶æ•°ã®ã¿ã§é€²æ—ã‚’å‡ºã™
                if parsed_count % 50000 == 0:
                    print(f"ğŸ”„ [MID: {mid}] **ç¾åœ¨ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ {parsed_count:,} ä»¶**ã€‚æ¬¡ã®DBæ›¸ãè¾¼ã¿ãƒãƒƒãƒã‚’å¾…æ©Ÿä¸­...", file=sys.stdout, flush=True)
                
                
                # ğŸš¨ ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ã¿ã€å…¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
                if not first_row_logged:
                    _display_mapping_for_first_row(row)
                    first_row_logged = True
                
                # 3. å˜ä¸€è¡Œã®ãƒ‘ãƒ¼ã‚¹
                record = _parse_single_row(row, mid, advertiser_name)
                
                # LinkshareProductã«ã¯ 'merchant_id' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
                if not record or not record.get('sku') or not record.get('merchant_id'):
                    continue

                current_batch.append(record)
                
                # ãƒãƒƒãƒå‡¦ç†
                if len(current_batch) >= 5000:
                    saved, created, updated = _bulk_import_products(mid, current_batch)
                    total_saved_rows += saved
                    print(f"â³ [MID: {mid}] å‡¦ç†æ¸ˆã¿ {parsed_count:,} ä»¶ã€‚ä¿å­˜: {saved:,} (æ–°è¦:{created:,}, æ›´æ–°:{updated:,})", file=sys.stdout, flush=True)
                    current_batch = []

            # 4. æœ€çµ‚ãƒãƒƒãƒã®å‡¦ç†
            if current_batch:
                saved, created, updated = _bulk_import_products(mid, current_batch)
                total_saved_rows += saved

            print(f"âœ… [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ã‚¹å®Œäº†ã€‚ç·ãƒ‘ãƒ¼ã‚¹ä»¶æ•°: {parsed_count:,} ä»¶", file=sys.stdout, flush=True)
            return True, total_saved_rows

    except Exception as e:
        print(f"âŒ [MID: {mid}] ãƒ‘ãƒ¼ã‚¹å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr, flush=True)
        print(traceback.format_exc(), file=sys.stderr, flush=True)
        return False, 0


def download_file(ftp_client: ftplib.FTP, filename: str, local_path_gz: str, local_path_txt: str, mid: str, file_size: int) -> Tuple[bool, int]:
    """
    FTPã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€gzipã§è§£å‡ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚
    UTF-8ãƒ‡ã‚³ãƒ¼ãƒ‰æ™‚ã«ä¸æ­£ãªãƒã‚¤ãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç„¡è¦–ã™ã‚‹ã‚ˆã†ä¿®æ­£ã€‚
    """
    print(f"ğŸ“¡ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ« {filename} ({human_readable_size(file_size)}) ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...", file=sys.stdout, flush=True)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¢ºèªã«åŸºã¥ãã€UTF-8 (ã‚¨ãƒ©ãƒ¼ç„¡è¦–) ã‚’ä½¿ç”¨ã™ã‚‹
    ENCODING = 'utf-8'
    ERROR_HANDLING = 'ignore' 
    
    try:
        # 1. GZãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with open(local_path_gz, 'wb') as f:
            ftp_client.retrbinary(f'RETR {filename}', f.write)

        print(f"ğŸ“¦ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚è§£å‡ä¸­...", file=sys.stdout, flush=True)
        
        # 2. GZãƒ•ã‚¡ã‚¤ãƒ«ã®è§£å‡ã¨ãƒ‡ã‚³ãƒ¼ãƒ‰ (ã‚¨ãƒ©ãƒ¼ç„¡è¦–)
        decompressed_size = 0
        
        # GZãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§é–‹ãã€ä¸­èº«ã‚’èª­ã¿è¾¼ã‚€
        with gzip.open(local_path_gz, 'rb') as f_in:
            # ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸçµæœã‚’UTF-8ã§ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ›¸ãå‡ºã™
            with open(local_path_txt, 'w', encoding='utf-8', newline='') as f_out:
                
                buffer_size = 1024 * 1024 # 1MB chunk
                while True:
                    chunk = f_in.read(buffer_size)
                    if not chunk:
                        break
                    
                    # ğŸ’¡ ä¿®æ­£: errors='ignore' ã‚’ä½¿ç”¨ã—ã€ä¸æ­£ãªUTF-8ãƒã‚¤ãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç„¡è¦–ã™ã‚‹
                    text_chunk = chunk.decode(ENCODING, errors=ERROR_HANDLING) 
                    f_out.write(text_chunk)
                    
                    # æ›¸ãè¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®— (ç„¡è¦–ã•ã‚ŒãŸãƒã‚¤ãƒˆã¯å«ã¾ã‚Œãªã„)
                    decompressed_size += len(text_chunk)
                        
        print(f"âœ… [MID: {mid}] è§£å‡ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {ENCODING}, ã‚¨ãƒ©ãƒ¼å‡¦ç†: {ERROR_HANDLING})ã€‚TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {human_readable_size(decompressed_size)}", file=sys.stdout, flush=True)
        
        # 3. GZãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        os.remove(local_path_gz)
        
        return True, decompressed_size

    except ftplib.all_errors as e:
        print(f"âŒ [MID: {mid}] FTPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}", file=sys.stderr)
        if os.path.exists(local_path_gz): os.remove(local_path_gz)
        if os.path.exists(local_path_txt): os.remove(local_path_txt)
        return False, 0
    except Exception as e:
        print(f"âŒ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/è§£å‡ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
        if os.path.exists(local_path_gz): os.remove(local_path_gz)
        if os.path.exists(local_path_txt): os.remove(local_path_txt)
        return False, 0


# ==============================================================================
# Django Management Command ã®å®šç¾©
# ==============================================================================
class Command(BaseCommand):
    """LinkShare FTPã‹ã‚‰ãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€DBã«ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹Djangoã‚³ãƒãƒ³ãƒ‰"""
    help = 'LinkShare FTPã‹ã‚‰ãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€LinkshareProductãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚'

    def add_arguments(self, parser):
        parser.add_argument(
            '--mid', 
            type=str, 
            help='å‡¦ç†ã™ã‚‹ç‰¹å®šã®ãƒãƒ¼ãƒãƒ£ãƒ³ãƒˆID (MID) ã‚’æŒ‡å®šã—ã¾ã™ã€‚', 
            default=None
        )
        parser.add_argument(
            '--limit',
            type=int,
            help='å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§æ•°ã€‚ãƒ‡ãƒãƒƒã‚°ã‚„ãƒ†ã‚¹ãƒˆæ™‚ã«ä¾¿åˆ©ã§ã™ã€‚',
            default=None
        )

    def handle(self, *args, **options):
        
        self.stdout.write("--- LinkShare ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰é–‹å§‹ (ãƒãƒ«ã‚¯å‡¦ç†) ---")

        # ğŸš¨ ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã®ç½®ãæ›ãˆ
        try:
            # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã«åˆã‚ã›ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„
            # ä¾‹: from app_name.models import LinkshareProduct as RealLinkshareProduct
            from api.models.linkshare_products import LinkshareProduct as RealLinkshareProduct
            
            globals()['LinkshareProduct'] = RealLinkshareProduct
            self.stdout.write("âœ… ãƒ¢ãƒ‡ãƒ« (LinkshareProduct) ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸã€‚")
            
        except ImportError as e:
            self.stdout.write(self.style.ERROR(f"ğŸš¨ CRITICAL: ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚DBã¸ã®ä¿å­˜ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚"))
            self.stderr.write(self.style.ERROR(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}"))
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        if not os.path.exists(DOWNLOAD_DIR):
            os.makedirs(DOWNLOAD_DIR)
            self.stdout.write(f"ğŸ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {DOWNLOAD_DIR} ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        # FTPæ¥ç¶š
        ftp_client = _get_ftp_client()

        if not ftp_client:
            self.stdout.write(self.style.ERROR("ğŸš¨ FTPæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"))
            return

        total_processed_files = 0
        total_saved_rows = 0
        mid_list: List[Tuple[str, str, str, Optional[datetime], int]] = [] 

        try:
            # 1. FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®å–å¾—
            mid_list = get_ftp_mid_list(ftp_client) 
            
            limit = options['limit']

            if options['mid']:
                mid_list = [item for item in mid_list if item[0] == options['mid']]

            # --limit ã®é©ç”¨
            if limit is not None and limit > 0:
                mid_list = mid_list[:limit]

            if not mid_list:
                self.stdout.write(self.style.WARNING("âŒ å‡¦ç†å¯¾è±¡ã¨ãªã‚‹LinkShareãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"))
                return

            self.stdout.write(f"âœ… {len(mid_list)} ä»¶ã®MIDãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

            # FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®è¡¨ç¤º
            self.stdout.write(self.style.NOTICE("\n--- å‡¦ç†å¯¾è±¡ã®LinkShare FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ---"))
            self.stdout.write(f"{'MID':<6} | {'ãƒ•ã‚¡ã‚¤ãƒ«å':<40} | {'ã‚µã‚¤ã‚º':<10} | æœ€çµ‚æ›´æ–° (UTC)")
            self.stdout.write("-" * 75)
            for mid_id, filename, file_type, mtime_dt, file_size in mid_list:
                size_hr = human_readable_size(file_size)
                mtime_str = mtime_dt.strftime('%Y-%m-%d %H:%M:%S') if mtime_dt else 'N/A'
                self.stdout.write(f"{mid_id:<6} | {filename:<40} | {size_hr:<10} | {mtime_str}")
            self.stdout.write("----------------------------------------------\n")
            
            # --- ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹ ---
            for mid, filename, file_type, mtime_dt, file_size in mid_list:
                self.stdout.write(f"\n--- [MID: {mid}] å‡¦ç†é–‹å§‹ ({filename}) ---")
                
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®æ±ºå®š
                local_gz_path = os.path.join(DOWNLOAD_DIR, filename)
                local_txt_path = local_gz_path.replace('.gz', '.txt')

                # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç† (Atomic: å¤±æ•—æ™‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                with transaction.atomic():
                    success = False
                    current_saved_rows = 0
                    try:
                        # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è§£å‡ (UTF-8, errors='ignore')
                        is_downloaded, downloaded_size = download_file(
                            ftp_client, 
                            filename, 
                            local_gz_path, 
                            local_txt_path, 
                            mid, 
                            file_size
                        )
                        
                        if is_downloaded:
                            # 2. ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜ 
                            success, current_saved_rows = parse_and_process_file(local_txt_path, mid) 
                            
                            # 3. å‡¦ç†æ¸ˆã¿TXTãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                            if os.path.exists(local_txt_path):
                                os.remove(local_txt_path)
                                # ğŸš¨ ä¿®æ­£: Django OutputWrapper.write() ã®ãŸã‚ã€fileã¨flushå¼•æ•°ã‚’å‰Šé™¤
                                self.stdout.write(f"ğŸ§¹ [MID: {mid}] å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(local_txt_path)} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚") 

                        
                    except Exception as e:
                        # å‡¦ç†ä¸­ã®è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã‚’æ•æ‰ã—ã€ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        self.stderr.write(self.style.ERROR(f"\n[MID: {mid}] å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚"))
                        self.stderr.write(self.style.ERROR(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}"))
                        self.stderr.write(traceback.format_exc()) 

                    if success:
                        total_processed_files += 1
                        total_saved_rows += current_saved_rows
                        self.stdout.write(self.style.SUCCESS(f"\n[MID: {mid}] å‡¦ç†å®Œäº†ã€‚DBä¿å­˜ä»¶æ•°: {current_saved_rows:,} ä»¶"))
                    else:
                        self.stdout.write(self.style.ERROR(f"\n[MID: {mid}] å‡¦ç†å¤±æ•— (ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯)ã€‚"))

        finally:
            # FTPæ¥ç¶šã®çµ‚äº†å‡¦ç†
            if ftp_client:
                try:
                    ftp_client.quit()
                    self.stdout.write("\nğŸ“¡ FTPæ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
                except ftplib.all_errors:
                    pass
            
        self.stdout.write(f"\n==================================================================================")
        self.stdout.write(f"--- æœ€çµ‚çµæœ: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰å®Œäº† ---")
        self.stdout.write(f"æ­£å¸¸å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_processed_files} / {len(mid_list)} ä»¶")
        self.stdout.write(self.style.SUCCESS(f"åˆè¨ˆä¿å­˜è¡Œæ•°: {total_saved_rows:,} è¡Œ"))
        self.stdout.write("==================================================================================")