# -*- coding: utf-8 -*-
import re
import json
import time
import urllib.parse
import requests
import os
import argparse
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- è¨­å®š ---
BASE_DIR = "/usr/src/app/scrapers/src"
SAVE_DIR = os.path.join(BASE_DIR, "json")
FINAL_FILE = os.path.join(SAVE_DIR, "mouse_results.json")
OLLAMA_API_URL = "http://ollama-v2:11434/api/generate"
BASE_SEARCH_URL = "https://www.mouse-jp.co.jp/store/goods/search.aspx?search=x&limit=300"

# ãƒ¡ãƒ¢ãƒª6GBã®RTX 3050ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«
REASONING_MODEL = "gemma3:4b" 

os.makedirs(SAVE_DIR, exist_ok=True)

def call_ollama_simple(prompt):
    """ Ollama APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ """
    payload = {
        "model": REASONING_MODEL,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": 0.1, "num_predict": 1000}
    }
    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’120ç§’ã«è¨­å®š
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
        res_json = response.json()
        return res_json.get("response", "").strip()
    except Exception as e:
        print(f"âŒ Ollama Connection Error: {e}")
        return ""

def ask_ai_about_spec_lite(raw_text, web_price=None):
    """ ãƒ†ã‚­ã‚¹ãƒˆã¨ä¾¡æ ¼ãƒ’ãƒ³ãƒˆã‹ã‚‰ã‚¹ãƒšãƒƒã‚¯ã‚’æŠ½å‡ºã™ã‚‹ """
    # ãƒã‚¤ã‚ºå‰Šæ¸›ï¼šè£½å“ç‰¹é•·ã‚ˆã‚Šå‰ã®ä¸»è¦ã‚¹ãƒšãƒƒã‚¯éƒ¨åˆ†ã®ã¿æŠ½å‡º
    clean_text = raw_text.split("è£½å“ç‰¹é•·")[0][:2500]
    
    # ä¾¡æ ¼æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ„ã¿è¾¼ã‚€
    price_hint = f"REFERENCE PRICE: {web_price} JPY" if web_price else "REFERENCE PRICE: Unknown"

    prompt = f"""Extract PC specs from the text below as JSON.
{price_hint}
If information is missing, use null.
Return ONLY JSON.

TEXT:
{clean_text}

JSON TEMPLATE:
{{
  "product_name": "string",
  "price": number,
  "cpu": "string",
  "gpu": "string",
  "ram": "string",
  "storage": "string",
  "npu_exists": boolean
}}
"""
    print(f"--- [DEBUG: Sending Prompt with Price Hint: {web_price}] ---")
    raw_res = call_ollama_simple(prompt)
    print(f"--- [DEBUG: AI RAW RESPONSE] ---\n{raw_res}\n")
    
    try:
        # ```json { ... } ``` ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰ {} éƒ¨åˆ†ã ã‘ã‚’æŠœãå‡ºã™
        match = re.search(r'\{.*\}', raw_res, re.DOTALL)
        if match:
            extracted_json = json.loads(match.group(0))
            # AIãŒä¾¡æ ¼ã‚’æŠœãå‡ºã›ãªã‹ã£ãŸå ´åˆã€WEBã‹ã‚‰å–å¾—ã—ãŸä¾¡æ ¼ã‚’è£œå®Œã™ã‚‹
            if (extracted_json.get("price") is None or extracted_json.get("price") == 0) and web_price:
                extracted_json["price"] = web_price
            return extracted_json
    except Exception as e:
        print(f"âš ï¸ JSON Parse Error: {e}")
    
    return {"error": "parse_failed", "raw": raw_res, "price": web_price}

def fetch_detail_info(page, url):
    try:
        print(f"\nğŸš€ Accessing: {url}")
        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾…ã¡
        page.wait_for_timeout(2000)
        
        content = page.content()
        soup = BeautifulSoup(content, 'html.parser')
        
        # --- 1. ä¾¡æ ¼ã¨ç”»åƒURLã®å–å¾— (æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ JSON-LD) ---
        price_val = 0
        image_url = ""
        json_lds = soup.find_all("script", type="application/ld+json")
        for jld in json_lds:
            try:
                ld_data = json.loads(jld.string)
                if isinstance(ld_data, list): ld_data = ld_data[0]
                
                # ä¾¡æ ¼ã®æŠ½å‡º
                if not price_val:
                    price_val = ld_data.get("offers", {}).get("price", 0)
                
                # ç”»åƒURLã®æŠ½å‡º
                if not image_url:
                    img = ld_data.get("image")
                    image_url = img[0] if isinstance(img, list) else img
            except: continue
        
        # --- 2. ç”»åƒURLã®è£œå®Œ (OGPã‚¿ã‚°) ---
        if not image_url:
            og_img = soup.find("meta", property="og:image")
            if og_img:
                image_url = og_img.get("content")

        # --- 3. ã‚¹ãƒšãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º ---
        spec_element = page.query_selector(".block-goods-detail")
        raw_text = spec_element.inner_text() if spec_element else soup.get_text()
        
        # --- 4. AIè§£æ (ä¾¡æ ¼ãƒ’ãƒ³ãƒˆä»˜ã) ---
        ai_data = ask_ai_about_spec_lite(raw_text, web_price=price_val)
        
        # --- 5. åŸºæœ¬æƒ…å ±ã®è£œå®Œ ---
        name_tag = soup.find("h1")
        web_name = name_tag.get_text(strip=True) if name_tag else "Unknown"
        
        # æœ€çµ‚çš„ãªä¾¡æ ¼ã®ç¢ºå®š
        final_price = ai_data.get("price") or price_val

        return {
            "unique_id": f"mouse_{int(time.time())}_{url.split('/')[-1].replace('.html','')}",
            "name": ai_data.get("product_name") or web_name,
            "price": final_price,
            "image_url": image_url,
            "url": url,
            "ai_extracted_json": ai_data,
            "last_update": time.strftime("%Y-%m-%d %H:%M:%S")
        }
    except Exception as e:
        print(f"âš ï¸ Error during fetch: {e}")
        return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', required=True)
    parser.add_argument('--limit', type=int, default=5)
    args = parser.parse_args()

    results = []
    with sync_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼‰
        browser = p.chromium.launch(headless=True)
        # ã‚µã‚¤ãƒˆã«å¼¾ã‹ã‚Œãªã„ã‚ˆã†User-Agentã‚’è¨­å®š
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        print(f"ğŸ” Listing products from: {BASE_SEARCH_URL}")
        page.goto(BASE_SEARCH_URL)
        page.wait_for_timeout(2000)
        
        # å•†å“è©³ç´°ãƒªãƒ³ã‚¯ã®æŠ½å‡º
        links = page.query_selector_all('a[href*="/store/g/g"]')
        urls = []
        for l in links:
            href = l.get_attribute('href')
            if href:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ã‚¢ãƒ³ã‚«ãƒ¼ï¼ˆ#ï¼‰ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–
                full_url = urllib.parse.urljoin("https://www.mouse-jp.co.jp", href).split('?')[0].split('#')[0]
                urls.append(full_url)
        
        # é‡è¤‡æ’é™¤ã—ã¦ã‚½ãƒ¼ãƒˆ
        target_urls = sorted(list(set(urls)))
        print(f"Found {len(target_urls)} unique products.")

        # æŒ‡å®šä»¶æ•°åˆ†ãƒ«ãƒ¼ãƒ—
        for i, url in enumerate(target_urls[:int(args.limit)]):
            print(f"--- {i+1}/{args.limit} ---")
            data = fetch_detail_info(page, url)
            if data:
                results.append(data)
                # é€”ä¸­ã§æ­¢ã¾ã£ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚‹ã‚ˆã†1ä»¶ã”ã¨ã«ä¿å­˜
                with open(FINAL_FILE, "w", encoding="utf-8") as f:
                    json.dump(results, f, ensure_ascii=False, indent=4)
            
            # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·è»½æ¸›
            time.sleep(1)

        browser.close()
    print(f"\nâœ… All tasks finished. Results saved to {FINAL_FILE}")

if __name__ == "__main__":
    main()