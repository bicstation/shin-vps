import time
import csv
import random
import os
import subprocess
import re
from playwright.sync_api import sync_playwright

def run_docker_import(csv_path):
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†å¾Œã€è‡ªå‹•ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã€
    Djangoã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹
    """
    container_name = "api_django_v2"
    container_csv_path = "/usr/src/app/scrapers/acer_products_final.csv"

    print("\n" + "="*40)
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®è‡ªå‹•åæ˜ ã‚’é–‹å§‹ã—ã¾ã™")
    print("="*40)
    
    try:
        # 1. ã‚³ãƒ³ãƒ†ãƒŠå†…ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã«å‚™ãˆã¦ä½œæˆ
        subprocess.run(["docker", "exec", container_name, "mkdir", "-p", "/usr/src/app/scrapers"], check=True)

        # 2. docker cp ã§ã‚³ãƒ³ãƒ†ãƒŠã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é€ã‚‹
        copy_cmd = ["docker", "cp", csv_path, f"{container_name}:{container_csv_path}"]
        subprocess.run(copy_cmd, check=True)
        print(f"ğŸ“‚ [Step 1/2] CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒ†ãƒŠ({container_name})å†…ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚")

        # 3. Djangoã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
        import_cmd = [
            "docker", "exec", 
            container_name, 
            "python", "manage.py", "import_acer"
        ]
        
        print(f"ğŸš€ [Step 2/2] ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        result = subprocess.run(
            import_cmd, 
            check=True, 
            text=True, 
            capture_output=True,
            encoding='utf-8'
        )
        
        print("-" * 40)
        print(f"ğŸ“‹ Djangoã‹ã‚‰ã®å ±å‘Š:\n{result.stdout.strip()}")
        print("-" * 40)
        print("\nâœ¨ ã™ã¹ã¦ã®å·¥ç¨‹ãŒæ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")

    except subprocess.CalledProcessError as e:
        print(f"âŒ Dockerã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e.stderr or e.stdout}")
    except Exception as e:
        print(f"âš ï¸ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def scrape_acer_to_csv_realtime():
    # ä¿å­˜ãƒ‘ã‚¹ã®è¨­å®š
    base_dir = os.path.dirname(os.path.abspath(__file__))
    output_csv = os.path.join(base_dir, "acer_products_final.csv")
    
    targets = [
        {"category": "laptop", "url": "https://store.acer.com/ja-jp/notebooks?p="},
        {"category": "monitor", "url": "https://store.acer.com/ja-jp/monitors?p="},
        {"category": "desktop", "url": "https://store.acer.com/ja-jp/desktops?p="},
        {"category": "peripheral", "url": "https://store.acer.com/ja-jp/peripheral?p="},
    ]
    
    total_count = 0

    # CSVã®åˆæœŸåŒ–
    with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['category', 'name', 'price', 'url', 'image_url', 'description'])

    with sync_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶ã®èµ·å‹•ï¼ˆã‚¹ãƒ†ãƒ«ã‚¹è¨­å®šï¼‰
        browser = p.chromium.launch(
            headless=True, 
            args=[
                '--disable-http2',
                '--disable-blink-features=AutomationControlled',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--window-position=0,0',
                '--ignore-certificate-errors',
            ]
        )
        
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            locale="ja-JP",
            timezone_id="Asia/Tokyo",
            extra_http_headers={
                "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            }   
        )
        
        page = context.new_page()
        # webdriverãƒ•ãƒ©ã‚°ã‚’éš ã™
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        for target in targets:
            category_name = target["category"]
            base_url = target["url"]
            last_page_data = set()
            page_num = 1
            
            print(f"\nğŸš€ {category_name} ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")

            while True:
                # URLã‚’ã“ã“ã§å®šç¾©ï¼ˆUnboundLocalErrorã‚’é˜²æ­¢ï¼‰
                current_url = f"{base_url}{page_num}"
                print(f"ğŸ“„ [{category_name}] ãƒšãƒ¼ã‚¸ {page_num} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼šdomcontentloadedã§é€²ã‚ã‚‹
                    page.goto(current_url, wait_until="domcontentloaded", timeout=60000)
                    
                    # äººé–“ã‚‰ã—ã„ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿ
                    page.wait_for_timeout(random.uniform(3000, 5000))

                    # å•†å“ãƒªã‚¹ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    try:
                        page.wait_for_selector(".product-item-info", state="attached", timeout=15000)
                    except:
                        print(f"ğŸ {category_name} ã®å…¨ãƒšãƒ¼ã‚¸ã‚’çµ‚äº†ã—ã¾ã—ãŸï¼ˆã¾ãŸã¯å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰ã€‚")
                        break

                    # é…å»¶èª­ã¿è¾¼ã¿å¯¾ç­–ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                    for _ in range(5):
                        page.evaluate("window.scrollBy(0, 800)")
                        time.sleep(0.4)
                    
                    items = page.query_selector_all(".product-item-info")
                    current_page_names = []

                    for item in items:
                        try:
                            name_el = item.query_selector(".product-item-name a")
                            price_el = item.query_selector("span.price")
                            
                            if name_el and price_el:
                                name = name_el.inner_text().strip()
                                item_url = name_el.get_attribute("href") or ""
                                
                                # ç”»åƒå–å¾—ï¼ˆsrcã¾ãŸã¯data-srcï¼‰
                                img_el = item.query_selector(".product-image-photo")
                                image_url = ""
                                if img_el:
                                    image_url = img_el.get_attribute("src") or img_el.get_attribute("data-src") or ""
                                
                                # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã¯ã‚¹ã‚­ãƒƒãƒ—
                                if not image_url or "pixel.jpg" in image_url:
                                    continue

                                # ä¾¡æ ¼ã®æ•°å€¤åŒ–
                                price_text = price_el.inner_text()
                                price = int(re.sub(r'\D', '', price_text))
                                
                                # èª¬æ˜æ–‡ã®æ•´å½¢
                                desc_el = item.query_selector(".product-item-details .description") or item.query_selector(".product-item-details")
                                description = desc_el.inner_text().replace('\n', ' / ').strip() if desc_el else ""

                                current_page_names.append(name)

                                # CSVã¸è¿½è¨˜ä¿å­˜
                                with open(output_csv, 'a', newline='', encoding='utf-8-sig') as f:
                                    writer = csv.writer(f)
                                    writer.writerow([category_name, name, price, item_url, image_url, description])
                                
                                total_count += 1
                        except Exception:
                            continue
                    
                    # åŒä¸€å†…å®¹ã®ãƒšãƒ¼ã‚¸ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰ã®ãƒã‚§ãƒƒã‚¯
                    current_page_set = set(current_page_names)
                    if not current_page_names or (last_page_data and current_page_set.issubset(last_page_data)):
                        print(f"ğŸ“Š é‡è¤‡ã¾ãŸã¯ç©ºãƒšãƒ¼ã‚¸ã®ãŸã‚ã€{category_name} ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                        break

                    last_page_data = current_page_set
                    print(f"âœ… {category_name} ãƒšãƒ¼ã‚¸ {page_num} å®Œäº†ï¼ˆç´¯è¨ˆ: {total_count}ä»¶ï¼‰")
                    
                    page_num += 1

                except Exception as e:
                    print(f"âš ï¸ ãƒšãƒ¼ã‚¸é·ç§»ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®ã‚‹ã¨å½¹ç«‹ã¡ã¾ã™
                    # page.screenshot(path=f"error_page_{page_num}.png")
                    break

        browser.close()
    
    print(f"\nâœ¨ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼ç´¯è¨ˆå–å¾—ä»¶æ•°: {total_count}")
    
    if total_count > 0:
        run_docker_import(output_csv)
    else:
        print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯å®Ÿè¡Œã—ã¾ã›ã‚“ã€‚")

if __name__ == "__main__":
    scrape_acer_to_csv_realtime()