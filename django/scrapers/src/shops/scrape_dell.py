import os
import django
import re
import json
import time
import random
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from django.db import transaction

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

def get_refined_genre(url, name):
    """
    è£½å“åã¨URLã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æ—¥æœ¬èªãƒ»è‹±èªä¸¡æ–¹ã§é«˜ç²¾åº¦ã«åˆ¤å®šã™ã‚‹
    """
    # åˆ¤å®šç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆã—ã¦å°æ–‡å­—åŒ–
    text = (url + " " + name).lower()
    
    # 1. ãƒ¢ãƒ‹ã‚¿ãƒ¼
    if any(k in text for k in ["monitor", "ãƒ¢ãƒ‹ã‚¿ãƒ¼", "ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤", "display"]):
        return "monitor"
    
    # 2. ã‚²ãƒ¼ãƒŸãƒ³ã‚°PC (Alienwareãƒ–ãƒ©ãƒ³ãƒ‰ã‚„Gamingã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰)
    if any(k in text for k in ["alienware", "gaming", "ã‚²ãƒ¼ãƒŸãƒ³ã‚°", "g-series"]):
        return "gaming_pc"
    
    # 3. å‘¨è¾ºæ©Ÿå™¨ãƒ»ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼ (ãƒãƒ–ã€ãƒ‰ãƒƒã‚¯ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã€ãƒãƒƒã‚°ãªã©)
    # PCæœ¬ä½“ã¨èª¤åˆ¤å®šã•ã‚Œãªã„ã‚ˆã†ã€å…ˆã«åˆ¤å®š
    if any(k in text for k in [
        "backpack", "ãƒãƒƒã‚¯ãƒ‘ãƒƒã‚¯", "mouse", "ãƒã‚¦ã‚¹", "keyboard", "ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰", 
        "headset", "ãƒ˜ãƒƒãƒ‰ã‚»ãƒƒãƒˆ", "adapter", "ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼", "ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼", "speaker", 
        "ã‚±ãƒ¼ã‚¹", "sleeve", "ã‚¹ãƒªãƒ¼ãƒ–", "dock", "ãƒ‰ãƒƒã‚¯", "hub", "ãƒãƒ–", "webcam", "ã‚¦ã‚§ãƒ–ã‚«ãƒ¡ãƒ©"
    ]):
        return "accessories"
    
    # 4. ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ (laptop)
    if any(k in text for k in [
        "laptop", "ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³", "inspiron", "xps", "2-in-1", "ãƒãƒ¼ãƒˆpc", 
        "latitude", "vostro", "convertible", "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ–ãƒ«"
    ]):
        # â€»Vostro/Latitudeã‚’laptopã«çµ±åˆã€‚ç´°åˆ†åŒ–ã—ãŸã„å ´åˆã¯å…ˆã«ãƒ“ã‚¸ãƒã‚¹åˆ¤å®šã‚’å…¥ã‚Œã‚‹
        return "laptop"
    
    # 5. ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— (ä¸€ä½“å‹PCã€ãƒã‚¤ã‚¯ãƒ­PCã€ã‚¿ãƒ¯ãƒ¼ã‚’å«ã‚€)
    if any(k in text for k in [
        "desktop", "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—", "optiplex", "precision", "ã‚¹ãƒªãƒ ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—",
        "all-in-one", "ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³ãƒ¯ãƒ³", "tower", "ã‚¿ãƒ¯ãƒ¼", "micro", "ãƒã‚¤ã‚¯ãƒ­"
    ]):
        return "desktop"
    
    # åˆ¤å®šä¸èƒ½ãªå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return "pc"

def extract_from_json_ld(soup):
    """HTMLå†…ã®JSON-LDã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    result = {"name": "", "price": 0, "image": None}
    scripts = soup.find_all('script', type='application/ld+json')
    for script in scripts:
        try:
            data = json.loads(script.string)
            items = data if isinstance(data, list) else [data]
            for item in items:
                if item.get('@type') == 'Product':
                    result["name"] = item.get('name', "")
                    offers = item.get('offers')
                    if offers:
                        if isinstance(offers, list): offers = offers[0]
                        p = offers.get('price')
                        if p:
                            result["price"] = int(float(str(p).replace(',', '')))
                    img = item.get('image')
                    if img:
                        result["image"] = img[0] if isinstance(img, list) else img
                    return result
        except: continue
    return result

def scrape_detail_page(page, url, current_index, total_count):
    """å€‹åˆ¥è£½å“ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦DBä¿å­˜"""
    url = url.split('#')[0].split('?')[0].rstrip('/')
    
    try:
        unique_id = "dell-" + url.split('/')[-1]
        
        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿
        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        page.wait_for_timeout(2500)
        
        soup = BeautifulSoup(page.content(), 'html.parser')
        json_data = extract_from_json_ld(soup)
        
        # åç§°ã®å–å¾—ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è£œå®Œï¼‰
        name = json_data["name"] or page.title().split('|')[0].strip()
        name = name.replace('Dell æ—¥æœ¬', '').strip()
        
        # ã€å¼·åŒ–ç‰ˆã€‘ã‚¸ãƒ£ãƒ³ãƒ«ã®åˆ¤å®š
        genre = get_refined_genre(url, name)
        
        # ä¾¡æ ¼ã®å–å¾—
        price = json_data["price"]
        if price == 0:
            price_el = soup.select_one('[data-testid="shared-ps-dell-price"], .ps-dell-price, .dell-price')
            if price_el:
                price_text = re.sub(r'[^\d]', '', price_el.get_text())
                if price_text: price = int(price_text)

        # ç”»åƒURL
        image_url = json_data["image"] or ""
        if not image_url:
            img_handle = page.query_selector('img[data-testid="shared-ps-image"], .ps-image img')
            if img_handle:
                src = img_handle.get_attribute("src")
                image_url = "https:" + src if src and src.startswith('//') else src

        # DBä¿å­˜å‡¦ç†
        with transaction.atomic():
            PCProduct.objects.update_or_create(
                unique_id=unique_id,
                defaults={
                    'site_prefix': 'DELL',
                    'maker': 'Dell',
                    'raw_genre': genre,
                    'unified_genre': genre,
                    'name': name,
                    'price': price,
                    'url': url,
                    'image_url': image_url,
                    'description': f"Dellå…¬å¼ {genre} ã‚«ãƒ†ã‚´ãƒªè£½å“ - {name}",
                    'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'è©³ç´°ç¢ºèª',
                    'is_active': True,
                }
            )
        
        price_display = f"Â¥{price:,}" if price > 0 else "ä¾¡æ ¼ä¸æ˜"
        print(f"ğŸ” [{current_index + 1}/{total_count}] âœ… åˆ†é¡æ›´æ–° [{genre.upper()}]: {name[:30]}... ({price_display})")
        return True
    except Exception as e:
        print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {url} -> {e}")
        return False

def run_crawler():
    """ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚¹ã‚­ãƒ£ãƒ³ã‹ã‚‰å·¡å›é–‹å§‹"""
    target_categories = [
        "https://www.dell.com/ja-jp/shop/deals/top-pc-deals",
        "https://www.dell.com/ja-jp/shop/scc/sc/laptops",
        "https://www.dell.com/ja-jp/shop/scc/sc/desktops",
        "https://www.dell.com/ja-jp/shop/deals/gaming-deals",
        "https://www.dell.com/ja-jp/shop/deals/business-pc-deals",
        "https://www.dell.com/ja-jp/shop/deals/monitors-deals",
        "https://www.dell.com/ja-jp/shop/deals/pc-accessories-deals",
        "https://www.dell.com/ja-jp/shop/deals/clearance-pc-deals",
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 800}
        )
        
        page = context.new_page()
        # é€šä¿¡é‡å‰Šæ¸›
        page.route("**/*.{png,jpg,jpeg,gif,webp,svg,woff,woff2,css}", 
                   lambda route: route.abort() if route.request.resource_type != "document" else route.continue_())

        all_product_urls = set()
        for cat_url in target_categories:
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚¹ã‚­ãƒ£ãƒ³ä¸­: {cat_url}")
            try:
                page.goto(cat_url, wait_until="commit", timeout=60000)
                page.wait_for_timeout(3000)
                for _ in range(5):
                    page.evaluate("window.scrollBy(0, 1000)")
                    page.wait_for_timeout(800)
                
                hrefs = page.eval_on_selector_all('a[href*="/shop/"]', 
                    'elements => elements.map(e => e.href)')
                
                for h in hrefs:
                    clean_h = h.split('#')[0].split('?')[0].rstrip('/')
                    if any(p in clean_h for p in ["spd", "pdp", "pd", "cp"]):
                        all_product_urls.add(clean_h)
            except Exception as e:
                print(f"   âŒ ã‚¹ã‚­ãƒ£ãƒ³å¤±æ•—: {cat_url}")
        
        url_list = sorted(list(all_product_urls))
        total_count = len(url_list)
        print(f"ğŸš€ åˆè¨ˆ {total_count}ä»¶ã‚’é«˜ç²¾åº¦åˆ†é¡ãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†é–‹å§‹")
        
        for i, url in enumerate(url_list): 
            scrape_detail_page(page, url, i, total_count)
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            time.sleep(random.uniform(0.8, 1.5))
            
        browser.close()
        print(f"âœ¨ å®Œäº†ã—ã¾ã—ãŸã€‚ã™ã¹ã¦ã®è£½å“ãŒã‚ˆã‚Šæ­£ç¢ºã«åˆ†é¡ã•ã‚Œã¾ã—ãŸã€‚")

if __name__ == "__main__":
    run_crawler()