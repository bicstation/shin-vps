import os
import django
import re
import json
import time
import random
import hashlib
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

# ==========================================
# ğŸ”‘ 1. è¨­å®šæƒ…å ±
# ==========================================
FIXED_AFFILIATE_LINK = 'https://click.linksynergy.com/fs-bin/click?id=nNBA6GzaGrQ&offerid=1568114.10004952&type=3&subid=0'

# ==========================================
# ğŸ› ï¸ 2. è§£æã‚¨ãƒ³ã‚¸ãƒ³
# ==========================================

def get_image_url_from_source(html_content):
    """é«˜ç”»è³ªãªè£½å“ç”»åƒã‚’æŠ½å‡ºã€‚"""
    pattern = r'https?://i\.dell\.com/is/image/DellContent/[^"\s?{}<>]+'
    matches = re.findall(pattern, html_content)
    for url in matches:
        if any(x in url.lower() for x in ['60x48', 'seasonal', 'logo', 'icon', 'flag', 'nav', 'fnav', 'banner']):
            continue
        return f"{url}?fmt=png-alpha&wid=800"
    return ""

def clean_spec_text(text):
    """ä¸è¦ãªæ–‡è¨€ã€é›»è©±ç•ªå·ã€ç›¸è«‡çª“å£ãªã©ã‚’å¾¹åº•æ’é™¤"""
    if not text or "æœªæ¤œå‡º" in text:
        return "æœªæ¤œå‡º"
    
    noise_patterns = [
        r"è¦‹ç©ã‚Šãƒ»è³¼å…¥ç›¸è«‡.*?(å¹³æ—¥|ã¾ã§)", 
        r"0120-\d+-\d+", 
        r"ãƒãƒ£ãƒƒãƒˆãƒ»LINE", 
        r"ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã¨2-in-1 PC", 
        r"XPSãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³", 
        r"ã™ã¹ã¦å±•é–‹", 
        r"è©³ç´°æƒ…å ±",
        r"ãŠå‹§ã‚ã—ã¾ã™", 
        r"ãŠå®¢æ§˜ã«æœ€é©ãª", 
        r"ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼",
        r"ã‚¤ãƒ³ãƒ†ãƒ«ã®è©³ç´°æƒ…å ± è£½å“",
        r"Windows 11 (Home|Pro), Copilot\+ PC",
        r"ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚° ã‚·ã‚¹ãƒ†ãƒ è¨€èªãƒ‘ãƒƒã‚¯"
    ]
    
    cleaned = text
    for p in noise_patterns:
        cleaned = re.sub(p, "", cleaned, flags=re.IGNORECASE)
    
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned if len(cleaned) > 0 else "æœªæ¤œå‡º"

def extract_specs_ultimate(page, soup):
    """
    1. ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿å±æ€§(data-testid)ã‹ã‚‰æŠ½å‡º
    2. å¤±æ•—ã—ãŸå ´åˆã¯ã€ç”»é¢ä¸Šã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ­£è¦è¡¨ç¾ã§æŠ½å‡º
    """
    specs = {'cpu': 'æœªæ¤œå‡º', 'mem': 'æœªæ¤œå‡º', 'ssd': 'æœªæ¤œå‡º', 'gpu': 'æœªæ¤œå‡º', 'os': 'æœªæ¤œå‡º', 'disp': 'æœªæ¤œå‡º'}
    
    # æˆ¦ç•¥1: data-testid ã«ã‚ˆã‚‹ç›´æ¥æŠ½å‡º
    spec_map = {
        'cpu': 'processor', 'mem': 'memory', 'ssd': 'hard-drive', 
        'gpu': 'video-card', 'os': 'operating-system', 'disp': 'display'
    }
    for key, tid in spec_map.items():
        el = soup.find(attrs={"data-testid": f"shared-ps-spec-description-{tid}"})
        if el:
            specs[key] = clean_spec_text(el.get_text())

    # æˆ¦ç•¥2: ç”»é¢ä¸Šã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åŸ·å¿µã®æ¤œç´¢
    # (ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯å¾Œã®æœ€æ–°ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—)
    visible_text = page.evaluate("() => document.body.innerText")
    
    patterns = {
        'cpu': r'((?:ç¬¬\d+ä¸–ä»£)?\s*(?:Core|Ryzen|Ultra|i[3579]|Apple|Pentium|Snapdragon)[^ \n\r\t|]{2,}[^|\n\r\t]+?(?:ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼|CPU))',
        'mem': r'(\d+\s*GB\s*(?:LPDDR\d*|DDR\d*|çµ±åˆ|å†…è”µ|ãƒ¡ãƒ¢ãƒª|RAM)[^|\n\r\t]*)',
        'ssd': r'(\d+\s*(?:GB|TB)\s*(?:M\.2|NVMe|PCIe|SSD|ãƒãƒ¼ãƒ‰ãƒ‰ãƒ©ã‚¤ãƒ–|ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸))',
        'gpu': r'((?:NVIDIA|GeForce|RTX|Radeon|Arc|ã‚¤ãƒ³ãƒ†ãƒ«|UHD|ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹)[^|\n\r\t]+?(?:Video\s*Card|ãƒ“ãƒ‡ã‚ªã‚«ãƒ¼ãƒ‰|GPU|å†…è”µ)?)',
        'os': r'(Windows\s*11\s*(?:Home|Pro)[^|\n\r\t]*)',
        'disp': r'(\d+\.?\d?\s*ã‚¤ãƒ³ãƒ[^|\n\r\t]+?(?:ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤|æ¶²æ™¶|ãƒ¢ãƒ‹ã‚¿ãƒ¼|è§£åƒåº¦|2K|4K|OLED))'
    }

    for key, pattern in patterns.items():
        if specs[key] == 'æœªæ¤œå‡º':
            m = re.search(pattern, visible_text, re.I)
            if m:
                specs[key] = clean_spec_text(m.group(1))

    return f"{specs['cpu']} / {specs['mem']} / {specs['ssd']} / {specs['gpu']} / {specs['disp']} / {specs['os']}"

def extract_main_price(soup):
    """ä¿å®ˆã‚µãƒ¼ãƒ“ã‚¹ç­‰ã®å®‰ä¾¡ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–ã—ã€PCæœ¬ä½“ã®ä¾¡æ ¼ã‚’ç‹™ã†"""
    prices = []
    # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
    for script in soup.find_all('script', type='application/ld+json'):
        try:
            data = json.loads(script.string)
            items = data if isinstance(data, list) else [data]
            for item in items:
                offers = item.get('offers', {})
                p = offers[0].get('price') if isinstance(offers, list) else offers.get('price')
                if p: prices.append(int(float(str(p).replace(',', ''))))
        except: continue
    
    # ã‚»ãƒ¬ã‚¯ã‚¿æŠ½å‡º
    for sel in ['.ps-dell-price', '[data-testid="shared-ps-dell-price"]', '.monetization-price']:
        for el in soup.select(sel):
            p_text = re.sub(r'[^\d]', '', el.get_text())
            if p_text: prices.append(int(p_text))
    
    # æœ¬ä½“ä¾¡æ ¼ï¼ˆ4.5ä¸‡å††ä»¥ä¸Šï¼‰ã‚’å„ªå…ˆ
    valid = [p for p in prices if 45000 < p < 3000000]
    return min(valid) if valid else (max(prices) if prices else 0)

# ==========================================
# ğŸš€ 3. ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
# ==========================================

def scrape_detail_page(page, url, current_index, total_count):
    url_clean = url.split('#')[0].split('?')[0].rstrip('/')
    remaining = total_count - (current_index + 1)
    
    print(f"ğŸ” [{current_index + 1}/{total_count}] å·¡å›ä¸­... {url_clean}")
    
    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®š
        page.goto(url_clean, wait_until="domcontentloaded", timeout=60000)
        
        # ğŸ’¡ é‡è¦ï¼šé‚ªé­”ãªãƒãƒŠãƒ¼ã®å‰Šé™¤ã¨ã€è©³ç´°ã‚¹ãƒšãƒƒã‚¯ã®å¼·åˆ¶è¡¨ç¤º
        page.evaluate("""() => {
            // CookieãƒãƒŠãƒ¼ã‚„ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤
            const overlaySelectors = ['#onetrust-banner-sdk', '.optanon-alert-box-wrapper', '.highcharts-container'];
            overlaySelectors.forEach(sel => {
                const el = document.querySelector(sel);
                if(el) el.remove();
            });
            // ã€Œã™ã¹ã¦ã®ä»•æ§˜ã‚’è¡¨ç¤ºã€ãªã©ã®ãƒœã‚¿ãƒ³ãŒã‚ã‚Œã°ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹
            const buttons = Array.from(document.querySelectorAll('button, a'));
            const specBtn = buttons.find(b => b.innerText.includes('ã™ã¹ã¦ã®ä»•æ§˜') || b.innerText.includes('View all specs'));
            if(specBtn) specBtn.click();
        }""")
        
        # å±•é–‹ã¨ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã®ãŸã‚ã®å¾…æ©Ÿ
        page.wait_for_timeout(4000)
        page.evaluate("window.scrollTo(0, 1000)")
        
        html_content = page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        name = page.title().split('|')[0].replace('Dell æ—¥æœ¬', '').strip()
        price = extract_main_price(soup)
        image_url = get_image_url_from_source(html_content)
        description = extract_specs_ultimate(page, soup)
        genre = "laptop" if "laptop" in url_clean else "desktop"

        print(f" ğŸ“¦ è£½å“å : {name}")
        print(f" ğŸ’° ä¾¡  æ ¼ : Â¥{price:,}" if price > 0 else " ğŸ’° ä¾¡  æ ¼ : ä¾¡æ ¼ä¸æ˜")
        print(f" ğŸ“ æ§‹æˆ   : {description}")
        print(f" ğŸ–¼ï¸ ç”»åƒURL: {image_url if image_url else 'âš ï¸ å–å¾—å¤±æ•—'}")
        print(f" ğŸš€ æ®‹ã‚Š   : {remaining}ä»¶")
        print("-" * 50)

        unique_id = "dell-" + hashlib.md5(url_clean.encode()).hexdigest()[:12]
        save_data = {
            'unique_id': unique_id, 'site_prefix': 'DELL', 'maker': 'Dell',
            'raw_genre': genre, 'unified_genre': genre, 'name': name,
            'price': price, 'url': url_clean, 'affiliate_url': FIXED_AFFILIATE_LINK,
            'image_url': image_url, 'description': description, 'is_active': True,
            'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'å—æ³¨åœæ­¢',
        }
        PCProduct.objects.update_or_create(unique_id=unique_id, defaults=save_data)
        return True

    except Exception as e:
        print(f" âŒ è§£æå¤±æ•—: {e}")
        return False

def run_crawler():
    target_categories = [
        "https://www.dell.com/ja-jp/shop/deals/top-pc-deals",
        "https://www.dell.com/ja-jp/shop/scc/sc/laptops",
        "https://www.dell.com/ja-jp/shop/scc/sc/desktops",
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æœ€æ–°ã®ã‚‚ã®ã«
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 1200}
        )
        page = context.new_page()
        
        all_product_urls = set()
        print("âš™ï¸  Bicstation Dellã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (æœ€çµ‚ãƒ»åŸ·å¿µã®ã‚¹ãƒšãƒƒã‚¯æŠ½å‡ºç‰ˆ)...")

        for cat_url in target_categories:
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚­ãƒ£ãƒ³: {cat_url}")
            try:
                page.goto(cat_url, wait_until="networkidle", timeout=60000)
                page.wait_for_timeout(3000)
                # å•†å“ãƒªã‚¹ãƒˆã®ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã«å¯¾å¿œ
                for _ in range(10):
                    page.mouse.wheel(0, 1500)
                    page.wait_for_timeout(1000)
                    hrefs = page.evaluate('() => Array.from(document.querySelectorAll("a")).map(a => a.href)')
                    for h in hrefs:
                        if any(p in h for p in ["/spd/", "/pd/", "/pdp/"]):
                            all_product_urls.add(h.split('#')[0].split('?')[0].rstrip('/'))
                print(f"   ğŸ“Š ç™ºè¦‹æ¸ˆã¿URL: {len(all_product_urls)}ä»¶")
            except Exception as e:
                print(f" âš ï¸ ã‚¹ã‚­ãƒ£ãƒ³å¤±æ•—: {e}")
        
        url_list = sorted(list(all_product_urls))
        print(f"\nğŸš€ å…¨ {len(url_list)}ä»¶ã®å€‹åˆ¥è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚\n" + "="*60)
        
        for i, url in enumerate(url_list):
            scrape_detail_page(page, url, i, len(url_list))
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚§ã‚¤ãƒˆ
            time.sleep(random.uniform(1.5, 2.5))
            
        browser.close()
        print(f"\nâœ¨ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    run_crawler()