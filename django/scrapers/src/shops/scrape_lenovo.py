import os
import django
import re
import time
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

def get_genre_from_url(url):
    """URLã‹ã‚‰è£½å“ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æ¨æ¸¬ã™ã‚‹"""
    if "/laptops/" in url or "/yoga/" in url or "/thinkpad/" in url:
        return "laptop"
    if "/desktops/" in url or "/legion/" in url:
        return "desktop"
    if "/workstations/" in url:
        return "workstation"
    if "/servers/" in url:
        return "server"
    if "/tablets/" in url:
        return "tablet"
    return "pc"

def extract_specs(soup):
    specs_list = []
    container = soup.select_one('.sph-o-overview, .overview, [class*="overview"]')
    if container:
        ul = container.find('ul')
        if ul:
            for li in ul.find_all('li'):
                for sup in li.find_all('sup'):
                    sup.decompose()
                text = li.get_text(" ", strip=True)
                text = re.sub(r'\s+', ' ', text)
                if len(text) > 3:
                    specs_list.append(text)
    return " / ".join(list(dict.fromkeys(specs_list)))

def extract_image_url(page):
    selector = ".gallery-canvas .canvas-item img, .gallery-container img"
    try:
        img_handle = page.wait_for_selector(selector, timeout=5000)
        if img_handle:
            src = img_handle.get_attribute("src")
            if src:
                if src.startswith('//'): return "https:" + src
                if src.startswith('/'): return "https://www.lenovo.com" + src
                return src
    except:
        pass
    return ""

def extract_price(soup, html_content):
    for element in soup.find_all(['span', 'dd', 'div', 'p']):
        text = element.get_text()
        if 'è²©å£²ä¾¡æ ¼' in text:
            digits = re.sub(r'[^\d]', '', text)
            if digits and 30000 < int(digits) < 2000000: # ã‚µãƒ¼ãƒãƒ¼ç­‰ã‚‚è€ƒæ…®ã—ä¸Šé™ã‚’ä¸Šã’
                return int(digits)
    return 0

def scrape_detail_page(page, url):
    print(f"ğŸ” å·¡å›ä¸­... {url}")
    try:
        unique_id = url.split('/')[-1]
        page.goto(url, wait_until="domcontentloaded", timeout=30000)
        page.evaluate("window.scrollBy(0, 500)")
        page.wait_for_timeout(2000)
        
        soup = BeautifulSoup(page.content(), 'html.parser')
        
        genre = get_genre_from_url(url) # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤åˆ¥
        price = extract_price(soup, page.content())
        image_url = extract_image_url(page)
        specs_text = extract_specs(soup)
        
        save_data = {
            'unique_id': unique_id,
            'site_prefix': 'LEN',
            'maker': 'Lenovo',
            'raw_genre': genre,
            'unified_genre': genre,
            'name': page.title().split('|')[0].strip(),
            'price': price,
            'url': url,
            'image_url': image_url,
            'description': specs_text,
            'raw_html': page.content(),
            'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'å—æ³¨åœæ­¢',
            'is_active': True,
        }

        PCProduct.objects.update_or_create(unique_id=unique_id, defaults=save_data)
        print(f"âœ… ä¿å­˜: [{genre}] {save_data['name']}")
        return True
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def run_crawler():
    # ç¶²ç¾…çš„ãªã‚«ãƒ†ã‚´ãƒªãƒ¼URLãƒªã‚¹ãƒˆ
    target_categories = [
        # ãƒãƒ¼ãƒˆ
        "https://www.lenovo.com/jp/ja/c/laptops/thinkpad/",
        "https://www.lenovo.com/jp/ja/c/laptops/yoga/",
        "https://www.lenovo.com/jp/ja/d/deals/ai-pc/?ipromoID=laptops_splitter_Search_by_type_AI-PCs",
        "https://www.lenovo.com/jp/ja/d/standard-laptops/?ipromoID=laptops_splitter_Search_by_type_Standard-Notes",
        "https://www.lenovo.com/jp/ja/d/mobile-laptops/?ipromoID=laptops_splitter_Search_by_type_2in1",
        "https://www.lenovo.com/jp/ja/d/convertible-2-in-1-notebooks/?ipromoID=laptops_splitter_Search_by_type_Mobile-Notes",
        "https://www.lenovo.com/jp/ja/d/thinkpad-p-series/?ipromoID=laptops_splitter_Search_by_type_workstation",
        "https://www.lenovo.com/jp/ja/d/chromebook-laptops/?ipromoID=laptops_splitter_Search_by_type_CHROMEBOOK",
        "https://www.lenovo.com/jp/ja/laptops/results/?visibleDatas=2115%3ALegion%2CLOQ&ipromoID=Gaming_shop_Find_the_right_espot1",
        "https://www.lenovo.com/jp/ja/c/laptops/lenovo-legion-laptops/",
        # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—
        "https://www.lenovo.com/jp/ja/desktops/results/?visibleDatas=2124%3ATiny%25EF%25BC%2588%25E8%25B6%2585%25E5%25B0%258F%25E5%259E%258B%25EF%25BC%2589&ipromoID=desktops_splitter_Find_desktops_by_type_Tiny",
        "https://www.lenovo.com/jp/ja/desktops/subseries-results/?visibleDatas=2325:%E3%82%B2%E3%83%BC%E3%83%9F%E3%83%B3%E3%82%B0",
        "https://www.lenovo.com/jp/ja/desktops/subseries-results/?visibleDatas=2325:%E6%99%AE%E6%AE%B5%E4%BD%BF%E3%81%84%E3%83%BB%E3%82%A8%E3%83%B3%E3%82%BF%E3%83%BC%E3%83%86%E3%82%A4%E3%83%A1%E3%83%B3%E3%83%88",
        "https://www.lenovo.com/jp/ja/desktops/subseries-results/?visibleDatas=2325:%E3%82%AF%E3%83%AA%E3%82%A8%E3%82%A4%E3%82%BF%E3%83%BC&ipromoID=desktops_splitter_Search_by_Use_creator",
        "https://www.lenovo.com/jp/ja/desktops/results/?visibleDatas=2115%3ALegion%2CLOQ&ipromoID=Gaming_shop_Find_the_right_espot2",
        "https://www.lenovo.com/jp/ja/c/desktops/legion-desktops/",
        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        "https://www.lenovo.com/jp/ja/workstations/?ipromoID=Megamenu_workstation",
        # ã‚µãƒ¼ãƒãƒ¼ï¼ˆæ§‹é€ ãŒé•ã†å ´åˆã¯è¦èª¿æ•´ã§ã™ãŒã¾ãšã¯å…±é€šã§è©¦è¡Œï¼‰
        "https://www.lenovo.com/jp/ja/servers-storage/?ipromoID=Megamenu_Servers-Storage",
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        page = context.new_page()
        
        all_product_urls = set()
        for cat_url in target_categories:
            print(f"ğŸ“‚ ã‚¹ã‚­ãƒ£ãƒ³ä¸­: {cat_url}")
            try:
                page.goto(cat_url, wait_until="domcontentloaded")
                page.wait_for_timeout(5000)
                # å…¨è£½å“è©³ç´°ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º (/p/ ä»¥ä¸‹ã®è£½å“ãƒšãƒ¼ã‚¸)
                hrefs = page.eval_on_selector_all('a[href*="/p/"]', 'elements => elements.map(e => e.href)')
                all_product_urls.update({url.split('?')[0].rstrip('/') for url in hrefs if "/p/" in url})
            except Exception as e:
                print(f"  âŒ å–å¾—å¤±æ•—: {e}")
        
        print(f"ğŸš€ åˆè¨ˆ {len(all_product_urls)}ä»¶ã®å…¨è£½å“ã‚’å‡¦ç†é–‹å§‹")
        for i, url in enumerate(all_product_urls):
            scrape_detail_page(page, url)
            time.sleep(2)
        browser.close()

if __name__ == "__main__":
    run_crawler()