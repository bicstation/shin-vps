import csv
import os
import subprocess
from bs4 import BeautifulSoup

def run_docker_import_sycom(csv_path):
    container_name = "api_django_v2"
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    print(f"ğŸš€ Dockerã‚³ãƒ³ãƒ†ãƒŠã¸ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ãƒ»DBæ›´æ–°ä¸­...")
    try:
        subprocess.run(["docker", "cp", csv_path, f"{container_name}:/usr/src/app/scrapers/sycom_products.csv"], check=True)
        # æŒ‡å®šã•ã‚ŒãŸ docker-compose.stg.yml ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œ
        import_cmd = [
            "docker", "compose", "-f", "docker-compose.stg.yml", 
            "exec", "django-v2", 
            "python", "manage.py", "import_sycom"
        ]
        subprocess.run(import_cmd, check=True, cwd=project_root)
        print(f"âœ… Djangoã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼")
    except Exception as e:
        print(f"âŒ Dockeré€£æºã‚¨ãƒ©ãƒ¼: {e}")

def scrape_sycom_from_html_source():
    # ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ä¿å­˜ã—ãŸHTMLãƒ•ã‚¡ã‚¤ãƒ«
    input_file = os.path.join(os.path.dirname(__file__), "sycom_data.txt")
    output_csv = os.path.join(os.path.dirname(__file__), "sycom_products.csv")
    
    if not os.path.exists(input_file):
        print(f"âŒ {input_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚½ãƒ¼ã‚¹ã‚’è¡¨ç¤ºã—ã€sycom_page.html ã¨ã„ã†åå‰ã§ä¿å­˜ã—ã¦ãã ã•ã„ã€‚")
        return

    print(f"ğŸ“– {input_file} ã‚’è§£æä¸­...")
    with open(input_file, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")

    all_data = []

    # ğŸ’¡ å…±æœ‰ã„ãŸã ã„ãŸHTMLæ§‹é€ ã«åŸºã¥ãã€å„å•†å“ã‚¢ã‚¤ãƒ†ãƒ (div.item)ã‚’ãƒ«ãƒ¼ãƒ—
    items = soup.find_all("div", class_="item")
    
    for item in items:
        # 1. å•†å“åã‚’å–å¾— (<p class="name01">)
        name_tag = item.find("p", class_="name01")
        # 2. ä¾¡æ ¼ã‚’å–å¾— (<span id="model_xxxxxx">)
        price_tag = item.find("span", id=lambda x: x and x.startswith('model_'))
        # 3. ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºURLã‚’å–å¾—
        link_tag = item.find("a", href=True)

        if name_tag and price_tag:
            name = name_tag.get_text(strip=True)
            # ã‚«ãƒ³ãƒã‚’é™¤å»ã—ã¦æ•°å€¤åŒ–
            price_text = price_tag.get_text(strip=True).replace(",", "")
            price = int(price_text)
            
            url = link_tag["href"]
            if not url.startswith("http"):
                url = "https://www.sycom.co.jp" + url

            all_data.append(["Desktop", f"[Sycom] {name}", price, url, "", ""])
            print(f"   âœ… æŠ½å‡ºæˆåŠŸ: {name} | {price}å††")

    if all_data:
        # é‡è¤‡å‰Šé™¤
        unique_data = {d[1]: d for d in all_data}.values()
        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['category', 'name', 'price', 'url', 'image_url', 'description'])
            writer.writerows(unique_data)
        
        print(f"âœ¨ åˆè¨ˆ {len(unique_data)} ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        run_docker_import_sycom(output_csv)
    else:
        print("âŒ å•†å“æƒ…å ±ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚HTMLã®ä¿å­˜å½¢å¼ï¼ˆCtrl+Sï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    scrape_sycom_from_html_source()