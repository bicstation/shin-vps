import os
import django
import re
import time
import hashlib
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

def extract_price(text):
    """HPã®ä¾¡æ ¼ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    if not text: return 0
    nums = re.sub(r'[^\d]', '', text)
    return int(nums) if nums else 0

def scrape_hp_category(page, url, genre):
    """ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã‚’è§£æã—ã¦DBä¿å­˜"""
    print(f"ğŸ” HPã‚«ãƒ†ã‚´ãƒªè§£æä¸­... {url}")
    try:
        page.goto(url, wait_until="networkidle", timeout=60000)
        
        # è£½å“ã‚¢ã‚¤ãƒ†ãƒ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        try:
            page.wait_for_selector(".product-item", timeout=10000)
        except:
            print(f" âš ï¸ è£½å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {url}")
            return

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ç”»åƒã‚’èª­ã¿è¾¼ã¾ã›ã‚‹
        for _ in range(3):
            page.evaluate("window.scrollBy(0, 1000)")
            page.wait_for_timeout(800)

        soup = BeautifulSoup(page.content(), 'html.parser')
        items = soup.select(".product-item")
        
        print(f"ğŸ“¦ ã“ã®ãƒšãƒ¼ã‚¸ã§ {len(items)} ä»¶ã®è£½å“ã‚’è¦‹ã¤ã‘ã¾ã—ãŸ")

        for item in items:
            try:
                # 1. è£½å“å
                name_tag = item.select_one(".name, .product-name")
                if not name_tag: continue
                name = name_tag.get_text(strip=True)

                # 2. ãƒªãƒ³ã‚¯ã¨IDï¼ˆæ—¥æœ¬èªæ’é™¤ï¼‰
                link_tag = item.select_one("a[href*='/directplus/'], a[href*='/shop/pdp/']")
                if not link_tag: continue
                
                raw_url = link_tag['href']
                product_url = raw_url if raw_url.startswith('http') else "https://jp.ext.hp.com" + raw_url
                
                # --- ã€é‡è¦ã€‘æ—¥æœ¬èªæ’é™¤ãƒ­ã‚¸ãƒƒã‚¯ ---
                # URLã®æœ«å°¾ã‚’å–å¾—ã—ã€è‹±æ•°å­—ã¨ãƒã‚¤ãƒ•ãƒ³ä»¥å¤–ã‚’æ¶ˆã™
                raw_id_part = product_url.split('/')[-1] or product_url.split('/')[-2]
                safe_id = re.sub(r'[^a-zA-Z0-9-]', '', raw_id_part)
                
                # IDãŒç©ºã€ã¾ãŸã¯çŸ­ã™ãã‚‹å ´åˆã¯ãƒãƒƒã‚·ãƒ¥åŒ–
                if len(safe_id) < 3:
                    safe_id = hashlib.md5(product_url.encode()).hexdigest()[:10]
                
                unique_id = f"hp-{safe_id}"
                # -------------------------------

                # 3. ä¾¡æ ¼
                price_tag = item.select_one(".price-amount, .price")
                price = extract_price(price_tag.get_text()) if price_tag else 0

                # 4. ç”»åƒ
                img_tag = item.select_one("img")
                image_url = ""
                if img_tag:
                    image_url = img_tag.get('data-original') or img_tag.get('src') or ""
                    if image_url.startswith('//'): image_url = "https:" + image_url
                    elif image_url.startswith('/'): image_url = "https://jp.ext.hp.com" + image_url

                # 5. ã‚¹ãƒšãƒƒã‚¯
                spec_tags = item.select(".spec-list li, .summary-spec li")
                specs = " / ".join([s.get_text(strip=True) for s in spec_tags])

                # DBä¿å­˜
                PCProduct.objects.update_or_create(
                    unique_id=unique_id,
                    defaults={
                        'site_prefix': 'HP',
                        'maker': 'HP',
                        'raw_genre': genre,
                        'unified_genre': genre,
                        'name': name,
                        'price': price,
                        'url': product_url,
                        'image_url': image_url,
                        'description': specs or f"HPå…¬å¼ {genre} - {name}",
                        'is_active': True,
                        'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'ç¢ºèªä¸­',
                    }
                )
                print(f"  âœ… ä¿å­˜: {unique_id} | {name[:20]}...")

            except Exception as e:
                continue

    except Exception as e:
        print(f"  âŒ ã‚«ãƒ†ã‚´ãƒªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

def run_crawler():
    # HPã®ä¸»è¦ã‚«ãƒ†ã‚´ãƒªURLï¼ˆä»¥å‰ã®ã‚³ãƒ¼ãƒ‰ã‚ˆã‚Šæœ€æ–°ã®ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒšãƒ¼ã‚¸ã‚’è¿½åŠ ï¼‰
    categories = [
        ("https://jp.ext.hp.com/promotions/personal/weekend/", "laptop"),     # é€±æœ«ã‚»ãƒ¼ãƒ«
        ("https://jp.ext.hp.com/notebooks/personal/omnibook_ultra/", "laptop"),
        ("https://jp.ext.hp.com/desktops/personal/", "desktop"),
        ("https://www.hp.com/jp-ja/shop/vpcs/gaming-desktops.html", "gaming_pc"), # ã‚²ãƒ¼ãƒŸãƒ³ã‚°
        ("https://jp.ext.hp.com/notebooks/business/", "laptop"),
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        page = context.new_page()

        for url, genre in categories:
            scrape_hp_category(page, url, genre)
            time.sleep(2)

        browser.close()
        print("âœ¨ HPã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†")

if __name__ == "__main__":
    run_crawler()