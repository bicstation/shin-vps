# -*- coding: utf-8 -*-
import re
import json
import time
import urllib.parse
import requests
import base64
import os
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- è¨­å®š ---
OLLAMA_API_URL = "http://ollama-v2:11434/api/generate"
BASE_SEARCH_URL = "https://www.mouse-jp.co.jp/store/goods/search.aspx?search=x&limit=100"

# --- ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å›ºå®šè¨­å®š ---
# æŒ‡å®šã•ã‚ŒãŸçµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨
SAVE_DIR = "/home/maya/dev/shin-vps/django/scrapers/src/json"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR, exist_ok=True)

DEBUG_IMG_DIR = os.path.join(SAVE_DIR, "debug_screenshots")
BACKUP_FILE = os.path.join(SAVE_DIR, "mouse_backup.json")
FINAL_FILE = os.path.join(SAVE_DIR, "mouse_final_results.json")

os.makedirs(DEBUG_IMG_DIR, exist_ok=True)

def ask_ollama_about_spec(base64_image, raw_text_hint):
    """
    ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿åˆã‚ã›ã¦Llavaã«è§£æã‚’ä¾é ¼ã—ã¾ã™ã€‚
    """
    try:
        prompt = f"""
        Extract PC hardware specifications. 
        Use the provided image and this text hint from the page:
        ---
        {raw_text_hint[:800]} 
        ---
        Respond ONLY in valid JSON format:
        {{"cpu": "...", "gpu": "...", "ram": "...", "storage": "...", "price": "..."}}
        - Use "Unknown" if not found.
        """
        payload = {
            "model": "llava",
            "prompt": prompt,
            "images": [base64_image],
            "stream": False,
            "options": {"temperature": 0.0}
        }
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=60)
        raw_res = response.json().get("response", "").strip()
        json_match = re.search(r'\{.*\}', raw_res, re.DOTALL)
        return json_match.group(0) if json_match else raw_res
    except Exception as e:
        return json.dumps({"error": str(e)})

def get_mouse_category(name, soup):
    name_up = name.upper()
    breadcrumb = soup.find("ul", id="bread-crumb-list")
    bc_text = breadcrumb.get_text() if breadcrumb else ""
    if any(x in bc_text or x in name_up for x in ["ãƒ¢ãƒ‹ã‚¿ãƒ¼", "ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤", "IIYAMA"]):
        return "monitor", "æ¶²æ™¶ãƒ¢ãƒ‹ã‚¿ãƒ¼"
    if any(x in bc_text or x in name_up for x in ["ãƒãƒ¼ãƒˆ", "LAPTOP", "B4-", "F4-", "DAIV Z4"]):
        return "laptop", "ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³"
    return "desktop", "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ‘ã‚½ã‚³ãƒ³"

def fetch_detail_info(page, url):
    """
    è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    """
    try:
        page.goto(url, wait_until="networkidle", timeout=60000)
        # ç”»åƒèª­ã¿è¾¼ã¿ã®ãŸã‚ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        for _ in range(3):
            page.mouse.wheel(0, 800)
            time.sleep(0.5)

        soup = BeautifulSoup(page.content(), 'html.parser')
        
        # --- ç”»åƒURLæŠ½å‡º (JSON-LD ï¼ OGP ï¼ HTML) ---
        image_url = ""
        json_lds = soup.find_all("script", type="application/ld+json")
        ld_data_final = {}
        for jld in json_lds:
            try:
                ld_data = json.loads(jld.string)
                if isinstance(ld_data, list): ld_data = ld_data[0]
                img = ld_data.get("image")
                if img:
                    image_url = img[0] if isinstance(img, list) else img
                    ld_data_final = ld_data
                    break
            except: continue
        
        if not image_url:
            og_img = soup.find("meta", property="og:image")
            if og_img: image_url = og_img.get("content")
            
        if not image_url:
            img_tag = soup.select_one(".goods-image-main img") or soup.select_one("#main_image")
            if img_tag:
                image_url = img_tag.get("src") or img_tag.get("data-src")

        if image_url:
            image_url = urllib.parse.urljoin(url, image_url)
            if "spacer.gif" in image_url: image_url = ""

        # ã‚¹ãƒšãƒƒã‚¯è¦ç´ 
        spec_selector = ".block-goods-detail"
        raw_text_hint = ""
        try:
            page.wait_for_selector(spec_selector, timeout=5000)
            target_el = page.query_selector(spec_selector)
            if target_el:
                raw_text_hint = target_el.inner_text()
        except:
            target_el = None

        # ã‚¹ã‚¯ã‚·ãƒ§
        product_id = url.split('/')[-1] or str(int(time.time()))
        img_path = os.path.join(DEBUG_IMG_DIR, f"{product_id}.png")
        if target_el: target_el.screenshot(path=img_path)
        else: page.screenshot(path=img_path)
        
        with open(img_path, "rb") as f:
            img_base64 = base64.b64encode(f.read()).decode('utf-8')

        ai_json_str = ask_ollama_about_spec(img_base64, raw_text_hint)
        name = soup.find("h1").get_text(strip=True) if soup.find("h1") else "Unknown"
        
        price_raw = 0
        if ld_data_final:
            try: price_raw = ld_data_final.get("offers", {}).get("price", 0)
            except: pass

        unified_genre, raw_genre = get_mouse_category(name, soup)

        return {
            "unique_id": f"mouse_{product_id}",
            "site_prefix": "mouse",
            "maker": "Mouse Computer",
            "name": name,
            "price": int(str(price_raw).replace(',', '')) if price_raw else 0,
            "url": url,
            "image_url": image_url,
            "raw_genre": raw_genre,
            "unified_genre": unified_genre,
            "description": raw_text_hint,
            "ai_extracted_json": ai_json_str,
            "stock_status": "åœ¨åº«ã‚ã‚Š"
        }
    except Exception as e:
        print(f"\n   âš ï¸ è§£æå¤±æ•—: {url} | {e}")
        return None

def run_mouse_full_scan():
    print("\n" + "="*80)
    print("ğŸš€ [Mouse Computer] Djangoçµ±åˆãƒ»ãƒ‘ã‚¹å›ºå®šãƒ¢ãƒ¼ãƒ‰")
    print(f"ğŸ“‚ ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {SAVE_DIR}")
    print("="*80 + "\n")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(viewport={'width': 1920, 'height': 1080})
        page = context.new_page()

        target_urls = []
        offset = 0
        while len(target_urls) < 100:
            print(f"ğŸ”— ãƒªã‚¹ãƒˆå–å¾—ä¸­... ({len(target_urls)}ä»¶ç¢ºä¿)", end="\r")
            try:
                page.goto(f"{BASE_SEARCH_URL}&o={offset}", wait_until="domcontentloaded")
                soup = BeautifulSoup(page.content(), 'html.parser')
                links = soup.find_all('a', href=re.compile(r'/store/g/g'))
                if not links: break
                new_links = [urllib.parse.urljoin("https://www.mouse-jp.co.jp", l.get('href')).split('?')[0] for l in links]
                unique_new = [l for l in new_links if l not in target_urls]
                if not unique_new: break
                target_urls.extend(unique_new)
                offset += 100
            except:
                break

        results = []
        for i, url in enumerate(target_urls):
            current_idx = i + 1
            print(f"[{current_idx:03}/{len(target_urls):03}] ğŸ è§£æé–‹å§‹: {url}")
            
            data = fetch_detail_info(page, url)
            
            if data:
                img_st = data['image_url'] if data['image_url'] else "âŒ å–å¾—å¤±æ•—"
                print(f"   ğŸ–¼ï¸  ç”»åƒURL : {img_st}")
                print(f"   ğŸ’° ä¾¡æ ¼    : {data['price']:,} å††")
                print(f"   ğŸ¤– AIè§£æ   : {data['ai_extracted_json']}")
                print("-" * 70)
                results.append(data)

            # 5ä»¶ã”ã¨ã«ä¸­é–“ä¿å­˜
            if current_idx % 5 == 0:
                with open(BACKUP_FILE, "w", encoding="utf-8") as f:
                    json.dump(results, f, ensure_ascii=False, indent=4)
                print(f"   ğŸ’¾ ä¸­é–“ä¿å­˜å®Œäº†: {os.path.basename(BACKUP_FILE)}")

        # æœ€çµ‚ä¿å­˜
        with open(FINAL_FILE, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        
        print("\n" + "="*80)
        print(f"ğŸ‰ å®Œäº†ï¼ å…¨ {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
        print(f"ğŸ“„ æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«: {FINAL_FILE}")
        print(f"ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ: {DEBUG_IMG_DIR}")
        print("="*80 + "\n")
        browser.close()

if __name__ == "__main__":
    run_mouse_full_scan()