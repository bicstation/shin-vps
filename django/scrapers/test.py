import os
import django
import re
import time
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

def print_debug_info(data):
    """ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«æ•´å½¢ã—ã¦è¡¨ç¤º"""
    print("\n" + "="*60)
    print("ğŸ“‹ ã€ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒƒãƒ”ãƒ³ã‚°ç¢ºèªã€‘")
    print(f"ğŸ”— URL: {data['url']}")
    print(f"ğŸ†” ID : {data['unique_id']}")
    print(f"ğŸ”¤ æ¥é ­è¾: {data['site_prefix']} | ãƒ¡ãƒ¼ã‚«ãƒ¼: {data['maker']}")
    print(f"ğŸ“ ã‚¸ãƒ£ãƒ³ãƒ«: {data['unified_genre']} ({data['raw_genre']})")
    print(f"ğŸ’° ä¾¡æ ¼: {data['price']} å††")
    print(f"ğŸ–¼ï¸ ç”»åƒ: {data['image_url']}")
    print(f"âœ… æ²è¼‰ä¸­: {data['is_active']}")
    print(f"ğŸ“ ã‚¹ãƒšãƒƒã‚¯: \n   {data['description'][:150]}...")
    print("="*60 + "\n")

def extract_specs(soup):
    """
    ã‚¹ã‚¯ã‚·ãƒ§ã«åŸºã¥ãã€.overview ã¾ãŸã¯ .sph-o-overview ã‹ã‚‰ã‚¹ãƒšãƒƒã‚¯ã‚’æŠ½å‡º
    """
    specs_list = []
    
    # ã‚¹ã‚¯ã‚·ãƒ§ã®2ãƒ‘ã‚¿ãƒ¼ãƒ³ (.overview ã¨ .sph-o-overview) ã‚’ä¸¡æ–¹æ¢ã™
    container = soup.select_one('.overview, .sph-o-overview, [class*="overview"]')
    
    if container:
        ul = container.find('ul')
        if ul:
            for li in ul.find_all('li'):
                # Â® ã‚„ â„¢ ãªã©ã®ä¸Šä»˜ãæ–‡å­—ã‚’é™¤å»
                for sup in li.find_all('sup'):
                    sup.decompose()
                
                # liå†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆpã‚¿ã‚°ç­‰ã«åŒ…ã¾ã‚Œã¦ã„ã¦ã‚‚OKï¼‰
                text = li.get_text(" ", strip=True)
                
                # ä¸è¦ãªè¨˜å·ã‚„ç©ºè¡Œã‚’æ•´ç†
                text = re.sub(r'\s+', ' ', text)
                if len(text) > 3: # æ¥µç«¯ã«çŸ­ã„ã‚´ãƒŸãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
                    specs_list.append(text)
    
    # ä¸‡ãŒä¸€ä¸Šè¨˜ã§å–ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¾“æ¥ã®ã‚¹ãƒšãƒƒã‚¯è¡¨ï¼‰
    if not specs_list:
        for row in soup.select('.product-specs tr, .system-specs tr, .techSpecs-table tr'):
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 2:
                specs_list.append(f"{cells[0].get_text(strip=True)}: {cells[1].get_text(strip=True)}")

    return " / ".join(list(dict.fromkeys(specs_list)))

def extract_image_url(soup, raw_content):
    """ç”»åƒURLç‰¹å®šãƒ­ã‚¸ãƒƒã‚¯"""
    image_patterns = re.findall(r'https://p[0-9]-ofp\.static\.pub/[^\s"\'<>]+?\.(?:png|jpg|jpeg)[^\s"\'<>]*', raw_content, re.IGNORECASE)
    if image_patterns:
        exclude = ["sustainability", "logo", "banner", "icon", "badge", "feature"]
        priority = ["/products/", "/product-img/", "420x420", "584x584", "400x400"]
        valid = [img.split('"')[0].split("'")[0] for img in image_patterns if not any(ex in img.lower() for ex in exclude)]
        for img in valid:
            if any(pri in img for pri in priority): return img
        if valid: return valid[0]
    return ""

def extract_price(soup, html_content):
    """ä¾¡æ ¼æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯"""
    for element in soup.find_all(['span', 'dd', 'div', 'p']):
        text = element.get_text()
        if 'è²©å£²ä¾¡æ ¼' in text:
            digits = re.sub(r'[^\d]', '', text)
            if not digits: digits = re.sub(r'[^\d]', '', element.parent.get_text())
            if digits and 100000 < int(digits) < 600000: return int(digits)
    
    prices = re.findall(r'Â¥\s?([0-9,]+)', html_content)
    valid = [int(p.replace(',', '')) for p in prices if 100000 < int(p.replace(',', '')) < 600000]
    return max(valid) if valid else 0

def scrape_detail_page(page, url):
    """å€‹åˆ¥è§£æãƒ»ä¿å­˜ãƒ­ã‚¸ãƒƒã‚¯"""
    print(f"ğŸ” å·¡å›ä¸­... {url}")
    try:
        unique_id = url.split('/')[-1]
        page.goto(url, wait_until="domcontentloaded", timeout=30000)
        
        # ã©ã¡ã‚‰ã‹ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼ãŒå‡ºã‚‹ã¾ã§å¾…æ©Ÿ
        try:
            page.wait_for_selector(".overview, .sph-o-overview", timeout=10000)
        except:
            pass
        
        page.evaluate("window.scrollBy(0, 500)")
        page.wait_for_timeout(2000)
        
        raw_content = page.content()
        soup = BeautifulSoup(raw_content, 'html.parser')
        
        price = extract_price(soup, raw_content)
        image_url = extract_image_url(soup, raw_content)
        specs_text = extract_specs(soup)
        
        save_data = {
            'unique_id': unique_id,
            'site_prefix': 'LEN',
            'maker': 'Lenovo',
            'raw_genre': 'laptop',
            'unified_genre': 'laptop',
            'name': page.title().split('|')[0].strip(),
            'price': price,
            'url': url,
            'image_url': image_url,
            'description': specs_text,
            'raw_html': raw_content,
            'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'å—æ³¨åœæ­¢',
            'is_active': True,
        }

        print_debug_info(save_data)
        PCProduct.objects.update_or_create(unique_id=unique_id, defaults=save_data)
        return True
            
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        return False

def extract_product_urls(page, list_url):
    page.goto(list_url, wait_until="domcontentloaded")
    page.wait_for_timeout(5000)
    hrefs = page.eval_on_selector_all('a[href*="/p/laptops/"]', 'elements => elements.map(e => e.href)')
    return list({url.split('#')[0].split('?')[0].rstrip('/') for url in hrefs})

def run_crawler():
    target_series = [
        "https://www.lenovo.com/jp/ja/c/laptops/thinkpad/thinkpad-x-series/",
        "https://www.lenovo.com/jp/ja/c/laptops/thinkpad/thinkpad-t-series/",
        "https://www.lenovo.com/jp/ja/c/laptops/thinkpad/thinkpad-l-series/",
        "https://www.lenovo.com/jp/ja/c/laptops/thinkpad/thinkpad-e-series/",
        "https://www.lenovo.com/jp/ja/c/laptops/thinkpad/thinkpad-p-series/",
        "https://www.lenovo.com/jp/ja/c/laptops/thinkpad/thinkpad-z-series/",
        "https://www.lenovo.com/jp/ja/c/laptops/yoga/yoga-2-in-1-series/",
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        page = context.new_page()
        
        all_product_urls = set()
        for series_url in target_series:
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­: {series_url}")
            try:
                urls = extract_product_urls(page, series_url)
                all_product_urls.update(urls)
            except Exception as e:
                print(f"  âŒ ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {e}")
        
        print(f"ğŸš€ åˆè¨ˆ {len(all_product_urls)}ä»¶ã‚’å‡¦ç†é–‹å§‹ã—ã¾ã™ã€‚")
        for i, url in enumerate(all_product_urls):
            print(f"\n[{i+1}/{len(all_product_urls)}]")
            scrape_detail_page(page, url)
            time.sleep(2)
            
        browser.close()

if __name__ == "__main__":
    run_crawler()