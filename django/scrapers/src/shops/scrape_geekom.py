import os
import django
import re
import hashlib
import time
import random
import json
import urllib.parse
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
# ç’°å¢ƒã«åˆã‚ã›ãŸåˆæœŸåŒ–
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models.pc_products import PCProduct

# ==========================================
# ğŸ”‘ 1. è¨­å®šæƒ…å ± (GEEKOM æ—¥æœ¬å…¬å¼ & A8.net)
# ==========================================
# A8.net ã®ãƒ™ãƒ¼ã‚¹URL
A8_BASE_URL = "https://px.a8.net/svt/ejp?a8mat=459XR1+CCSU76+5G4A+BW0YB&a8ejpredirect="
MAKER_NAME = "GEEKOM"
BASE_DOMAIN = "geekom.jp"

# ğŸ’¡ è§£æçµæœã«åŸºã¥ã„ãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLãƒªã‚¹ãƒˆ
TARGET_COLLECTIONS = [
    "https://geekom.jp/collections/intel",
    "https://geekom.jp/collections/amd-ryzen",
    "https://geekom.jp/pages/game-minipc",
    "https://geekom.jp/pages/office-minipc"
]

# ==========================================
# ğŸ› ï¸ 2. è§£æã‚¨ãƒ³ã‚¸ãƒ³
# ==========================================

def extract_detailed_specs(soup, product_name):
    """
    HTMLãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ CPU / GPU / RAM / SSD ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º
    """
    full_text = soup.get_text()
    desc_meta = soup.select_one('meta[name="description"]')
    meta_text = desc_meta['content'] if desc_meta else ""
    # ãƒ¡ã‚¿æƒ…å ±ã¨æœ¬æ–‡ã‚’çµ±åˆã—ã¦æ¤œç´¢å¯¾è±¡ã«ã™ã‚‹
    search_target = f"{product_name} {meta_text} {full_text}"

    specs = []

    # 1. CPU (Core Ultra, i9/i7/i5, Ryzen 9/7/5 ç­‰)
    cpu_pattern = r'((?:AMD\s?)?Ryzenâ„¢?\s?\d\s\d{4}[A-Z]{1,2}|(?:Intel\s?)?Coreâ„¢?\s?i\d-\d+[A-Z]?|(?:Intel\s?)?Ultra\s?\d\s\d{3}[A-Z]?)'
    cpu_match = re.search(cpu_pattern, search_target, re.I)
    specs.append(cpu_match.group(1).replace('â„¢', '').strip() if cpu_match else "CPUæœªç¢ºèª")

    # 2. GPU (RTX 4060, Radeon 780M, Iris Xe ç­‰)
    gpu_pattern = r'(RTX\s?\d{4}(?:\s?Ti)?|Radeon\s?\d{2,3}[A-Z]?|Iris\s?Xe|Intel\s?Graphics)'
    gpu_match = re.search(gpu_pattern, search_target, re.I)
    if gpu_match:
        specs.append(gpu_match.group(1).strip())
    elif "Mega Mini G1" in product_name:
        specs.append("RTX 4060")
    else:
        specs.append("å†…è”µã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹")

    # 3. ãƒ¡ãƒ¢ãƒª(RAM)
    ram_pattern = r'(\d{1,3}GB\s?(?:DDR\d|LPDDR\d|RAM))'
    ram_match = re.search(ram_pattern, search_target, re.I)
    specs.append(ram_match.group(1).strip() if ram_match else "RAMæœªç¢ºèª")

    # 4. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸(SSD)
    ssd_pattern = r'(\d{1,3}(?:GB|TB)\s?(?:SSD|NVMe|PCIe))'
    ssd_match = re.search(ssd_pattern, search_target, re.I)
    specs.append(ssd_match.group(1).strip() if ssd_match else "SSDæœªç¢ºèª")

    return " / ".join(specs)

def extract_correct_price(soup, product_data):
    """
    Shopifyç‰¹æœ‰ã®ã‚¯ãƒ©ã‚¹åãŠã‚ˆã³æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ­£ç¢ºãªè²©å£²ä¾¡æ ¼ã‚’æŠ½å‡º
    """
    price_selectors = [
        '.price-item--sale',                # ã‚»ãƒ¼ãƒ«ä¾¡æ ¼å„ªå…ˆ
        '.price__last .price-item',        # é€šå¸¸ä¾¡æ ¼
        '.product__price .price-item--sale',
        '.current-price'
    ]
    
    for selector in price_selectors:
        tag = soup.select_one(selector)
        if tag:
            digits = re.sub(r'[^\d]', '', tag.get_text())
            if digits and int(digits) > 1000:
                return int(digits)

    # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿(JSON-LD)ã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    offers = product_data.get('offers', {})
    if isinstance(offers, list): offers = offers[0]
    try:
        raw_price = int(float(offers.get('price', 0)))
        return raw_price if raw_price > 1000 else 0
    except:
        return 0

def scrape_geekom_page(page, url, current_index, total_count):
    """å€‹åˆ¥å•†å“ãƒšãƒ¼ã‚¸ã®è§£æã¨Djangoã¸ã®ä¿å­˜"""
    url_clean = url.split('?')[0].split('#')[0].rstrip('/')
    print(f"ğŸ” [{current_index + 1}/{total_count}] è§£æä¸­: {url_clean}")
    
    try:
        page.goto(url_clean, wait_until="domcontentloaded", timeout=60000)
        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆç”»åƒã‚„ä¾¡æ ¼ï¼‰ã‚’ãƒˆãƒªã‚¬ãƒ¼
        page.evaluate("window.scrollTo(0, 500)")
        page.wait_for_timeout(2000) 
        
        soup = BeautifulSoup(page.content(), 'html.parser')
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        product_data = {}
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                items = data if isinstance(data, list) else [data]
                for item in items:
                    if isinstance(item, dict) and item.get('@type') == 'Product':
                        product_data = item
                        break
            except: continue

        # è£½å“åå–å¾—
        name = product_data.get('name') or (soup.select_one('h1').get_text().strip() if soup.select_one('h1') else "ä¸æ˜ãªè£½å“")
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¢ã‚¯ã‚»ã‚µãƒªã‚„ä¿è¨¼ãƒšãƒ¼ã‚¸ã‚’é£›ã°ã™ï¼‰
        if any(word in name for word in ["é€æ–™", "ã‚¯ãƒ¼ãƒãƒ³", "ä¿è¨¼", "ä¿é™º", "ã‚®ãƒ•ãƒˆã‚«ãƒ¼ãƒ‰"]):
            print(f" â© ã‚¹ã‚­ãƒƒãƒ—: {name}")
            return False

        # ä¾¡æ ¼å–å¾—
        price = extract_correct_price(soup, product_data)
        
        # ç”»åƒURLå–å¾—
        meta_img = soup.select_one('meta[property="og:image"]')
        image_url = meta_img.get('content') if meta_img else ""
        if image_url.startswith('//'): image_url = "https:" + image_url

        # ã‚¹ãƒšãƒƒã‚¯æŠ½å‡º
        description = extract_detailed_specs(soup, name)
        
        # ğŸ”— A8.net ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆURLç”Ÿæˆ
        encoded_prod_url = urllib.parse.quote(url_clean, safe='')
        final_affiliate_url = f"{A8_BASE_URL}{encoded_prod_url}"

        # åœ¨åº«åˆ¤å®š
        offers = product_data.get('offers', {})
        if isinstance(offers, list): offers = offers[0]
        availability = offers.get('availability', '') if isinstance(offers, dict) else ""
        stock_status = 'åœ¨åº«ã‚ã‚Š' if "InStock" in availability and price > 0 else 'åœ¨åº«åˆ‡ã‚Œãƒ»äºˆç´„å—ä»˜ä¸­'

        # Djangoä¿å­˜å‡¦ç†
        unique_id = "geekom-" + hashlib.md5(url_clean.encode()).hexdigest()[:12]
        PCProduct.objects.update_or_create(
            unique_id=unique_id,
            defaults={
                'site_prefix': 'GEEKOM',
                'maker': MAKER_NAME,
                'name': name,
                'price': price,
                'url': url_clean,
                'affiliate_url': final_affiliate_url,
                'image_url': image_url,
                'description': description,
                'is_active': True,
                'stock_status': stock_status,
                'raw_genre': 'mini-pc',
                'unified_genre': 'mini-pc',
            }
        )
        print(f"   âœ… ä¿å­˜å®Œäº†: {name} (Â¥{price:,})")
        return True

    except Exception as e:
        print(f"   âŒ è§£æå¤±æ•—: {e}")
        return False

# ==========================================
# ğŸš€ 3. ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
# ==========================================

def run_geekom_crawler():
    """å…¨æŒ‡å®šURLã®å·¡å›å®Ÿè¡Œ"""
    with sync_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ç”»åƒèª­ã¿è¾¼ã¿ã‚’ã‚ªãƒ•ã«ã™ã‚‹è¨­å®šã‚‚å¯èƒ½ã ãŒã€ä»Šå›ã¯ç¢ºå®Ÿæ€§é‡è¦–ï¼‰
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        all_product_urls = set()

        print(f"ğŸ“‚ GEEKOM å…¨ã‚«ãƒ†ã‚´ãƒªå·¡å›é–‹å§‹...")
        
        for list_url in TARGET_COLLECTIONS:
            print(f"ğŸŒ å·¡å›ä¸­: {list_url}")
            try:
                page.goto(list_url, wait_until="networkidle", timeout=60000)
                # ãƒšãƒ¼ã‚¸å†…ã®ã™ã¹ã¦ã®aã‚¿ã‚°ã‹ã‚‰ /products/ ã‚’å«ã¿ã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒšãƒ¼ã‚¸ã§ã¯ãªã„ã‚‚ã®ã‚’æŠ½å‡º
                hrefs = page.evaluate('''() => {
                    return Array.from(document.querySelectorAll('a'))
                                .map(a => a.href)
                                .filter(href => href.includes('/products/') && !href.includes('/collections/'));
                }''')
                
                for h in hrefs:
                    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆã‚¯ã‚¨ãƒªå‰Šé™¤ï¼‰ã—ã¦ã‚»ãƒƒãƒˆã«è¿½åŠ ï¼ˆé‡è¤‡è‡ªå‹•æ’é™¤ï¼‰
                    clean_h = h.split('?')[0].split('#')[0].rstrip('/')
                    all_product_urls.add(clean_h)
                    
            except Exception as e:
                print(f"   âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {e}")

        # è§£æå¯¾è±¡ã®ç¢ºå®š
        product_urls = sorted(list(all_product_urls))
        print(f"ğŸ“Š è§£æå¯¾è±¡è£½å“æ•°: {len(product_urls)}ä»¶")

        # å€‹åˆ¥ãƒšãƒ¼ã‚¸è§£æãƒ«ãƒ¼ãƒ—
        for i, url in enumerate(product_urls):
            scrape_geekom_page(page, url, i, len(product_urls))
            # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è€ƒæ…®ã—ãŸãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
            time.sleep(random.uniform(2.0, 4.0))

        browser.close()
        print(f"\nâœ¨ GEEKOM å…¨è£½å“ãƒ‡ãƒ¼ã‚¿ã®åŒæœŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    run_geekom_crawler()