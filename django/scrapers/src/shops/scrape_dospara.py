import json
import csv
import os
import sys
from playwright.sync_api import sync_playwright

# ãƒ‘ã‚¹è§£æ±º
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../"))
if project_root not in sys.path:
    sys.path.append(project_root)

def scrape_dospara_api():
    output_dir = os.path.join(project_root, "data", "raw")
    os.makedirs(output_dir, exist_ok=True)
    output_csv = os.path.join(output_dir, "dospara_products_api.csv")
    
    target_url = "https://www.dospara.co.jp/TC143"
    captured_data = []

    print(f"ğŸš€ ãƒ‰ã‚¹ãƒ‘ãƒ©ã®è£å´é€šä¿¡ã‚’è§£æä¸­: {target_url}")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        # ã€é‡è¦ã€‘é€šä¿¡ã‚’ç›£è¦–ã—ã€JSONãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒãƒã™ã‚‹
        def handle_response(response):
            # ãƒ‰ã‚¹ãƒ‘ãƒ©ã®è£½å“ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹APIã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆèª¿æŸ»æ¸ˆã¿ï¼‰
            if "search" in response.url and "json" in response.url or "/api/" in response.url:
                try:
                    data = response.json()
                    # éšå±¤æ§‹é€ ã¯ã‚µã‚¤ãƒˆã«ã‚ˆã‚Šç•°ãªã‚Šã¾ã™ãŒã€é€šå¸¸ã¯ 'items' ã‚„ 'products' ã«å…¥ã£ã¦ã„ã¾ã™
                    if isinstance(data, dict):
                        # ã“ã“ã§ãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’æŠ½å‡ºï¼ˆãƒ‰ã‚¹ãƒ‘ãƒ©ã®æ§‹é€ ã«åˆã‚ã›ã‚‹ï¼‰
                        # â€»ãƒ‡ãƒãƒƒã‚°ç”¨ã«å–å¾—ã—ãŸJSONã®ã‚­ãƒ¼ã‚’è¡¨ç¤º
                        items = data.get('data', {}).get('items', [])
                        for item in items:
                            captured_data.append({
                                'name': item.get('name'),
                                'price': item.get('price'),
                                'url': f"https://www.dospara.co.jp/5shopping/detail.php?it={item.get('id')}",
                                'spec': item.get('description', '')
                            })
                except:
                    pass

        page.on("response", handle_response)

        try:
            # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€APIãŒå©ã‹ã‚Œã‚‹ã®ã‚’å¾…ã¤
            page.goto(target_url, wait_until="networkidle", timeout=60000)
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¿½åŠ èª­ã¿è¾¼ã¿ã‚’ç™ºç”Ÿã•ã›ã‚‹
            page.mouse.wheel(0, 2000)
            page.wait_for_timeout(5000)

            # ä¸‡ãŒä¸€APIã‚­ãƒ£ãƒƒãƒã«å¤±æ•—ã—ãŸå ´åˆã®äºˆå‚™ç­–ï¼ˆJavaScriptã‹ã‚‰ç›´æ¥å¤‰æ•°ã‚’æŠœãï¼‰
            if not captured_data:
                print("âš ï¸ é€šä¿¡å‚å—ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒšãƒ¼ã‚¸å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥æŠ½å‡ºã—ã¾ã™...")
                raw_items = page.evaluate("() => window.__NEXT_DATA__?.props?.pageProps?.products || []")
                for item in raw_items:
                    captured_data.append({
                        'name': item.get('name'),
                        'price': item.get('price'),
                        'url': item.get('url'),
                        'spec': item.get('spec')
                    })

            # CSVæ›¸ãå‡ºã—
            if captured_data:
                with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    writer.writerow(['category', 'name', 'price', 'url', 'description'])
                    for p in captured_data:
                        if p['name']:
                            writer.writerow(['galleria', p['name'], p['price'], p['url'], p['spec']])
                
                print(f"âœ¨ æˆåŠŸï¼ {len(captured_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’è£å´ã‹ã‚‰å–å¾—ã—ã¾ã—ãŸã€‚")
            else:
                print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µã‚¤ãƒˆãŒå³é‡ã«ãƒ—ãƒ­ãƒ†ã‚¯ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚")

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        finally:
            browser.close()

if __name__ == "__main__":
    scrape_dospara_api()