import os
import django
import requests
from bs4 import BeautifulSoup
import hashlib
import time
import re
import urllib.parse
import html

# --- Djangoç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models.pc_products import PCProduct

MAKER_NAME = "Sycom"
SITE_PREFIX = "SYCOM"
A8_BASE_URL = "https://px.a8.net/svt/ejp?a8mat=2ZCPLP+CO42OY+34WQ+BW8O2&a8ejpredirect="
FIXED_IMAGE_URL = "https://www.sycom.co.jp/custom/files_rs01/RaptorLake-S-Refresh/G-Master_Spear_B760-D4/gallery/001.jpg"

def clean_html_tags(text):
    """HTMLã‚¿ã‚°ã¨ä¸è¦ãªç©ºç™½ã‚’å¾¹åº•çš„ã«é™¤å»"""
    if not text: return ""
    text = re.sub(r'<[^>]+>', '', text) # ã‚¿ã‚°é™¤å»
    text = html.unescape(text)
    text = re.sub(r'[\r\n\t\xa0]+', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

def clean_name(text):
    if not text: return ""
    text = clean_html_tags(text)
    text = re.sub(r'â˜….*?è¿„\)', '', text)
    text = text.replace("BTO ãƒ‘ã‚½ã‚³ãƒ³(PC)ã®ã€@Sycomã€‘(ã‚µã‚¤ã‚³ãƒ )", "")
    return text.strip().strip("ï½œ")

def run_sycom_crawler():
    target_urls = [
        "https://www.sycom.co.jp/bto/game_pc/",
        "https://www.sycom.co.jp/bto/middle_tower/",
        "https://www.sycom.co.jp/ranking/",
        "https://www.sycom.co.jp/bto/dual_water_cooling/",
        "https://www.sycom.co.jp/bto/silent_pc/"
    ]
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
    KW_RE = re.compile(r'(CPU|ãƒã‚¶ãƒ¼ãƒœãƒ¼ãƒ‰|ãƒ¡ãƒ¢ãƒª|ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸|ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯|OS|å¤–å½¢å¯¸æ³•|é›»æº|ã‚±ãƒ¼ã‚¹|ãƒ•ã‚©ãƒ¼ãƒ ãƒ•ã‚¡ã‚¯ã‚¿|ãƒ“ãƒ‡ã‚ªã‚«ãƒ¼ãƒ‰|SSD|æ¨™æº–)')

    print(f"\nğŸš€ {SITE_PREFIX} æ¨™æº–æ§‹æˆãƒ»å®Œå…¨è£œå®Œè§£æãƒ¢ãƒ¼ãƒ‰èµ·å‹•...")

    # 1. ãƒªãƒ³ã‚¯åé›†
    product_urls = []
    for start_url in target_urls:
        try:
            res = requests.get(start_url, headers=headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            for a in soup.find_all('a', href=re.compile(r'/custom/model\?no=\d+')):
                url = urllib.parse.urljoin("https://www.sycom.co.jp", a.get('href'))
                if url not in product_urls: product_urls.append(url)
        except: continue

    print(f"âœ… è§£æå¯¾è±¡: {len(product_urls)} ä»¶\n" + "="*70)

    total_saved = 0
    for p_url in product_urls:
        try:
            time.sleep(1.0)
            res = requests.get(p_url, headers=headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            name = clean_name(soup.title.string.split('ï½œ')[0] if soup.title else "Sycom PC")
            
            specs = []

            # --- æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼šã‚µã‚¤ã‚³ãƒ ã®ã€Œæ¨™æº–æ§‹æˆã€ã‚¨ãƒªã‚¢ã‚’ç¶²ç¾… ---
            # æˆ¦ç•¥1: tableå†…ã®th/tdæ§‹é€ ã‚’ç‹™ã†
            for table in soup.find_all('table'):
                for row in table.find_all('tr'):
                    cells = row.find_all(['th', 'td'])
                    if len(cells) >= 2:
                        label = clean_html_tags(cells[0].get_text())
                        value = clean_html_tags(cells[1].get_text())
                        if KW_RE.search(label) or "æ¨™æº–æ§‹æˆ" in label:
                            # å¤‰æ›´ãƒœã‚¿ãƒ³ãªã©ã®ã‚´ãƒŸã‚’æ’é™¤
                            val_clean = value.split("è©³ç´°ã¯ã“ã¡ã‚‰")[0].split("å¤‰æ›´ã™ã‚‹")[0].strip()
                            if len(val_clean) > 2:
                                specs.append(f"ã€{label}ã€‘{val_clean}")

            # æˆ¦ç•¥2: dl/dt/dd æ§‹é€ ï¼ˆRadiantã‚·ãƒªãƒ¼ã‚ºç­‰ï¼‰ã‚’ç‹™ã†
            for dl in soup.find_all('dl'):
                dt = dl.find('dt')
                dd = dl.find('dd')
                if dt and dd:
                    label = clean_html_tags(dt.get_text())
                    value = clean_html_tags(dd.get_text())
                    if KW_RE.search(label):
                        specs.append(f"ã€{label}ã€‘{value.strip()}")

            # æˆ¦ç•¥3: spec_list ãªã©ã®ç®‡æ¡æ›¸ãã‚’ç‹™ã†
            for li in soup.select('.spec_list li, .standard_spec li, .spec_box li'):
                txt = clean_html_tags(li.get_text())
                if 'ï¼š' in txt or ':' in txt:
                    if KW_RE.search(txt): specs.append(txt)

            unique_specs = list(dict.fromkeys(specs))
            
            # --- ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ› ---
            print(f"ğŸ“¦ [MODEL] {name}")
            if unique_specs:
                for s in unique_specs:
                    print(f"   âœ¨ {s}")
            else:
                print(f"   âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå¤±æ•— -> ãƒšãƒ¼ã‚¸å†…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…¨ä»¶æ•‘æ¸ˆã‚’è©¦ã¿ã¾ã™")
                # æœ€çµ‚æ‰‹æ®µï¼šãƒ†ã‚­ã‚¹ãƒˆä¸­ã«ã€ŒCPUï¼šã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚Œã°å…¨éƒ¨æ‹¾ã†
                page_text = soup.get_text()
                for line in page_text.split('\n'):
                    line = clean_html_tags(line)
                    if KW_RE.search(line) and ('ï¼š' in line or ':' in line):
                        if 10 < len(line) < 150: # é©åˆ‡ãªé•·ã•ã®ã‚‚ã®ã ã‘
                            print(f"   âœ¨ (è£œå®Œ) {line}")
                            specs.append(line)
            
            description = "\n".join(list(dict.fromkeys(specs)))

            # --- DBä¿å­˜ ---
            uid = "sycom-" + hashlib.md5(p_url.encode()).hexdigest()[:12]
            PCProduct.objects.update_or_create(
                unique_id=uid,
                defaults={
                    'site_prefix': SITE_PREFIX, 'maker': MAKER_NAME, 'name': name,
                    'price': 0, 'url': p_url, 'image_url': FIXED_IMAGE_URL,
                    'affiliate_url': f"{A8_BASE_URL}{urllib.parse.quote(p_url)}",
                    'description': description, 'is_active': True, 'stock_status': "è¦åœ¨åº«ç¢ºèª"
                }
            )
            print(f"âœ… DBåŒæœŸå®Œäº†: {uid}\n" + "-"*60)
            total_saved += 1

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ ({p_url}): {e}")

    print(f"\nâœ¨ ãƒŸãƒƒã‚·ãƒ§ãƒ³å®Œäº†ï¼ åˆè¨ˆ {total_saved} ä»¶ã®æ§‹æˆã‚’å®Œå…¨ã«è£œå®Œã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    run_sycom_crawler()