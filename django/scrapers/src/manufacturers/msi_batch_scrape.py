import time
import csv
import random
import os
import subprocess
from playwright.sync_api import sync_playwright

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯é–¢æ•° (ã„ã˜ã£ã¦ã„ã¾ã›ã‚“) ---
def get_category_from_name(name):
    """å•†å“åã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¥µé™ã¾ã§è©³ç´°ã«åˆ¤å®šã™ã‚‹"""
    n = name.upper()
    
    # 1. ãƒžã‚¶ãƒ¼ãƒœãƒ¼ãƒ‰
    mb_keywords = [
        "B860", "B850", "X870", "Z890", "Z790", "B760", "B650", "X670", "B550", "A620",
        "MORTAR", "TOMAHAWK", "CARBON", "WIFI", "PRO B", "PRO Z", "PRO H", "PRO A", "BAZOOKA", 
        "GODLIKE", "ACE", "UNIFY", "GAMING PLUS", "PRO-VDH", "H370", "B360", "H270", "Z370",
        "èƒŒé¢ã‚³ãƒã‚¯ã‚¿å¯¾å¿œ"
    ]
    if any(k in n for k in mb_keywords):
        return "Motherboard"
    
    # 2. é›»æº (PSU)
    psu_keywords = [
        "é›»æº", "UNIT", "PSU", "A850", "A750", "A650", "A1000", "A1250", "GL", "GS", "BNL", "PCIE5", "GOLD"
    ]
    if any(k in n for k in psu_keywords) and ("W" in n or any(d in n for d in ["550", "650", "750", "850", "1000"])):
        return "PSU"

    # 3. ãƒ¢ãƒ‹ã‚¿ãƒ¼
    monitor_keywords = [
        "ã‚¤ãƒ³ãƒ", "ã‚²ãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ‹ã‚¿ãƒ¼", "HZ", "DISPLAY", "æ¶²æ™¶", "ãƒ¢ãƒ‹ã‚¿ãƒ¼", "QD-OLED", "æ¹¾æ›²", "G24", "G27", "G32"
    ]
    if any(k in n for k in monitor_keywords) and not any(k in n for k in ["ãƒŽãƒ¼ãƒˆ", "SUMMIT", "CLAW"]):
        return "Monitor"

    # 4. ãƒŽãƒ¼ãƒˆPC
    notebook_keywords = [
        "ãƒŽãƒ¼ãƒˆ", "STEALTH", "CYBORG", "PRESTIGE", "KATANA", "RAIDER", "VECTOR", "SUMMIT", "MODERN", "CLAW"
    ]
    if any(k in n for k in notebook_keywords):
        return "Notebook"

    # 5. å‘¨è¾ºæ©Ÿå™¨
    peripheral_keywords = [
        "ãƒžã‚¦ã‚¹", "ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰", "ãƒ˜ãƒƒãƒ‰ã‚»ãƒƒãƒˆ", "CLUTCH", "VIGOR", "VERSA", "GK30", "GK320", "CONTROLLER", "MOUSE"
    ]
    if any(k in n for k in peripheral_keywords):
        return "Peripheral"

    # 6. ã‚¯ãƒ¼ãƒ©ãƒ¼ãƒ»ãƒ•ã‚¡ãƒ³
    cooler_keywords = [
        "LIQUID", "æ°´å†·", "ã‚¯ãƒ¼ãƒ©ãƒ¼", "CORELIQUID", "COREFROZR", "SILENT GALE", "P12", "F12"
    ]
    if any(k in n for k in cooler_keywords):
        return "Cooler"

    # 7. ã‚±ãƒ¼ã‚¹
    case_keywords = [
        "FORGE", "VELOX", "PANO", "CASE", "GUNGNIR", "PROSPECT", "ã‚±ãƒ¼ã‚¹", "CHASSIS"
    ]
    if any(k in n for k in case_keywords):
        return "Case"

    # 8. ãƒ“ãƒ‡ã‚ªã‚«ãƒ¼ãƒ‰
    gpu_keywords = [
        "GEFORCE", "RTX", "GTX", "VENTUS", "SUPRIM", "GT 710", "GT 1030", "GRAPHICS CARD", "AERO ITX"
    ]
    if any(k in n for k in gpu_keywords):
        return "Graphics Card"

    return "Other"

# --- ðŸ’¡ æ–°è¨­: Dockerã¸ã®è‡ªå‹•åæ˜ ãƒ­ã‚¸ãƒƒã‚¯ ---
def run_docker_import_msi(csv_path):
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†å¾Œã€è‡ªå‹•ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã€
    Djangoã®MSIã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒžãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹
    """
    base_dir = os.path.dirname(os.path.abspath(__file__))
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¸é¡ã‚‹
    project_root = os.path.abspath(os.path.join(base_dir, "..", ".."))
    
    container_name = "api_django_v2" 
    container_csv_path = "/usr/src/app/scrapers/tsukumo_msi_products.csv"

    print("\n" + "="*40)
    print("ðŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®è‡ªå‹•åæ˜ ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆMSIï¼‰")
    print("="*40)
    
    try:
        # 1. docker cp ã§ã‚³ãƒ³ãƒ†ãƒŠã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é€ã‚‹
        copy_cmd = ["docker", "cp", csv_path, f"{container_name}:{container_csv_path}"]
        subprocess.run(copy_cmd, check=True)
        print(f"ðŸ“‚ [Step 1/2] æœ€æ–°CSVã‚’ã‚³ãƒ³ãƒ†ãƒŠã¸ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚")

        # 2. docker compose exec ã§ Djangoã‚³ãƒžãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
        import_cmd = [
            "docker", "compose", "-f", "docker-compose.stg.yml",
            "exec", "django-v2", "python", "manage.py", "import_tsukumo_msi"
        ]
        
        result = subprocess.run(
            import_cmd, 
            check=True, 
            text=True, 
            capture_output=True, 
            cwd=project_root,
            encoding='utf-8'
        )
        
        print(f"ðŸš€ [Step 2/2] Djangoã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ãŒæˆåŠŸã—ã¾ã—ãŸã€‚")
        print("-" * 40)
        print(f"ðŸ“‹ Djangoã‹ã‚‰ã®å ±å‘Š:\n{result.stdout.strip()}")
        print("-" * 40)

    except subprocess.CalledProcessError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e.stderr}")
    except Exception as e:
        print(f"âš ï¸ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° (åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’æ´»ã‹ã—ãŸã¾ã¾æ§‹é€ ç¶­æŒ) ---
def scrape_tsukumo_msi():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    output_csv = os.path.join(base_dir, "tsukumo_msi_products.csv")

    with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['category', 'name', 'price', 'url', 'image_url'])

    base_url_template = "https://shop.tsukumo.co.jp/search/p{}/?maker_id[]=7089&end_of_sales=1&keyword=MSI"
    all_processed_urls = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False) 
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()

        page_num = 1
        while True:
            url = base_url_template.format(page_num)
            print(f"ðŸ“„ ãƒšãƒ¼ã‚¸ {page_num} ã‚’æœ€çµ‚ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
            
            try:
                page.goto(url, wait_until="domcontentloaded", timeout=60000)
                page.mouse.wheel(0, 1500)
                page.wait_for_timeout(3000)

                links = page.query_selector_all("a[href*='/goods/']")
                if not links: break

                new_items_in_page = 0
                for link in links:
                    try:
                        href = link.get_attribute("href")
                        if not href: continue
                        full_url = "https://shop.tsukumo.co.jp" + href if href.startswith("/") else href
                        if full_url in all_processed_urls: continue

                        parent = link.evaluate_handle("el => el.closest('li') || el.closest('.item') || el.parentElement.parentElement")
                        
                        raw_name = link.inner_text().strip().split('\n')[-1]
                        if not raw_name or len(raw_name) < 5: continue
                        display_name = raw_name if raw_name.startswith("MSI") else f"MSI {raw_name}"

                        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
                        category = get_category_from_name(display_name)

                        # ä¾¡æ ¼å–å¾—
                        price_el = parent.query_selector(".product-list__price-main, [class*='price'], b")
                        price = 0
                        if price_el:
                            price_val = "".join(filter(str.isdigit, price_el.inner_text()))
                            price = int(price_val) if price_val else 0

                        # ç”»åƒå–å¾—
                        img_el = parent.query_selector("img")
                        image_url = img_el.get_attribute("src") if img_el else ""

                        with open(output_csv, 'a', newline='', encoding='utf-8-sig') as f:
                            writer = csv.writer(f)
                            writer.writerow([category, display_name, price, full_url, image_url])

                        all_processed_urls.add(full_url)
                        new_items_in_page += 1
                    except:
                        continue

                print(f"âœ… ãƒšãƒ¼ã‚¸ {page_num} å®Œäº†ï¼ˆç´¯è¨ˆ: {len(all_processed_urls)}ä»¶ï¼‰")
                if new_items_in_page == 0: break
                page_num += 1
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
                break

        browser.close()
    
    print(f"âœ¨ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼ CSV: {output_csv}")
    
    # ðŸ’¡ æœ€å¾Œã«Dockerã¸ã®åæ˜ å‡¦ç†ã‚’å®Ÿè¡Œ
    run_docker_import_msi(output_csv)

if __name__ == "__main__":
    scrape_tsukumo_msi()