import time
import csv
import random
import os
import subprocess # ğŸ’¡ è¿½åŠ 
from playwright.sync_api import sync_playwright

def run_docker_import(csv_path):
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†å¾Œã€è‡ªå‹•ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã€
    Djangoã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹
    """
    base_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(base_dir, "..", ".."))
    
    # è¨­å®šæƒ…å ±
    container_name = "api_django_v2"
    container_csv_path = "/usr/src/app/acer_detailed_final.csv"

    print("\n" + "="*40)
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®è‡ªå‹•åæ˜ ã‚’é–‹å§‹ã—ã¾ã™")
    print("="*40)
    
    try:
        # 1. docker cp ã§ã‚³ãƒ³ãƒ†ãƒŠã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é€ã‚‹
        copy_cmd = ["docker", "cp", csv_path, f"{container_name}:{container_csv_path}"]
        subprocess.run(copy_cmd, check=True)
        
        # ğŸ’¡ ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ã¾ã—ãŸ
        print(f"ğŸ“‚ [Step 1/2] Windowså´ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Dockerã‚³ãƒ³ãƒ†ãƒŠ({container_name}) ã®ã‚·ã‚§ãƒ«å†…ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸã€‚")

        # 2. docker compose exec ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®Ÿè¡Œ
        import_cmd = [
            "docker", "compose", "-f", "docker-compose.stg.yml",
            "exec", "django-v2", "python", "manage.py", "import_acer"
        ]
        
        result = subprocess.run(
            import_cmd, 
            check=True, 
            text=True, 
            capture_output=True, 
            cwd=project_root,
            encoding='utf-8'
        )
        
        # ğŸ’¡ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print(f"ğŸš€ [Step 2/2] Dockerã‚·ã‚§ãƒ«å†…ã§ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ãŒæˆåŠŸã—ã¾ã—ãŸã€‚")
        print("-" * 40)
        print(f"ğŸ“‹ Djangoã‹ã‚‰ã®å ±å‘Š:\n{result.stdout.strip()}")
        print("-" * 40)
        print("\nâœ¨ ã™ã¹ã¦ã®å·¥ç¨‹ãŒæ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")

    except subprocess.CalledProcessError as e:
        print(f"âŒ Dockerã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e.stderr}")
    except Exception as e:
        print(f"âš ï¸ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# (ã“ã‚Œã‚ˆã‚Šä¸Šã® scrape_acer_to_csv_realtime é–¢æ•°ãªã©ã¯å¤‰æ›´ãªã—)

def scrape_acer_to_csv_realtime():
    # 1. ä¿å­˜ãƒ‘ã‚¹ã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«å›ºå®š
    base_dir = os.path.dirname(os.path.abspath(__file__))
    output_csv = os.path.join(base_dir, "acer_products_final.csv")
    
    # ğŸ’¡ å·¡å›ã™ã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã®ãƒªã‚¹ãƒˆï¼ˆã‚«ãƒ†ã‚´ãƒªåã¨ãƒ™ãƒ¼ã‚¹URLã®ãƒšã‚¢ï¼‰
    targets = [
        {"category": "Notebook", "url": "https://store.acer.com/ja-jp/notebooks?p="},
        {"category": "Monitor", "url": "https://store.acer.com/ja-jp/monitors?p="},
        {"category": "Desktops", "url": "https://store.acer.com/ja-jp/desktops?p="},
        {"category": "Peripheral", "url": "https://store.acer.com/ja-jp/peripheral?p="},
    ]
    
    total_count = 0

    # CSVã‚’æ–°è¦ä½œæˆã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã‚€ï¼ˆcategoryã‚’è¿½åŠ ï¼‰
    with open(output_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['category', 'name', 'price', 'url', 'image_url', 'description'])

    with sync_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹è§£é™¤ + é€šä¿¡ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
        browser = p.chromium.launch(
            headless=False, 
            args=['--disable-http2', '--disable-blink-features=AutomationControlled']
        )
        
        context = browser.new_context(
            viewport={'width': 1280, 'height': 900},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = context.new_page()

        for target in targets:
            category_name = target["category"]
            base_url = target["url"]
            last_page_data = set()
            page_num = 1
            
            print(f"\nğŸš€ {category_name} ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")

            while True:
                current_url = f"{base_url}{page_num}"
                print(f"ğŸ“„ [{category_name}] ãƒšãƒ¼ã‚¸ {page_num} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                
                try:
                    page.goto(current_url, wait_until="commit", timeout=90000)
                    page.wait_for_timeout(5000) # ã‚µã‚¤ãƒˆã®é‡ã•å¯¾ç­–

                    # å•†å“ãƒªã‚¹ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
                    try:
                        page.wait_for_selector(".product-item-info", state="attached", timeout=15000)
                    except:
                        print(f"ğŸ {category_name} ã®å…¨ãƒšãƒ¼ã‚¸ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")
                        break

                    # LazyLoadå¯¾ç­–ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                    for _ in range(8):
                        page.evaluate("window.scrollBy(0, 700)")
                        time.sleep(0.4)
                    
                    page.wait_for_timeout(2000)
                    items = page.query_selector_all(".product-item-info")
                    current_page_names = []

                    for item in items:
                        try:
                            name_el = item.query_selector(".product-item-name a")
                            price_el = item.query_selector("span.price")
                            
                            if name_el and price_el:
                                name = name_el.inner_text().strip()
                                item_url = name_el.get_attribute("href") or ""
                                
                                # ç”»åƒå–å¾—
                                img_el = item.query_selector(".product-image-photo")
                                image_url = ""
                                if img_el:
                                    img_el.scroll_into_view_if_needed()
                                    for attr in ["data-src", "src", "data-original"]:
                                        candidate = img_el.get_attribute(attr)
                                        if candidate and "pixel.jpg" not in candidate and candidate.startswith("http"):
                                            image_url = candidate
                                            break
                                
                                if not image_url:
                                    continue

                                # ä¾¡æ ¼ã¨èª¬æ˜
                                price_text = price_el.inner_text()
                                price = int(price_text.replace('Â¥', '').replace(',', '').replace(' ', '').replace('ã€€', ''))
                                desc_el = item.query_selector(".product-item-details .description") or item.query_selector(".product-item-details")
                                description = desc_el.inner_text().replace('\n', ' / ').strip() if desc_el else ""

                                current_page_names.append(name)

                                # 1ä»¶ãšã¤CSVã¸è¿½è¨˜
                                with open(output_csv, 'a', newline='', encoding='utf-8') as f:
                                    writer = csv.writer(f)
                                    writer.writerow([category_name, name, price, item_url, image_url, description])
                                
                                total_count += 1
                        except:
                            continue
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    current_page_set = set(current_page_names)
                    if not current_page_names or (last_page_data and current_page_set.issubset(last_page_data)):
                        print(f"ğŸ {category_name} ã®é‡è¤‡ã‚’æ¤œçŸ¥ã—ãŸãŸã‚æ¬¡ã®ã‚«ãƒ†ã‚´ãƒªã¸ã€‚")
                        break

                    last_page_data = current_page_set
                    print(f"âœ… {category_name} ãƒšãƒ¼ã‚¸ {page_num} å®Œäº†ï¼ˆç´¯è¨ˆ: {total_count}ä»¶ï¼‰")
                    
                    page_num += 1
                    time.sleep(random.uniform(3.0, 5.0))

                except Exception as e:
                    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                    break

        browser.close()
    
    print(f"\nâœ¨ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼CSVãƒ•ã‚¡ã‚¤ãƒ«: {output_csv}")
    
    # ğŸ’¡ ã“ã“ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–¢æ•°ã‚’å‘¼ã³å‡ºã™
    run_docker_import(output_csv)

if __name__ == "__main__":
    scrape_acer_to_csv_realtime()