import time
import csv
import random
import os
import subprocess
import requests
import xml.etree.ElementTree as ET
from playwright.sync_api import sync_playwright

def get_product_urls_from_sitemap():
    sitemap_url = "https://www.stormst.com/sitemap_product_1.xml"
    print(f"ğŸ” ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰æœ€æ–°ã®å•†å“ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
    try:
        response = requests.get(sitemap_url)
        root = ET.fromstring(response.content)
        urls = [loc.text for loc in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc")]
        print(f"âœ… {len(urls)}ä»¶ã®å•†å“URLã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
        return urls
    except Exception as e:
        print(f"âŒ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—è§£æå¤±æ•—: {e}")
        return []

def run_docker_import_storm(csv_path):
    """
    ç”Ÿæˆã•ã‚ŒãŸCSVã‚’Dockerã‚³ãƒ³ãƒ†ãƒŠã«ã‚³ãƒ”ãƒ¼ã—ã€Djangoã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹
    """
    container_name = "api_django_v2"  # ã‚³ãƒ³ãƒ†ãƒŠå
    # ã‚³ãƒ³ãƒ†ãƒŠå†…ã®ä¿å­˜å…ˆãƒ‘ã‚¹ï¼ˆé©å®œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹æˆã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰
    container_csv_path = "/usr/src/app/scrapers/storm_products.csv"
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

    print(f"ğŸš€ Dockerã‚³ãƒ³ãƒ†ãƒŠ '{container_name}' ã¸ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ä¸­...")
    try:
        # 1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒ†ãƒŠã«ã‚³ãƒ”ãƒ¼ (docker cp)
        subprocess.run(["docker", "cp", csv_path, f"{container_name}:{container_csv_path}"], check=True)
        print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼å®Œäº†: {container_csv_path}")

        # 2. Djangoã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
        # â€» docker-compose exec ã‚’ä½¿ç”¨ã™ã‚‹ä¾‹
        print(f"âš™ï¸  Djangoã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œä¸­...")
        import_cmd = [
            "docker", "compose", "-f", "docker-compose.stg.yml", 
            "exec", "django-v2", 
            "python", "manage.py", "import_storm"
        ]
        
        result = subprocess.run(
            import_cmd, 
            check=True, 
            text=True, 
            capture_output=True, 
            cwd=project_root, 
            encoding='utf-8'
        )
        print(f"ğŸ’¡ Djangoå ±å‘Š:\n{result.stdout.strip()}")

    except subprocess.CalledProcessError as e:
        print(f"âŒ Dockeræ“ä½œã‚¨ãƒ©ãƒ¼: {e}")
        if e.stderr:
            print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e.stderr}")
    except Exception as e:
        print(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")

def scrape_storm():
    urls = get_product_urls_from_sitemap()
    if not urls: return

    # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«CSVã‚’å‡ºåŠ›
    output_csv = os.path.join(os.path.dirname(__file__), "storm_products.csv")
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®åˆæœŸåŒ–
    with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['category', 'name', 'price', 'url', 'image_url', 'description'])

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0")
        page = context.new_page()

        success_count = 0
        for i, url in enumerate(urls, 1):
            try:
                print(f"ğŸ“¦ [{i}/{len(urls)}] è§£æä¸­: {url}")
                page.goto(url, wait_until="domcontentloaded", timeout=60000)

                # metaæƒ…å ±ã®æŠ½å‡º
                meta_data = page.evaluate("""() => {
                    const getMeta = (prop) => document.querySelector(`meta[property="${prop}"]`)?.getAttribute('content');
                    return {
                        name: getMeta('og:title'),
                        price: getMeta('product:price:amount'),
                        image: getMeta('og:image')
                    };
                }""")

                # å•†å“åã®åŠ å·¥ ([STORM] ã‚’ä»˜ä¸)
                raw_name = meta_data['name'] or "Unknown Name"
                name = f"[STORM] {raw_name}"
                
                price = int(meta_data['price']) if meta_data['price'] and meta_data['price'].isdigit() else 0
                image_url = meta_data['image'] or ""

                # ã‚¹ãƒšãƒƒã‚¯æƒ…å ±ã¯bodyã‹ã‚‰å–å¾—
                desc_el = page.query_selector(".ec-productRole__description")
                description = ""
                if desc_el:
                    description = " / ".join([l.strip() for l in desc_el.inner_text().splitlines() if l.strip()])

                # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
                category = "Notebook" if "ãƒãƒ¼ãƒˆ" in raw_name or "Laptop" in raw_name else "Desktop"

                # CSVã¸è¿½è¨˜
                with open(output_csv, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    writer.writerow([category, name, price, url, image_url, description])
                
                success_count += 1
                time.sleep(random.uniform(0.3, 0.8))

            except Exception as e:
                print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ— ({url}): {e}")
                continue

        browser.close()
    
    print(f"\nâœ¨ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼ (æˆåŠŸ: {success_count}/{len(urls)} ä»¶)")
    
    # å…¨ä»¶å–å¾—å®Œäº†å¾Œã«Dockerã¸ã‚³ãƒ”ãƒ¼ & ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    run_docker_import_storm(output_csv)

if __name__ == "__main__":
    scrape_storm()