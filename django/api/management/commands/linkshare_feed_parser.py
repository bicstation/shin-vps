import os
import re
import ftplib
import time
import gzip
import csv
import traceback
import sys
import subprocess
from datetime import datetime, timezone
from typing import List, Tuple, Dict, Any, Optional, Set
from decimal import Decimal, InvalidOperation
import math 

# Djangoã®ã‚³ã‚¢æ©Ÿèƒ½ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, IntegrityError 
from django.utils import timezone
from django.conf import settings 

# ğŸš¨ ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ãƒŸãƒ¼å®šç¾© (handleå®Ÿè¡Œå‰ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§ã®NameErrorã‚’å›é¿ã™ã‚‹ãŸã‚)
class DummyModel:
    """handleå®Ÿè¡Œå‰ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§ã®NameErrorã‚’å›é¿ã™ã‚‹ãŸã‚ã®ãƒ€ãƒŸãƒ¼å®šç¾©"""
    objects = None
    def __init__(self, **kwargs): pass
    # LinkshareProduct ãŒæŒã¤ã“ã¨ãŒæƒ³å®šã•ã‚Œã‚‹å±æ€§ã‚’è¿½åŠ 
    id = None
    merchant_id = None
    created_at = None
    updated_at = None
    sku_unique = None 
    price = None
    in_stock = None
    is_active = None
    affiliate_url = None 
    sku = None
    product_url = None
    raw_csv_data = None 
    merchant_name = None 
    product_name = None # ğŸ‘ˆ æ–°ã—ãDBã«è¿½åŠ ã•ã‚ŒãŸãŸã‚ã€ãƒ€ãƒŸãƒ¼ã«ã‚‚è¿½åŠ 

LinkshareProduct = DummyModel

# ==============================================================================
# æ¥ç¶šãƒ»ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š (å®šæ•°)
# ==============================================================================
# ç’°å¢ƒå¤‰æ•°ãŒãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
FTP_HOST = os.getenv("LINKSHARE_FTP_HOST", "aftp.linksynergy.com")
FTP_USER = os.getenv("LINKSHARE_BS_USER", "rkp_3750988") 
FTP_PASS = os.getenv("LINKSHARE_BS_PASS", "u5NetPVZEAhABD7HuW2VRymP") 
FTP_PORT = 21
FTP_TIMEOUT = 180

MAX_SIZE_BYTES = 1073741824 # 1 GB ã®ãƒã‚¤ãƒˆå€¤
DOWNLOAD_DIR = "/tmp/ftp_downloads"

# LinkShareãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
FULL_DATA_PATTERN = re.compile(r"(\d+)_3750988_mp\.txt\.gz$")
DELTA_DATA_PATTERN = re.compile(r"(\d+)_3750988_delta\.txt\.gz$")

FIXED_DELIMITER = '|'
FIXED_DELIMITER_NAME = 'PIPE'

# ğŸš€ ä¿®æ­£: C4 (å•†å“å) ã‚’ DBãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ 'product_name' ã«ãƒãƒƒãƒ”ãƒ³ã‚°
FIELD_MAPPING = {
    # C3: SKU -> sku_unique (PKã¨ã—ã¦ä½¿ç”¨)
    'C3': {'DB_FIELD': 'sku_unique', 'TYPE': 'str', 'PK': True, 'DESCRIPTION': 'å•†å“ã‚³ãƒ¼ãƒ‰ (SKUã€æ–°ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼)'},
    # ğŸ’¡ ä¿®æ­£: C4 ã‚’ product_name ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    'C2': {'DB_FIELD': 'product_name', 'TYPE': 'str', 'DESCRIPTION': 'å•†å“å'},
    'C13': {'DB_FIELD': 'price', 'TYPE': 'Decimal', 'DESCRIPTION': 'ä¾¡æ ¼ (æ—§å®šä¾¡)'},
    # C6: affiliate_url ã‚’ DBãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦å¾©å¸°
    'C6': {'DB_FIELD': 'affiliate_url', 'TYPE': 'str', 'DESCRIPTION': 'ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆURL'}, 
    # 'C9': {'DB_FIELD': 'product_url', 'TYPE': 'str', 'DESCRIPTION': 'è£½å“URL'}, # C9 ã¯ product_url ã«å¯¾å¿œã™ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ãŒã€ã“ã“ã§ã¯çœç•¥ã•ã‚Œã¦ã„ã‚‹ãŸã‚ãã®ã¾ã¾
}

EXPECTED_COLUMNS_COUNT = 38 
DATE_FORMAT = '%Y%m%d %H:%M:%S'
DATE_REGEX = re.compile(r'^\d{8} \d{2}:\d{2}:\d{2}$')


# ==============================================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤
# ==============================================================================
def human_readable_size(size_bytes: int) -> str:
    size_name = ("B", "KB", "MB", "GB", "TB")
    if size_bytes == 0:
        return "0B"
    try:
        i = int(math.floor(math.log(size_bytes, 1024)))
        p = pow(1024, i) 
        s = round(size_bytes / p, 2)
        
        if i >= len(size_name):
            return f"{size_bytes} B"
            
        return f"{s:,.2f} {size_name[i]}"
    except ValueError:
        return f"{size_bytes} B"
    except Exception:
        return f"{size_bytes} B"

def _get_ftp_client() -> Optional[ftplib.FTP]:
    try:
        ftp_client = ftplib.FTP()
        ftp_client.set_pasv(True) 
        ftp_client.set_debuglevel(0)
        
        ftp_client.connect(FTP_HOST, FTP_PORT, FTP_TIMEOUT)
        ftp_client.login(FTP_USER, FTP_PASS)
        
        return ftp_client
        
    except ftplib.all_errors as e:
        print(f"âŒ [ERROR] FTPæ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—: {e}", file=sys.stderr)
        return None
        
    except Exception as e:
        print(f"âŒ [ERROR] FTPæ¥ç¶šå‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return None

def get_ftp_mid_list(ftp_client: ftplib.FTP) -> List[Tuple[str, str, str, Optional[datetime], int]]:
    file_list = []
    
    try:
        print("ğŸ“¡ [FTP] ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ (MLSD) ã‚’å–å¾—ä¸­...", file=sys.stdout, flush=True)
        # MLSDã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        for filename, facts in ftp_client.mlsd():
            
            if facts.get('type') != 'file' or 'size' not in facts:
                continue
                
            is_full_data = FULL_DATA_PATTERN.match(filename)
            is_delta_data = DELTA_DATA_PATTERN.match(filename)
            
            if is_full_data or is_delta_data:
                mid = is_full_data.group(1) if is_full_data else is_delta_data.group(1)
                file_type = 'full' if is_full_data else 'delta'
                
                try:
                    file_size = int(facts.get('size', 0))
                except ValueError:
                    file_size = 0
                
                mtime_str = facts.get('modify')
                mtime_dt = None
                if mtime_str:
                    try:
                        mtime_dt = datetime.strptime(mtime_str, '%Y%m%d%H%M%S').replace(tzinfo=timezone.utc)
                    except ValueError:
                        pass
                        
                file_list.append((mid, filename, file_type, mtime_dt, file_size))
                        
        print(f"âœ… [FTP] {len(file_list)} ä»¶ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
                
    except ftplib.all_errors as e:
        print(f"âŒ [ERROR] FTPãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {e}", file=sys.stderr)
        return []
        
    return file_list

def safe_cast(value: str, target_type: str, field_name: str) -> Optional[Any]:
    if not value or value.strip().lower() in ('null', 'none', 'n/a', ''):
        return None
        
    stripped_value = value.strip()
    
    if target_type == 'Decimal':
        try:
            temp_value = stripped_value.replace(',', '').replace('$', '').replace('Â¥', '')
            if not temp_value:
                return None
            return Decimal(temp_value)
        except InvalidOperation:
            return None
            
    elif target_type == 'datetime':
        if not DATE_REGEX.match(stripped_value):
            return None
        try:
            dt = datetime.strptime(stripped_value, DATE_FORMAT)
            return dt.replace(tzinfo=timezone.utc)
        except ValueError:
            return None
            
    return stripped_value


# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯
# ==============================================================================

def _display_mapping_for_first_row(row_list: List[str]):
    """æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ‘ãƒ¼ã‚¹çµæœã‚’è¡¨ç¤ºã—ã€ã‚«ãƒ©ãƒ ã‚ºãƒ¬ã®ç¢ºèªã‚’åŠ©ã‘ã‚‹ã€‚ï¼ˆãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼‰"""
    print("\n--- æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ‘ãƒ¼ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚° (ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: LinkShare Column -> DB Field -> Raw Value) ---", file=sys.stdout, flush=True)
    print(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(row_list)} / æœŸå¾…å€¤: {EXPECTED_COLUMNS_COUNT}", file=sys.stdout, flush=True)
    
    print(f"{'LS-COL':<7} | {'DB FIELD':<35} | {'RAW VALUE (å…ˆé ­50æ–‡å­—)':<50}", file=sys.stdout, flush=True)
    print("-" * 98, file=sys.stdout, flush=True)
    
    # ã‚³ã‚¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‘ãƒ¼ã‚¹è¡¨ç¤º
    for i in range(EXPECTED_COLUMNS_COUNT):
        col_name = f'C{i+1}'
        mapping = FIELD_MAPPING.get(col_name)
        
        # ğŸ’¡ ä¿®æ­£: C4 (product_name) ã‚’é€šå¸¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã—ã¦è¡¨ç¤º
        if mapping:
            db_field = mapping.get('DB_FIELD', 'N/A')
        else:
            continue
            
        raw_value = row_list[i] if i < len(row_list) else ""
        
        display_value = raw_value.replace('\n', '\\n').replace('\r', '\\r')
        if len(display_value) > 50:
            display_value = display_value[:47] + "..."
            
        print(f"{col_name:<7} | {db_field:<35} | '{display_value}'", file=sys.stdout, flush=True)
            
    print(f"{'ALL':<7} | {'raw_csv_data':<35} | 'å…¨ã‚«ãƒ©ãƒ ã®ç”Ÿãƒ‡ãƒ¼ã‚¿'", file=sys.stdout, flush=True)
    print("--------------------------------------------------------------------------------------------------", file=sys.stdout, flush=True)


def _parse_single_row(row_list: List[str], mid: str, advertiser_name: str) -> Optional[Dict[str, Any]]:
    """å˜ä¸€è¡Œã®CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€DBä¿å­˜ç”¨ã®è¾æ›¸å½¢å¼ã«å¤‰æ›ã™ã‚‹ã€‚"""
    if len(row_list) != EXPECTED_COLUMNS_COUNT:
        return None 

    data: Dict[str, Any] = {
        'merchant_id': mid, 
        'created_at': timezone.now(), 
        'updated_at': timezone.now(),
        'is_active': True,
        'in_stock': True, 
        'sku': 'NON-SKU', 
        'product_url': '', 
        'product_name': '', # ğŸ’¡ ä¿®æ­£: DBä¿å­˜ã™ã‚‹ãŸã‚ã€åˆæœŸå€¤ã¯æ®‹ã™
        'affiliate_url': '', 
        'merchant_name': advertiser_name, 
    }
    
    data['raw_csv_data'] = FIXED_DELIMITER.join(row_list)

    # ã‚³ã‚¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‘ãƒ¼ã‚¹
    for i in range(EXPECTED_COLUMNS_COUNT):
        col_name = f'C{i+1}'
        raw_value = row_list[i]
        
        mapping = FIELD_MAPPING.get(col_name)
        if not mapping:
            continue
            
        db_field = mapping.get('DB_FIELD')
        data_type = mapping.get('TYPE')

        if not db_field:
            continue
        
        # 'sku_unique' ãŒå­˜åœ¨ã™ã‚Œã°ã€'sku' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚‚åŒã˜å€¤ã‚’è¨­å®š
        if db_field == 'sku_unique':
            casted_value = safe_cast(raw_value, data_type, db_field)
            if casted_value:
                data[db_field] = casted_value 
                data['sku'] = casted_value 
            continue
            
        # ğŸ’¡ ä¿®æ­£: product_name (C4) ã‚„ price (C13), affiliate_url (C6) ãªã©ã®ã‚³ã‚¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å‡¦ç†
        data[db_field] = safe_cast(raw_value, data_type, db_field)

    # å¿…é ˆãƒã‚§ãƒƒã‚¯: sku_unique, product_name, price ã¯å¿…é ˆ
    if not data.get('sku_unique'):
        return None
    # ğŸ’¡ ä¿®æ­£: product_name ã®å¿…é ˆãƒã‚§ãƒƒã‚¯ã‚’å¾©å¸°/ç¶­æŒ
    if not data.get('product_name'):
        return None
    if data.get('price') is None:
        return None
    
    # ğŸš¨ ä¿®æ­£: DBã«å­˜åœ¨ã™ã‚‹ã‚ˆã†ã«ãªã£ãŸãŸã‚ã€ product_name ã®å‰Šé™¤ (del data['product_name']) ã¯è¡Œã‚ãªã„
    # del data['product_name'] 

    return data


def _bulk_import_products(mid: str, product_data_list: List[Dict[str, Any]]) -> Tuple[int, int, int]:
    
    if not product_data_list or LinkshareProduct == DummyModel:
        print(f"âš ï¸ [BULK] LinkshareProductãƒ¢ãƒ‡ãƒ«æœªå®šç¾©ã®ãŸã‚ã€DBä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚å‡¦ç†ä»¶æ•°: {len(product_data_list)}", file=sys.stderr, flush=True)
        return len(product_data_list), len(product_data_list), 0 

    incoming_sku_map = {data['sku_unique']: data for data in product_data_list if data.get('sku_unique')}
    skus_to_check = list(incoming_sku_map.keys())
    
    to_create_linkshare: List[LinkshareProduct] = []
    to_update_linkshare: List[LinkshareProduct] = []

    # 1. LinkshareProduct ã® Upsert æº–å‚™
    
    # ğŸš¨ ä¿®æ­£: .only() ã« product_name ã‚’å«ã‚ã‚‹
    existing_products = LinkshareProduct.objects.filter(
        merchant_id=mid,
        sku_unique__in=skus_to_check
    ).only(
        'sku_unique', 
        'id', 
        'price', 
        'in_stock', 
        'is_active', 
        'sku', 
        'product_url',
        'affiliate_url', 
        'raw_csv_data',
        'merchant_id',
        'created_at',
        'updated_at',
        'merchant_name',
        'product_name', # ğŸ‘ˆ å¾©å¸°/è¿½åŠ 
    )
    
    existing_sku_map = {p.sku_unique: p for p in existing_products}
    
    # ğŸ’¡ ä¿®æ­£: update_fields ã« product_name ã‚’å«ã‚ã‚‹
    update_fields = [
        'price', 'in_stock', 'is_active', 
        'sku', 'product_url', 'affiliate_url', 
        'raw_csv_data',
        'updated_at',
        'merchant_name',
        'product_name', # ğŸ‘ˆ å¾©å¸°/è¿½åŠ 
    ]
    
    # ğŸ’¡ ä¿®æ­£: required_fields ã« product_name ã‚’å«ã‚ã‚‹
    required_fields = [
        'price', 'in_stock', 'is_active', 'sku', 'product_url', 
        'raw_csv_data', 'affiliate_url', 'merchant_name', 'product_name'
    ] 
    
    for sku_unique, data in incoming_sku_map.items():
        
        # Djangoãƒ¢ãƒ‡ãƒ«ãŒæŒã¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’å³å¯†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        allowed_fields = set(required_fields + ['merchant_id', 'created_at', 'updated_at', 'sku_unique'])
        
        clean_data = {
            k: v for k, v in data.items() 
            if hasattr(LinkshareProduct, k) and k in allowed_fields
        }
        
        clean_data['updated_at'] = timezone.now() 

        if 'created_at' not in clean_data:
            clean_data['created_at'] = timezone.now() 
        
        if sku_unique in existing_sku_map:
            product_instance = existing_sku_map[sku_unique]
            is_updated = False
            
            for key in update_fields:
                if key not in clean_data:
                    continue
                
                current_value = getattr(product_instance, key)
                new_value = clean_data[key]
                
                is_diff = True
                if current_value == new_value:
                    is_diff = False
                elif isinstance(current_value, Decimal) and isinstance(new_value, Decimal):
                    if current_value.compare(new_value) == 0:
                        is_diff = False
                elif current_value is None and new_value == '': 
                    is_diff = False
                elif new_value is None and current_value == '':
                    is_diff = False
                        
                if is_diff:
                    setattr(product_instance, key, new_value)
                    is_updated = True
            
            if is_updated:
                to_update_linkshare.append(product_instance)
        else:
            # æ–°è¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            new_instance = LinkshareProduct(**clean_data)
            
            # ğŸŒŸ ä¿®æ­£ç‚¹: delattr(new_instance, 'merchant_name') ã¯ä»¥å‰ã®ä¿®æ­£ã§å‰Šé™¤æ¸ˆã¿ã®ãŸã‚ã€ã“ã“ã§ã¯ä½•ã‚‚ã—ãªã„
            # (merchant_name, product_name ã¨ã‚‚ã« DB ã‚«ãƒ©ãƒ ã¨ã—ã¦å­˜åœ¨ã™ã‚‹)
                
            to_create_linkshare.append(new_instance)
    
    updated_count = 0
    if to_update_linkshare:
        try:
            LinkshareProduct.objects.bulk_update(to_update_linkshare, update_fields, batch_size=5000)
            updated_count = len(to_update_linkshare)
        except Exception as e:
            print(f" âŒ [MID: {mid}] ãƒãƒ«ã‚¯æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
            
    created_count = 0
    if to_create_linkshare:
        try:
            LinkshareProduct.objects.bulk_create(
                to_create_linkshare, 
                batch_size=5000 
            )
            created_count = len(to_create_linkshare)
        except IntegrityError as e:
            print(f" âŒ [MID: {mid}] ãƒãƒ«ã‚¯ä½œæˆä¸­ã«IntegrityErrorãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
            
    return created_count + updated_count, created_count, updated_count


# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜ã‚’çµ±åˆã—ãŸãƒ¡ã‚¤ãƒ³å‡¦ç† (å¤‰æ›´ãªã—)
# ==============================================================================

def parse_and_process_file(local_path: str, mid: str) -> Tuple[bool, int]:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€DBã«ä¿å­˜ã™ã‚‹ã€‚"""
    
    current_batch: List[Dict[str, Any]] = []
    parsed_count = 0
    total_saved_rows = 0
    advertiser_name: str = 'N/A'
    first_row_logged = False
    
    delimiter = FIXED_DELIMITER 
    
    try:
        with open(local_path, 'r', encoding='utf-8', newline='') as f:
            reader = csv.reader(f, delimiter=delimiter)
            
            # 1. HDRè¡Œã®å‡¦ç† (1è¡Œç›®)
            try:
                hdr_row = next(reader)
                if hdr_row[0].strip() == 'HDR':
                    advertiser_name = hdr_row[2].strip() if len(hdr_row) > 2 else 'N/A'
                    print(f"ğŸ’¡ [MID: {mid}] Advertiser Nameã‚’å–å¾—: '{advertiser_name}'ã€‚æ¬¡ã®è¡Œã‹ã‚‰ãƒ‡ãƒ¼ã‚¿é–‹å§‹ï¼ˆã‚«ãƒ©ãƒ åç„¡ã—ï¼‰ã€‚", file=sys.stdout, flush=True)
                else:
                    print(f"âš ï¸ [MID: {mid}] HDRè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚", file=sys.stderr, flush=True)
                    f.seek(0) 
                    reader = csv.reader(f, delimiter=delimiter)

            except StopIteration:
                print(f"âŒ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚", file=sys.stderr, flush=True)
                return False, 0
                
            # 2. ãƒ‡ãƒ¼ã‚¿è¡Œã®å‡¦ç†
            for row in reader:
                if len(row) != EXPECTED_COLUMNS_COUNT:
                    continue
                    
                parsed_count += 1

                if parsed_count % 50000 == 0:
                    print(f"ğŸ”„ [MID: {mid}] **ç¾åœ¨ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ {parsed_count:,} ä»¶**ã€‚æ¬¡ã®DBæ›¸ãè¾¼ã¿ãƒãƒƒãƒã‚’å¾…æ©Ÿä¸­...", file=sys.stdout, flush=True)
                
                
                # ğŸš¨ ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ã¿ã€å…¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
                if not first_row_logged:
                    _display_mapping_for_first_row(row)
                    first_row_logged = True
                
                # 3. å˜ä¸€è¡Œã®ãƒ‘ãƒ¼ã‚¹
                record = _parse_single_row(row, mid, advertiser_name)
                
                if not record or not record.get('sku_unique') or not record.get('merchant_id'):
                    continue

                current_batch.append(record)
                
                # ãƒãƒƒãƒå‡¦ç†
                if len(current_batch) >= 5000:
                    saved, created, updated = _bulk_import_products(mid, current_batch)
                    total_saved_rows += saved
                    print(f"â³ [MID: {mid}] å‡¦ç†æ¸ˆã¿ {parsed_count:,} ä»¶ã€‚ä¿å­˜: {saved:,} (æ–°è¦:{created:,}, æ›´æ–°:{updated:,})", file=sys.stdout, flush=True)
                    current_batch = []

            # 4. æœ€çµ‚ãƒãƒƒãƒã®å‡¦ç†
            if current_batch:
                saved, created, updated = _bulk_import_products(mid, current_batch)
                total_saved_rows += saved

            print(f"âœ… [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ã‚¹å®Œäº†ã€‚ç·ãƒ‘ãƒ¼ã‚¹ä»¶æ•°: {parsed_count:,} ä»¶", file=sys.stdout, flush=True)
            return True, total_saved_rows

    except Exception as e:
        print(f"âŒ [MID: {mid}] ãƒ‘ãƒ¼ã‚¹å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr, flush=True)
        print(traceback.format_exc(), file=sys.stderr, flush=True)
        return False, 0


def download_file(ftp_client: ftplib.FTP, filename: str, local_path_gz: str, local_path_txt: str, mid: str, file_size: int) -> Tuple[bool, int]:
    print(f"ğŸ“¡ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ« {filename} ({human_readable_size(file_size)}) ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...", file=sys.stdout, flush=True)
    
    ENCODING = 'utf-8'
    ERROR_HANDLING = 'ignore' 
    
    try:
        # 1. GZãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with open(local_path_gz, 'wb') as f:
            ftp_client.retrbinary(f'RETR {filename}', f.write)

        print(f"ğŸ“¦ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚è§£å‡ä¸­...", file=sys.stdout, flush=True)
        
        # 2. GZãƒ•ã‚¡ã‚¤ãƒ«ã®è§£å‡ã¨ãƒ‡ã‚³ãƒ¼ãƒ‰ (ã‚¨ãƒ©ãƒ¼ç„¡è¦–)
        decompressed_size = 0
        
        with gzip.open(local_path_gz, 'rb') as f_in:
            with open(local_path_txt, 'w', encoding='utf-8', newline='') as f_out:
                
                buffer_size = 1024 * 1024 # 1MB chunk
                while True:
                    chunk = f_in.read(buffer_size)
                    if not chunk:
                        break
                    
                    text_chunk = chunk.decode(ENCODING, errors=ERROR_HANDLING) 
                    f_out.write(text_chunk)
                    
                    decompressed_size += len(text_chunk)
                        
        print(f"âœ… [MID: {mid}] è§£å‡ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {ENCODING}, ã‚¨ãƒ©ãƒ¼å‡¦ç†: {ERROR_HANDLING})ã€‚TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {human_readable_size(decompressed_size)}", file=sys.stdout, flush=True)
        
        # 3. GZãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        os.remove(local_path_gz)
        
        return True, decompressed_size

    except ftplib.all_errors as e:
        print(f"âŒ [MID: {mid}] FTPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}", file=sys.stderr)
        if os.path.exists(local_path_gz): os.remove(local_path_gz)
        if os.path.exists(local_path_txt): os.remove(local_path_txt)
        return False, 0
    except Exception as e:
        print(f"âŒ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/è§£å‡ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
        if os.path.exists(local_path_gz): os.remove(local_path_gz)
        if os.path.exists(local_path_txt): os.remove(local_path_txt)
        return False, 0


# ==============================================================================
# Django Management Command ã®å®šç¾©
# ==============================================================================
class Command(BaseCommand):
    
    help = 'LinkShare FTPã‹ã‚‰ãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€LinkshareProductãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚'

    def add_arguments(self, parser):
        parser.add_argument(
            '--mid', 
            type=str, 
            help='å‡¦ç†ã™ã‚‹ç‰¹å®šã®ãƒãƒ¼ãƒãƒ£ãƒ³ãƒˆID (MID) ã‚’æŒ‡å®šã—ã¾ã™ã€‚', 
            default=None
        )
        parser.add_argument(
            '--limit',
            type=int,
            help='å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§æ•°ã€‚ãƒ‡ãƒãƒƒã‚°ã‚„ãƒ†ã‚¹ãƒˆæ™‚ã«ä¾¿åˆ©ã§ã™ã€‚',
            default=None
        )

    def handle(self, *args, **options):
        
        self.stdout.write("--- LinkShare ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰é–‹å§‹ (ãƒãƒ«ã‚¯å‡¦ç†) ---")

        # ğŸš¨ ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã®ç½®ãæ›ãˆ
        try:
            from api.models.linkshare_products import LinkshareProduct as RealLinkshareProduct
            
            globals()['LinkshareProduct'] = RealLinkshareProduct
            self.stdout.write("âœ… ãƒ¢ãƒ‡ãƒ« (LinkshareProduct) ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸã€‚")
            
        except ImportError as e:
            self.stdout.write(self.style.ERROR(f"ğŸš¨ CRITICAL: ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚DBã¸ã®ä¿å­˜ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚"))
            self.stderr.write(self.style.ERROR(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}"))
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        if not os.path.exists(DOWNLOAD_DIR):
            os.makedirs(DOWNLOAD_DIR)
            self.stdout.write(f"ğŸ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {DOWNLOAD_DIR} ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        # FTPæ¥ç¶š
        ftp_client = _get_ftp_client()

        if not ftp_client:
            self.stdout.write(self.style.ERROR("ğŸš¨ FTPæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"))
            return

        total_processed_files = 0
        total_saved_rows = 0
        mid_list: List[Tuple[str, str, str, Optional[datetime], int]] = [] 

        try:
            # 1. FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®å–å¾—
            mid_list = get_ftp_mid_list(ftp_client) 
            
            limit = options['limit']

            if options['mid']:
                mid_list = [item for item in mid_list if item[0] == options['mid']]

            # --limit ã®é©ç”¨
            if limit is not None and limit > 0:
                mid_list = mid_list[:limit]

            if not mid_list:
                self.stdout.write(self.style.WARNING("âŒ å‡¦ç†å¯¾è±¡ã¨ãªã‚‹LinkShareãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"))
                return

            self.stdout.write(f"âœ… {len(mid_list)} ä»¶ã®MIDãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

            # FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®è¡¨ç¤º
            self.stdout.write(self.style.NOTICE("\n--- å‡¦ç†å¯¾è±¡ã®LinkShare FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ---"))
            self.stdout.write(f"{'MID':<6} | {'ãƒ•ã‚¡ã‚¤ãƒ«å':<40} | {'ã‚µã‚¤ã‚º':<10} | æœ€çµ‚æ›´æ–° (UTC)")
            self.stdout.write("-" * 75)
            for mid_id, filename, file_type, mtime_dt, file_size in mid_list:
                size_hr = human_readable_size(file_size)
                mtime_str = mtime_dt.strftime('%Y-%m-%d %H:%M:%S') if mtime_dt else 'N/A'
                self.stdout.write(f"{mid_id:<6} | {filename:<40} | {size_hr:<10} | {mtime_str}")
            self.stdout.write("----------------------------------------------\n")
            
            # --- ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹ ---
            for mid, filename, file_type, mtime_dt, file_size in mid_list:
                self.stdout.write(f"\n--- [MID: {mid}] å‡¦ç†é–‹å§‹ ({filename}) ---")
                
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®æ±ºå®š
                local_gz_path = os.path.join(DOWNLOAD_DIR, filename)
                local_txt_path = local_gz_path.replace('.gz', '.txt')

                # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç† (Atomic: å¤±æ•—æ™‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                with transaction.atomic():
                    success = False
                    current_saved_rows = 0
                    try:
                        # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è§£å‡
                        is_downloaded, downloaded_size = download_file(
                            ftp_client, 
                            filename, 
                            local_gz_path, 
                            local_txt_path, 
                            mid, 
                            file_size
                        )
                        
                        if is_downloaded:
                            # 2. ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜ 
                            success, current_saved_rows = parse_and_process_file(local_txt_path, mid) 
                            
                            # 3. å‡¦ç†æ¸ˆã¿TXTãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                            if os.path.exists(local_txt_path):
                                os.remove(local_txt_path)
                                self.stdout.write(f"ğŸ§¹ [MID: {mid}] å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(local_txt_path)} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚") 

                        
                    except Exception as e:
                        # å‡¦ç†ä¸­ã®è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã‚’æ•æ‰ã—ã€ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        self.stderr.write(self.style.ERROR(f"\n[MID: {mid}] å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚"))
                        self.stderr.write(self.style.ERROR(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}"))
                        self.stderr.write(traceback.format_exc()) 

                    if success:
                        total_processed_files += 1
                        total_saved_rows += current_saved_rows
                        self.stdout.write(self.style.SUCCESS(f"\n[MID: {mid}] å‡¦ç†å®Œäº†ã€‚DBä¿å­˜ä»¶æ•°: {current_saved_rows:,} ä»¶"))
                    else:
                        self.stdout.write(self.style.ERROR(f"\n[MID: {mid}] å‡¦ç†å¤±æ•— (ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯)ã€‚"))

        finally:
            # FTPæ¥ç¶šã®çµ‚äº†å‡¦ç†
            if ftp_client:
                try:
                    ftp_client.quit()
                    self.stdout.write("\nğŸ“¡ FTPæ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
                except ftplib.all_errors:
                    pass
            
        self.stdout.write(f"\n==================================================================================")
        self.stdout.write(f"--- æœ€çµ‚çµæœ: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰å®Œäº† ---")
        self.stdout.write(f"æ­£å¸¸å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_processed_files} / {len(mid_list)} ä»¶")
        self.stdout.write(self.style.SUCCESS(f"åˆè¨ˆä¿å­˜è¡Œæ•°: {total_saved_rows:,} è¡Œ"))
        self.stdout.write("==================================================================================")