import os
import django
import re
import json
import time
import random
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from django.db import transaction

# --- Djangoè¨­å®š ---
# å®Ÿè¡Œç’°å¢ƒã«åˆã‚ã›ã¦è¨­å®šã—ã¦ãã ã•ã„
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

def get_refined_genre(url, name):
    """
    è£½å“åã¨URLã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æ—¥æœ¬èªãƒ»è‹±èªä¸¡æ–¹ã§åˆ¤å®šã™ã‚‹
    """
    # åˆ¤å®šç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆã—ã¦å°æ–‡å­—åŒ–
    text = (url + " " + name).lower()
    
    # 1. ãƒ¢ãƒ‹ã‚¿ãƒ¼
    if any(k in text for k in ["monitor", "ãƒ¢ãƒ‹ã‚¿ãƒ¼", "ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤", "display"]):
        return "monitor"
    
    # 2. ã‚²ãƒ¼ãƒŸãƒ³ã‚°PC
    if any(k in text for k in ["alienware", "gaming", "ã‚²ãƒ¼ãƒŸãƒ³ã‚°", "g-series"]):
        return "gaming_pc"
    
    # 3. ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ (laptop)
    # æ—¥æœ¬èªã®ã€Œãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã€ã¨è‹±èªã®ã€Œlaptopã€ä¸¡æ–¹ã«å¯¾å¿œ
    if any(k in text for k in ["laptop", "ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³", "inspiron", "xps", "2-in-1", "ãƒãƒ¼ãƒˆpc"]):
        return "laptop"
    
    # 4. ãƒ“ã‚¸ãƒã‚¹ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³
    if any(k in text for k in ["vostro", "latitude"]):
        return "business_laptop"
    
    # 5. ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—
    if any(k in text for k in ["desktop", "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—", "optiplex", "precision", "ã‚¹ãƒªãƒ ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—"]):
        return "desktop"
    
    # 6. å‘¨è¾ºæ©Ÿå™¨ãƒ»ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼
    if any(k in text for k in ["backpack", "ãƒãƒƒã‚¯ãƒ‘ãƒƒã‚¯", "mouse", "ãƒã‚¦ã‚¹", "keyboard", "ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰", 
                               "headset", "ãƒ˜ãƒƒãƒ‰ã‚»ãƒƒãƒˆ", "adapter", "ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼", "ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼", "speaker", "ã‚±ãƒ¼ã‚¹"]):
        return "accessories"
    
    # åˆ¤å®šä¸èƒ½ãªå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return "pc"

def extract_from_json_ld(soup):
    """HTMLå†…ã®JSON-LDã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    result = {"name": "", "price": 0, "image": None}
    scripts = soup.find_all('script', type='application/ld+json')
    for script in scripts:
        try:
            data = json.loads(script.string)
            items = data if isinstance(data, list) else [data]
            for item in items:
                if item.get('@type') == 'Product':
                    result["name"] = item.get('name', "")
                    offers = item.get('offers')
                    if offers:
                        if isinstance(offers, list): offers = offers[0]
                        p = offers.get('price')
                        if p:
                            result["price"] = int(float(str(p).replace(',', '')))
                    img = item.get('image')
                    if img:
                        result["image"] = img[0] if isinstance(img, list) else img
                    return result
        except: continue
    return result

def scrape_detail_page(page, url, current_index, total_count):
    """å€‹åˆ¥è£½å“ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦DBä¿å­˜"""
    url = url.split('#')[0].split('?')[0].rstrip('/')
    
    try:
        unique_id = "dell-" + url.split('/')[-1]
        
        # é€šä¿¡è² è·ã‚’æŠ‘ãˆã‚‹ãŸã‚ HTMLå±•é–‹å®Œäº† ã§å‡¦ç†
        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        page.wait_for_timeout(2500) # JSæç”»å¾…ã¡
        
        soup = BeautifulSoup(page.content(), 'html.parser')
        json_data = extract_from_json_ld(soup)
        
        # åç§°ã®å–å¾—ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è£œå®Œï¼‰
        name = json_data["name"] or page.title().split('|')[0].strip()
        name = name.replace('Dell æ—¥æœ¬', '').strip()
        
        # ã€é‡è¦ã€‘å¼·åŒ–ã•ã‚ŒãŸã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š
        genre = get_refined_genre(url, name)
        
        # ä¾¡æ ¼ã®å–å¾—
        price = json_data["price"]
        if price == 0:
            price_el = soup.select_one('[data-testid="shared-ps-dell-price"], .ps-dell-price, .dell-price')
            if price_el:
                price_text = re.sub(r'[^\d]', '', price_el.get_text())
                if price_text: price = int(price_text)

        # ç”»åƒURL
        image_url = json_data["image"] or ""
        if not image_url:
            img_handle = page.query_selector('img[data-testid="shared-ps-image"], .ps-image img')
            if img_handle:
                src = img_handle.get_attribute("src")
                image_url = "https:" + src if src and src.startswith('//') else src

        # DBä¿å­˜å‡¦ç†ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§ç¢ºå®ŸåŒ–ï¼‰
        with transaction.atomic():
            PCProduct.objects.update_or_create(
                unique_id=unique_id,
                defaults={
                    'site_prefix': 'DELL',
                    'maker': 'Dell',
                    'raw_genre': genre,
                    'unified_genre': genre, # laptop, accessories ç­‰ãŒè¨­å®šã•ã‚Œã‚‹
                    'name': name,
                    'price': price,
                    'url': url,
                    'image_url': image_url,
                    'description': f"Dellå…¬å¼ {genre} ã‚«ãƒ†ã‚´ãƒªè£½å“",
                    'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'è©³ç´°ç¢ºèª',
                    'is_active': True,
                }
            )
        
        price_display = f"Â¥{price:,}" if price > 0 else "ä¾¡æ ¼ä¸æ˜"
        print(f"ğŸ” [{current_index + 1}/{total_count}] âœ… åˆ†é¡ [{genre.upper()}]: {name[:30]}... ({price_display})")
        return True
    except Exception as e:
        print(f"   âŒ è©³ç´°ã‚¨ãƒ©ãƒ¼: {url} -> {e}")
        return False

def run_crawler():
    """ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚¹ã‚­ãƒ£ãƒ³ã‹ã‚‰å·¡å›é–‹å§‹"""
    target_categories = [
        "https://www.dell.com/ja-jp/shop/deals/top-pc-deals",
        "https://www.dell.com/ja-jp/shop/scc/sc/laptops",
        "https://www.dell.com/ja-jp/shop/scc/sc/desktops",
        "https://www.dell.com/ja-jp/shop/deals/gaming-deals",
        "https://www.dell.com/ja-jp/shop/deals/business-pc-deals",
        "https://www.dell.com/ja-jp/shop/deals/monitors-deals",
        "https://www.dell.com/ja-jp/shop/deals/pc-accessories-deals",
        "https://www.dell.com/ja-jp/shop/deals/clearance-pc-deals",
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 800}
        )
        
        # é«˜é€ŸåŒ–ï¼šä¸è¦ãªã‚¢ã‚»ãƒƒãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯
        page = context.new_page()
        page.route("**/*.{png,jpg,jpeg,gif,webp,svg,woff,woff2,css}", 
                   lambda route: route.abort() if route.request.resource_type != "document" else route.continue_())

        all_product_urls = set()
        for cat_url in target_categories:
            print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚¹ã‚­ãƒ£ãƒ³ä¸­: {cat_url}")
            try:
                page.goto(cat_url, wait_until="commit", timeout=60000)
                page.wait_for_timeout(3000)
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…¨è£½å“ãƒªãƒ³ã‚¯ã‚’éœ²å‡ºã•ã›ã‚‹
                for _ in range(5):
                    page.evaluate("window.scrollBy(0, 1000)")
                    page.wait_for_timeout(800)
                
                hrefs = page.eval_on_selector_all('a[href*="/shop/"]', 
                    'elements => elements.map(e => e.href)')
                
                for h in hrefs:
                    clean_h = h.split('#')[0].split('?')[0].rstrip('/')
                    if any(p in clean_h for p in ["spd", "pdp", "pd", "cp"]):
                        all_product_urls.add(clean_h)
            except Exception as e:
                print(f"   âŒ ã‚¹ã‚­ãƒ£ãƒ³å¤±æ•—: {cat_url}")
        
        url_list = sorted(list(all_product_urls))
        total_count = len(url_list)
        print(f"ğŸš€ åˆè¨ˆ {total_count}ä»¶ã‚’æœ€é©ãªã‚¸ãƒ£ãƒ³ãƒ«ã«åˆ†é¡ã—ãªãŒã‚‰å‡¦ç†é–‹å§‹")
        
        # å…¨ä»¶ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ
        for i, url in enumerate(url_list): 
            scrape_detail_page(page, url, i, total_count)
            time.sleep(random.uniform(1, 2))
            
        browser.close()
        print(f"âœ¨ å®Œäº†ã—ã¾ã—ãŸã€‚DBã®ã‚¸ãƒ£ãƒ³ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    run_crawler()