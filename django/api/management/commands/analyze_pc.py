# -*- coding: utf-8 -*-
import json
import requests
import re
import os
from django.core.management.base import BaseCommand
from api.models.pc_products import PCProduct
from django.utils import timezone

# APIè¨­å®š
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
BASE_PROMPT_DIR = os.path.join(os.path.dirname(__file__), 'prompt')

class Command(BaseCommand):
    help = 'Gemma-3ã‚’ä½¿ç”¨ã—ã¦å¤–éƒ¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãè£½å“ã‚¹ãƒšãƒƒã‚¯è§£æã¨HTMLè¨˜äº‹ç”Ÿæˆã‚’è¡Œã„ã€DBã‚’æ›´æ–°ã™ã‚‹'

    def add_arguments(self, parser):
        parser.add_argument('unique_id', type=str, nargs='?')
        parser.add_argument('--limit', type=int, default=1, help='å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°')

    def load_prompt(self, filename):
        """promptãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        path = os.path.join(BASE_PROMPT_DIR, filename)
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            return ""

    def handle(self, *args, **options):
        unique_id = options['unique_id']
        limit = options['limit']

        # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã®ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        models_list = self.load_prompt('ai_models.txt')
        self.stdout.write(f"ğŸ“‚ ä½¿ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«å€™è£œ:\n{models_list.strip()}\n")

        if unique_id:
            products = PCProduct.objects.filter(unique_id=unique_id)
        else:
            # è§£ææœªå®Ÿæ–½ã®è£½å“ã‚’å„ªå…ˆ
            products = PCProduct.objects.filter(last_spec_parsed_at__isnull=True)[:limit]

        if not products.exists():
            self.stdout.write(self.style.WARNING("å¯¾è±¡è£½å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"))
            return

        for product in products:
            self.analyze_product(product)

    def analyze_product(self, product):
        self.stdout.write(f"\nğŸ” è§£æï¼†ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆé–‹å§‹: {product.name} ({product.unique_id})")

        # 1. å¤–éƒ¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª­ã¿è¾¼ã¿
        base_pc_prompt = self.load_prompt('analyze_pc_prompt.txt')
        mouse_rules = self.load_prompt('analyze_mouse_prompt.txt')
        
        # 2. ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ«ãƒ¼ãƒ«ã®é¸å®š
        brand_rules = ""
        name_lower = product.name.lower()
        id_lower = product.unique_id.lower()
        if "mouse" in name_lower or "mouse" in id_lower or product.maker == "MouseComputer":
            brand_rules = mouse_rules
        else:
            brand_rules = "ã€æ¨™æº–ãƒ«ãƒ¼ãƒ«ã€‘å‹ç•ªã‚„åç§°ã‹ã‚‰ãƒ¡ãƒ¼ã‚«ãƒ¼ã®å‘½åè¦å‰‡ã‚’æ¨æ¸¬ã—ã¦è§£æã—ã¦ãã ã•ã„ã€‚"

        # 3. æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ„ã¿ç«‹ã¦
        # è‡ªä½œPCææ¡ˆç”¨ã®ã‚«ãƒ©ãƒ ï¼ˆSocket, Chipset, RAM Type, PSUï¼‰ã‚’æŠ½å‡ºå¯¾è±¡ã«è¿½åŠ 
        full_prompt = f"""
{base_pc_prompt.format(maker=product.maker, name=product.name, price=product.price, description=product.description)}

ã€è¿½åŠ å‘½ä»¤ï¼šè©³ç´°ã‚¹ãƒšãƒƒã‚¯æŠ½å‡ºã€‘
è¨˜äº‹åŸ·ç­†ã¨åŒæ™‚ã«ã€ä»¥ä¸‹ã®ã‚¹ãƒšãƒƒã‚¯æƒ…å ±ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã€å›ç­”ã®æœ€å¾Œã«å¿…ãš [SPEC_JSON] {{...}} [/SPEC_JSON] ã®å½¢å¼ã§å«ã‚ã¦ãã ã•ã„ã€‚
ç‰¹ã«ã€CPUã‚„ãƒã‚¶ãƒ¼ãƒœãƒ¼ãƒ‰ã®å‹ç•ªã‹ã‚‰ã€Œã‚½ã‚±ãƒƒãƒˆã€ã‚„ã€Œãƒãƒƒãƒ—ã‚»ãƒƒãƒˆã€ã‚’è«–ç†çš„ã«æ¨è«–ã—ã¦ãã ã•ã„ã€‚

{{
    "memory_gb": æ•´æ•°, 
    "storage_gb": æ•´æ•°, 
    "npu_tops": å°æ•°,
    "cpu_model": "æ–‡å­—åˆ—", 
    "gpu_model": "æ–‡å­—åˆ—",
    "cpu_socket": "LGA1700/AM5ç­‰", 
    "chipset": "B760/Z790ç­‰", 
    "ram_type": "DDR5/DDR4",
    "power_wattage": æ•´æ•°(æ¨å¥¨é›»æºWæ•°),
    "display_info": "æ–‡å­—åˆ—", 
    "target_segment": "å±¤",
    "is_ai_pc": boolean, 
    "spec_score": 0-100
}}

ãƒ–ãƒ©ãƒ³ãƒ‰å›ºæœ‰ãƒ«ãƒ¼ãƒ«:
{brand_rules}
"""

        # 4. APIãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š (Gemma-3 27Bã‚’ä½¿ç”¨)
        model_id = "gemma-3-27b-it"
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_id}:generateContent?key={GEMINI_API_KEY}"
        
        payload = {
            "contents": [{"parts": [{"text": full_prompt}]}],
            "generationConfig": {
                "temperature": 0.2, 
            }
        }

        try:
            response = requests.post(api_url, json=payload, timeout=90)
            
            if response.status_code != 200:
                self.stdout.write(self.style.ERROR(f"âŒ API Error {response.status_code}: {response.text}"))
                return

            result = response.json()
            full_response_text = result['candidates'][0]['content']['parts'][0]['text'].strip()

            # --- ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ•ã‚§ãƒ¼ã‚º ---

            # A. ã‚¿ã‚¤ãƒˆãƒ«ã¨HTMLæœ¬æ–‡ã®åˆ†é›¢
            lines = full_response_text.split('\n')
            title = lines[0].replace('#', '').strip() # Markdownã®#ã‚’é™¤å»
            
            # ç‰¹æ®Šã‚¿ã‚°ã‚’é™¤ã„ãŸéƒ¨åˆ†ã‚’HTMLã¨ã—ã¦æŠ½å‡º
            html_content = "\n".join(lines[1:])
            html_content = re.sub(r'\[SUMMARY_DATA\].*?\[/SUMMARY_DATA\]', '', html_content, flags=re.DOTALL)
            html_content = re.sub(r'\[SPEC_JSON\].*?\[/SPEC_JSON\]', '', html_content, flags=re.DOTALL).strip()

            # B. [SUMMARY_DATA] ã®æŠ½å‡ºï¼ˆmeta descriptionç”¨ï¼‰
            summary_match = re.search(r'\[SUMMARY_DATA\](.*?)\[/SUMMARY_DATA\]', full_response_text, re.DOTALL)
            summary_text = summary_match.group(1).strip() if summary_match else ""

            # C. [SPEC_JSON] ã®æŠ½å‡º
            spec_match = re.search(r'\[SPEC_JSON\](.*?)\[/SPEC_JSON\]', full_response_text, re.DOTALL)
            if spec_match:
                spec_json_str = spec_match.group(1).strip()
                spec_data = json.loads(spec_json_str)
            else:
                # äºˆå‚™ï¼šã‚‚ã—ã‚¿ã‚°ãŒãªã‘ã‚Œã°å…¨ä½“ã‹ã‚‰JSONã‚’æ¢ã™
                json_match = re.search(r'\{.*"memory_gb".*\}', full_response_text, re.DOTALL)
                spec_data = json.loads(json_match.group(0)) if json_match else {}

            self.stdout.write(self.style.SUCCESS(f"--- è§£ææˆåŠŸ: {title[:30]}... ---"))

            # --- DBä¿å­˜ãƒ•ã‚§ãƒ¼ã‚º ---
            # åŸºæœ¬æƒ…å ±
            product.ai_summary = summary_text
            product.ai_content = html_content
            
            # AIæŠ½å‡ºã‚¹ãƒšãƒƒã‚¯
            product.cpu_model = spec_data.get('cpu_model')
            product.gpu_model = spec_data.get('gpu_model')
            product.memory_gb = spec_data.get('memory_gb')
            product.storage_gb = spec_data.get('storage_gb')
            product.npu_tops = spec_data.get('npu_tops')
            product.display_info = spec_data.get('display_info')
            product.is_ai_pc = spec_data.get('is_ai_pc', False)
            product.spec_score = spec_data.get('spec_score', 0)
            product.target_segment = spec_data.get('target_segment')

            # ğŸš€ è‡ªä½œPCææ¡ˆç”¨æ–°è¨­ã‚«ãƒ©ãƒ ã¸ã®ä¿å­˜
            product.cpu_socket = spec_data.get('cpu_socket')
            product.motherboard_chipset = spec_data.get('chipset')
            product.ram_type = spec_data.get('ram_type')
            product.power_recommendation = spec_data.get('power_wattage')
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ›´æ–°
            product.last_spec_parsed_at = timezone.now()
            product.save()
            
            self.stdout.write(self.style.SUCCESS(f"âœ… DBæ›´æ–°å®Œäº†: {product.unique_id} (Socket: {product.cpu_socket}, Chipset: {product.motherboard_chipset})"))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"âŒ ä¾‹å¤–ç™ºç”Ÿ ({product.unique_id}): {str(e)}"))
            if 'full_response_text' in locals():
                # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å†’é ­ã‚’ãƒ­ã‚°å‡ºåŠ›
                self.stdout.write(f"Raw Response Sample: {full_response_text[:200]}...")