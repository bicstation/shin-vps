import os
import re
import ftplib
import time
import gzip
import csv
import traceback
import sys
import subprocess
from datetime import datetime, timezone
from typing import List, Tuple, Dict, Any, Optional
from decimal import Decimal, InvalidOperation

# Djangoã®ã‚³ã‚¢æ©Ÿèƒ½ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from django.core.management.base import BaseCommand
from django.db import transaction, IntegrityError 
from django.utils import timezone

# ğŸš¨ ã€é‡è¦ï¼šä¿®æ­£ç®‡æ‰€ 1ã€‘å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹
# å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã«åˆã‚ã›ã¦ã€api.models.products ã‹ã‚‰ LinkshareProduct ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
try:
    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯ä»®å®šã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚
    from api.models.linkshare_products import LinkshareProduct 
    from api.models.raw_and_entities import RawApiData
except ImportError:
    # å®Ÿè¡Œç’°å¢ƒãŒãªã„å ´åˆã®ãƒ€ãƒŸãƒ¼å®šç¾© (ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œç’°å¢ƒã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ã«)
    class DummyModel:
        objects = None
        def __init__(self, **kwargs): pass
    LinkshareProduct = DummyModel
    RawApiData = DummyModel


# ==============================================================================
# æ¥ç¶šãƒ»ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š (å®šæ•°)
# ==============================================================================
FTP_HOST = os.getenv("LINKSHARE_FTP_HOST", "aftp.linksynergy.com")
FTP_USER = os.getenv("LINKSHARE_BS_USER", "rkp_3750988")
FTP_PASS = os.getenv("LINKSHARE_BS_PASS", "u5NetPVZEAhABD7HuW2VRymP")
FTP_PORT = 21
FTP_TIMEOUT = 180

MAX_SIZE_BYTES = 1073741824 # 1 GB ã®ãƒã‚¤ãƒˆå€¤
DOWNLOAD_DIR = "/tmp/ftp_downloads"

FULL_DATA_PATTERN = r"(\d+)_3750988_mp\.txt\.gz$"
DELTA_DATA_PATTERN = r"(\d+)_3750988_delta\.txt\.gz$"

FIXED_DELIMITER = '|'
FIXED_DELIMITER_NAME = 'PIPE'

# ==============================================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ (ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã€FTPæ¥ç¶š)
# ==============================================================================

def human_readable_size(size_bytes):
    """ãƒã‚¤ãƒˆå€¤ã‚’äººãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›ã™ã‚‹"""
    if size_bytes is None or size_bytes == 0:
        return "0 B"
    size_name = ("B", "KB", "MB", "GB", "TB")
    i = 0
    size_bytes = float(size_bytes)
    while size_bytes >= 1024 and i < len(size_name) - 1:
        size_bytes /= 1024
        i += 1
    return f"{size_bytes:,.2f} {size_name[i]}"

def _get_ftp_client() -> Optional[ftplib.FTP]:
    """FTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ã—ã€æ¥ç¶šãƒ»ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹"""
    print(f"ğŸŒ [DEBUG] _get_ftp_client å‘¼ã³å‡ºã—é–‹å§‹ã€‚", file=sys.stdout, flush=True)

    if not all([FTP_HOST, FTP_USER, FTP_PASS]):
        print("ğŸš¨ [DEBUG] æ¥ç¶šæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚", file=sys.stderr, flush=True)
        return None

    ftp_client = None
    try:
        print(f"ğŸ“¡ [DEBUG] æ¥ç¶šè©¦è¡Œ: {FTP_HOST}:{FTP_PORT}, ãƒ¦ãƒ¼ã‚¶ãƒ¼: {FTP_USER}")

        ftp_client = ftplib.FTP(timeout=FTP_TIMEOUT)
        print("ğŸ’¡ [DEBUG] ftplib.FTP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”ŸæˆæˆåŠŸã€‚", file=sys.stdout, flush=True)
        
        ftp_client.connect(FTP_HOST, FTP_PORT)
        print("ğŸ’¡ [DEBUG] ftp_client.connect æˆåŠŸã€‚", file=sys.stdout, flush=True)
        
        ftp_client.login(FTP_USER, FTP_PASS)
        print("ğŸ’¡ [DEBUG] ftp_client.login æˆåŠŸã€‚", file=sys.stdout, flush=True)

        # ãƒ‘ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ (PASV) ã‚’å¼·åˆ¶è¨­å®šã™ã‚‹
        ftp_client.set_pasv(True) 
        print("âœ… [DEBUG] ãƒ‘ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰ (PASV) ã‚’è¨­å®šã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
        
        print("âœ… [DEBUG] FTPæ¥ç¶šãŠã‚ˆã³ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã€‚_get_ftp_client çµ‚äº†ã€‚", file=sys.stdout, flush=True)
        return ftp_client

    except ftplib.all_errors as e:
        print(f"âŒ [DEBUG] FTPæ¥ç¶š/ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•— (ftplib.all_errors): {type(e).__name__} - {e}", file=sys.stderr, flush=True)
        if ftp_client:
            try: ftp_client.quit()
            except Exception: pass
        return None
    except Exception as e:
        print(f"âŒ [DEBUG] FTPæ¥ç¶š/ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•— (äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼): {type(e).__name__} - {e}", file=sys.stderr, flush=True)
        return None

def get_ftp_mid_list(data_path: str = "") -> List[Tuple[str, str, str, Optional[datetime], int]]:
    """FTPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰MIDã”ã¨ã®æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""
    print(f"ğŸ“‹ [DEBUG] get_ftp_mid_list å‘¼ã³å‡ºã—é–‹å§‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: '{data_path}'", file=sys.stdout, flush=True)
    
    ftp = _get_ftp_client()
    if not ftp:
        print("âŒ [DEBUG] get_ftp_mid_list çµ‚äº† (FTPæ¥ç¶šå¤±æ•—)ã€‚", file=sys.stdout, flush=True)
        return []

    if data_path:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¤‰æ›´ãƒ­ã‚¸ãƒƒã‚¯
        try:
            ftp.cwd(data_path)
            print(f"ğŸ’¡ [DEBUG] ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ '{data_path}' ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
        except Exception as e:
            print(f"âŒ [DEBUG] ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¤‰æ›´å¤±æ•—: {e}", file=sys.stderr, flush=True)
            ftp.quit()
            return []
    
    try:
        file_names = ftp.nlst()
        print(f"ğŸ“‹ [DEBUG] ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—æˆåŠŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(file_names)}", file=sys.stdout, flush=True)
    except Exception as e:
        print(f"âŒ [DEBUG] ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {e}", file=sys.stderr, flush=True)
        ftp.quit()
        return []

    ftp.voidcmd('TYPE I')
    print("ğŸ’¡ [DEBUG] TYPE I (ãƒã‚¤ãƒŠãƒªè»¢é€ãƒ¢ãƒ¼ãƒ‰) ã‚’è¨­å®šã€‚", file=sys.stdout, flush=True)

    # {MID: (filename, file_type, mtime_ts, file_size)}
    mid_file_details: Dict[str, Tuple[str, str, float, int]] = {}

    for filename in file_names:
        match_full = re.match(FULL_DATA_PATTERN, filename)
        match_delta = re.match(DELTA_DATA_PATTERN, filename)
        match = match_full or match_delta
        if not match: continue
        
        current_id = match.group(1)
        file_type = "FULL" if match_full else "DELTA"
        mtime_ts: float = 0.0
        file_size: int = 0
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        try:
            file_size = ftp.size(filename)
            if file_size > MAX_SIZE_BYTES: 
                print(f"âš ï¸ [DEBUG] ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¶…éã§ã‚¹ã‚­ãƒƒãƒ—: {filename} ({human_readable_size(file_size)})", file=sys.stdout, flush=True)
                continue
        except Exception: 
            continue
            
        try:
            mtime_response = ftp.sendcmd('MDTM ' + filename)
            if mtime_response.startswith('213 '):
                # YYYYMMDDHHMMSS å½¢å¼
                mtime_str = mtime_response[4:].strip()
                mtime_dt_naive = datetime.strptime(mtime_str, '%Y%m%d%H%M%S')
                mtime_ts = mtime_dt_naive.replace(tzinfo=timezone.utc).timestamp()
        except Exception: 
            pass
            
        if mtime_ts <= 0.0: continue
        
        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
        if current_id not in mid_file_details:
            mid_file_details[current_id] = (filename, file_type, mtime_ts, file_size)
        else:
            existing_filename, existing_type, existing_ts, existing_size = mid_file_details[current_id]
            
            # FULLãƒ•ã‚¡ã‚¤ãƒ«ã¯DELTAãƒ•ã‚¡ã‚¤ãƒ«ã‚ˆã‚Šå„ªå…ˆï¼ˆãƒ­ã‚¸ãƒƒã‚¯ã®ç°¡ç•¥åŒ–ã®ãŸã‚ã€ã“ã“ã§ã¯å˜ç´”ã«ä¸Šæ›¸ãï¼‰
            if file_type == "FULL" and existing_type == "DELTA":
                 mid_file_details[current_id] = (filename, file_type, mtime_ts, file_size)
            # åŒã˜ã‚¿ã‚¤ãƒ—ãªã‚‰æ–°ã—ã„ã‚‚ã®ã‚’æ¡ç”¨
            elif file_type == existing_type and mtime_ts > existing_ts:
                mid_file_details[current_id] = (filename, file_type, mtime_ts, file_size)

    try:
        ftp.quit()
        print("ğŸ’¡ [DEBUG] FTPæ¥ç¶šã‚’åˆ‡æ–­ã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
    except Exception:
        pass

    final_list = []
    for current_id, details in mid_file_details.items():
        filename, file_type, mtime_ts, file_size = details
        mtime_dt = datetime.fromtimestamp(mtime_ts, tz=timezone.utc) if mtime_ts > 0.0 else None
        final_list.append((current_id, filename, file_type, mtime_dt, file_size))

    print(f"âœ… [DEBUG] get_ftp_mid_list çµ‚äº†ã€‚æœ‰åŠ¹ãªMIDãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(final_list)}", file=sys.stdout, flush=True)
    return sorted(final_list, key=lambda x: x[0])


def _parse_linkshare_date(date_str: Optional[str]) -> Optional[datetime]:
    """Linkshareã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’UTCã®datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹"""
    if not date_str: return None
    try:
        # Linkshareã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¸€èˆ¬çš„ã«ä½¿ã‚ã‚Œã‚‹å½¢å¼
        return datetime.strptime(date_str, '%m/%d/%Y %H:%M:%S').replace(tzinfo=timezone.utc)
    except ValueError:
        return None

def _clean_decimal_field(value: Optional[str]) -> Optional[Decimal]:
    """é‡‘é¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ–‡å­—åˆ—ã‚’Decimalã«å¤‰æ›ã™ã‚‹"""
    if not value or not value.strip(): return None
    try:
        # ã‚«ãƒ³ãƒã‚’é™¤å»ã—ã¦Decimalã«å¤‰æ›
        cleaned_value = value.strip().replace(',', '')
        return Decimal(cleaned_value)
    except (InvalidOperation, ValueError):
        return None

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯ (ãƒãƒ«ã‚¯å‡¦ç†ã®ãŸã‚ã«Dictã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´)
# ==============================================================================

def _parse_single_row(row_list: List[str], mid: str) -> Optional[Dict[str, Any]]:
    """1è¡Œã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€DBä¿å­˜ç”¨ã®è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹"""
    
    # LinkShareå•†å“ãƒ‡ãƒ¼ã‚¿ã¯åŸºæœ¬38ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    if len(row_list) != 38: 
        return None
    
    sku = row_list[2].strip()
    if not sku: return None
    
    # ğŸš¨ ä¿®æ­£: å‹å¤‰æ›ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä»•æ§˜ã«åˆã‚ã›ã¦ä¿®æ­£
    discount_amount_dec = _clean_decimal_field(row_list[11]) # C12: Discount Amount
    sale_price_dec = _clean_decimal_field(row_list[13])      # C14: Sale Price
    retail_price_dec = _clean_decimal_field(row_list[14])     # C15: Retail Price
    shipping_dec = _clean_decimal_field(row_list[18])       # C19: Shipping
    begin_date_dt = _parse_linkshare_date(row_list[15])      # C16: Begin Date
    end_date_dt = _parse_linkshare_date(row_list[16])        # C17: End Date
    
    # LinkshareProductãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«ãƒãƒƒãƒ”ãƒ³ã‚°
    data = {
        # Key Fields
        'merchant_id': mid,
        'sku': sku,
        # C1: Link ID
        'link_id': row_list[0].strip(),
        # C2/C22: Manufacturer Name (C22ã‚’å„ªå…ˆã—ã€C2ãŒç©ºã®å ´åˆã¯C2ã‚’ä½¿ç”¨)
        # Note: æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¯C22ã‚’C2ã‚ˆã‚Šå¾Œã«ä½¿ç”¨ã—ã¦ã„ã¾ã—ãŸãŒã€Linkshareã®ä»•æ§˜ã«å¾“ã„Primary/Fallbackã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚
        'manufacturer_name': row_list[21].strip() if row_list[21].strip() else row_list[1].strip(), 
        # C4: Product Name
        'product_name': row_list[3].strip(), 
        # C5: Primary Category
        'primary_category': row_list[4].strip(),
        # C6: Sub Category
        'sub_category': row_list[5].strip(),
        # C7: Product URL
        'product_url': row_list[6].strip(),
        # C8: Image URL
        'image_url': row_list[7].strip(),
        # C9: Buy URL
        'buy_url': row_list[8].strip(),
        # C10: Short Description
        'short_description': row_list[9].strip(),
        # C11: Description
        'description': row_list[10].strip(),
        # C12: Discount Amount
        'discount_amount': discount_amount_dec,
        # C13: Discount Type
        'discount_type': row_list[12].strip(),
        # C14: Sale Price
        'sale_price': sale_price_dec,
        # C15: Retail Price
        'retail_price': retail_price_dec,
        # C16: Begin Date
        'begin_date': begin_date_dt,
        # C17: End Date
        'end_date': end_date_dt,
        # C18: Brand Name
        'brand_name': row_list[17].strip(),
        # C19: Shipping
        'shipping': shipping_dec,
        # C20: Keywords
        'keywords': row_list[19].strip(),
        # C25: Common Product Code
        'class_id': row_list[24].strip(),
        # ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (C21, C23, C24, C26-C38) ã¯æœªä½¿ç”¨ã¨ã—ã¦é™¤å¤–
    }
    return data

def _display_mapping_for_first_row(row_list: List[str]):
    """æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®å„ã‚«ãƒ©ãƒ ã¨DBãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡¨ç¤ºã™ã‚‹"""
    # LinkShareãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚° (0ã‹ã‚‰å§‹ã¾ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
    
    # ğŸš¨ ä¿®æ­£: too many values to unpack (expected 4)ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã€ã™ã¹ã¦ã®ã‚¿ãƒ—ãƒ«ã‚’4è¦ç´ ã«çµ±ä¸€
    if len(row_list) != 38:
        print(f"\n[ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ç¢ºèª] âš ï¸ ä¸æ­£ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•° ({len(row_list)} / 38) ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
        return

    # (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, DB Field Name (Source Col), ãƒ‡ãƒ¼ã‚¿å‹, Raw Value) ã®4è¦ç´ ã‚¿ãƒ—ãƒ«ã§çµ±ä¸€
    mapping_data = [
        (0, "link_id (C1: Link ID)", "str", row_list[0].strip()),
        (1, "manufacturer_name (C2: Merchant Name, Fallback)", "str", row_list[1].strip()),
        (2, "sku (C3: SKU, Primary Key)", "str", row_list[2].strip()),
        (3, "product_name (C4: Product Name)", "str", row_list[3].strip()),
        (4, "primary_category (C5: Primary Category)", "str", row_list[4].strip()),
        (5, "sub_category (C6: Sub Category)", "str", row_list[5].strip()),
        (6, "product_url (C7: Product URL)", "str", row_list[6].strip()),
        (7, "image_url (C8: Image URL)", "str", row_list[7].strip()),
        (8, "buy_url (C9: Buy URL)", "str", row_list[8].strip()),
        (9, "short_description (C10: Short Description)", "str", row_list[9].strip()),
        (10, "description (C11: Description)", "str", row_list[10].strip()),
        (11, "discount_amount (C12: Discount Amount)", "Decimal", row_list[11].strip()),
        (12, "discount_type (C13: Discount Type)", "str", row_list[12].strip()),
        (13, "sale_price (C14: Sale Price)", "Decimal", row_list[13].strip()),
        (14, "retail_price (C15: Retail Price)", "Decimal", row_list[14].strip()),
        (15, "begin_date (C16: Begin Date)", "datetime (UTC)", row_list[15].strip()),
        (16, "end_date (C17: End Date)", "datetime (UTC)", row_list[16].strip()),
        (17, "brand_name (C18: Brand Name)", "str", row_list[17].strip()),
        (18, "shipping (C19: Shipping)", "Decimal", row_list[18].strip()),
        (19, "keywords (C20: Keywords)", "str", row_list[19].strip()),
        # 5è¦ç´ ã®ã‚¿ãƒ—ãƒ«ã‚’4è¦ç´ ã«ä¿®æ­£: "C21 (Skipped)"ã‚’ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«å«ã‚ã‚‹
        (20, "C21 (Skipped)", "str", row_list[20].strip()),
        (21, "manufacturer_name (C22: Manufacturer Name, Primary)", "str", row_list[21].strip()),
        (22, "C23 (Skipped)", "str", row_list[22].strip()),
        (23, "C24 (Skipped)", "str", row_list[23].strip()),
        (24, "class_id (C25: Common Product Code)", "str", row_list[24].strip()),
        (25, "C26: Currency Unit (Skipped)", "str", row_list[25].strip()),
        (26, "C27: M1 (Skipped)", "str", row_list[26].strip()),
        (27, "C28: Impression URL (Skipped)", "str", row_list[27].strip()),
        (28, "C29 (Skipped)", "str", row_list[28].strip()),
        (29, "C30 (Skipped)", "str", row_list[29].strip()),
        (30, "C31 (Skipped)", "str", row_list[30].strip()),
        (31, "C32 (Skipped)", "str", row_list[31].strip()),
        (32, "C33 (Skipped)", "str", row_list[32].strip()),
        (33, "C34 (Skipped)", "str", row_list[33].strip()),
        (34, "C35 (Skipped)", "str", row_list[34].strip()),
        (35, "C36 (Skipped)", "str", row_list[35].strip()),
        (36, "C37 (Skipped)", "str", row_list[36].strip()),
        (37, "C38 (Skipped)", "str", row_list[37].strip()),
    ]
    
    print("\n" + "=" * 80, file=sys.stdout, flush=True)
    print("ğŸ“‹ CSVãƒ‡ãƒ¼ã‚¿è¡Œ (æœ€åˆã®1è¡Œ) ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã¨å€¤ã®ç¢ºèª", file=sys.stdout, flush=True)
    print("=" * 80, file=sys.stdout, flush=True)
    print("{:<5} {:<45} {:<30}".format("Idx", "DB Field Name (Source Col)", "Raw Value (Truncated)"), file=sys.stdout, flush=True)
    print("-" * 80, file=sys.stdout, flush=True)

    for index, db_field, data_type, raw_value in mapping_data:
        # é•·ã™ãã‚‹å€¤ã¯åˆ‡ã‚Šè©°ã‚ã¦è¡¨ç¤º
        display_value = raw_value.replace('\n', ' ').replace('\r', ' ')
        if len(display_value) > 30:
            display_value = display_value[:27] + "..."
            
        print("{:<5} {:<45} {:<30}".format(
            index, 
            f"{db_field} [{data_type}]", 
            f"'{display_value}'"
        ), file=sys.stdout, flush=True)

    print("-" * 80, file=sys.stdout, flush=True)
    print("ğŸ’¡ DBãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åãŒå¤ªå­—ã§ã€æ‹¬å¼§å†…ãŒLinkShareã®ã‚«ãƒ©ãƒ åã¨ãƒ‡ãƒ¼ã‚¿å‹ã§ã™ã€‚æœªä½¿ç”¨ã‚«ãƒ©ãƒ ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚", file=sys.stdout, flush=True)
    print("=" * 80 + "\n", file=sys.stdout, flush=True)


def _bulk_import_products(mid: str, product_data_list: List[Dict[str, Any]]) -> Tuple[int, int, int]:
    """
    åé›†ã•ã‚ŒãŸå•†å“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ«ã‚¯ã§DBã«ä¿å­˜/æ›´æ–°ã™ã‚‹ã€‚
    è¿”ã‚Šå€¤: (åˆè¨ˆå‡¦ç†è¡Œæ•°, ä½œæˆè¡Œæ•°, æ›´æ–°è¡Œæ•°)
    """
    if not product_data_list or LinkshareProduct == DummyModel:
        return 0, 0, 0

    incoming_sku_map = {data['sku']: data for data in product_data_list}
    skus_to_check = list(incoming_sku_map.keys())
    
    to_create = []
    to_update = []
    
    # 1. æ—¢å­˜è£½å“ã®ã‚¯ã‚¨ãƒª
    # Command.handleã§ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãŒé–‹å§‹ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€select_for_update()ã¯æ’ä»–ãƒ­ãƒƒã‚¯ã‚’ã‹ã‘ã‚‹
    existing_products = LinkshareProduct.objects.filter(
        merchant_id=mid,
        sku__in=skus_to_check
    )
    
    existing_sku_map = {p.sku: p for p in existing_products}
    
    # 2. ä½œæˆ/æ›´æ–°ã®åˆ†é¡ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æº–å‚™
    # bulk_updateã®æ›´æ–°å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒªã‚¹ãƒˆ
    update_fields = [
        'link_id', 'manufacturer_name', 'product_name', 'primary_category', 'sub_category',
        'product_url', 'image_url', 'buy_url', 'short_description', 'description', 
        'discount_amount', 'discount_type', 'sale_price', 'retail_price', 'begin_date', 
        'end_date', 'brand_name', 'shipping', 'keywords', 'class_id',
    ]

    for sku, data in incoming_sku_map.items():
        if sku in existing_sku_map:
            # æ›´æ–° (æ—¢å­˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä¸Šæ›¸ã)
            product_instance = existing_sku_map[sku]
            for key, value in data.items():
                # SKUã¨MIDã¯æ›´æ–°å¯¾è±¡å¤–
                if key in update_fields:
                    setattr(product_instance, key, value)
            to_update.append(product_instance)
        else:
            # ä½œæˆ (æ–°è¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹)
            to_create.append(LinkshareProduct(**data))
    
    created_count = 0
    updated_count = 0
    
    # 3. ãƒãƒ«ã‚¯ä½œæˆ
    if to_create:
        try:
            LinkshareProduct.objects.bulk_create(to_create, batch_size=5000)
            created_count = len(to_create)
        except IntegrityError as e:
            print(f" âŒ ãƒãƒ«ã‚¯ä½œæˆä¸­ã«IntegrityErrorãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
    
    # 4. ãƒãƒ«ã‚¯æ›´æ–°
    if to_update:
        try:
            # bulk_updateã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒªã‚¹ãƒˆã¨æ›´æ–°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¸¡ã™
            LinkshareProduct.objects.bulk_update(to_update, update_fields, batch_size=5000)
            updated_count = len(to_update)
        except Exception as e:
            print(f" âŒ ãƒãƒ«ã‚¯æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
            
    return created_count + updated_count, created_count, updated_count

# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜ã‚’çµ±åˆã—ãŸãƒ¡ã‚¤ãƒ³å‡¦ç† (ãƒãƒ«ã‚¯å‡¦ç†å¯¾å¿œ)
# ==============================================================================

def parse_and_process_file(local_path: str, mid: str) -> Tuple[bool, int]:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£å‡ã€ãƒ‘ãƒ¼ã‚¹ã—ã€DBã«ãƒãƒ«ã‚¯ã§ä¿å­˜ã™ã‚‹"""
    print(f"ğŸ’¾ [DEBUG {mid}] STARTING PARSE: {os.path.basename(local_path)}.", file=sys.stdout, flush=True)

    delimiter = FIXED_DELIMITER
    delimiter_name = FIXED_DELIMITER_NAME
    encoding_list = ['utf-8', 'cp932', 'shift_jis', 'euc_jp','latin-1']
    total_saved_rows = 0
    
    temp_txt_path = None
    path_to_open = local_path
    file_open_func = lambda path, enc: open(path, 'r', encoding=enc, errors='replace')
    
    try:
        # GZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç† (gunzipå¤–éƒ¨ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨)
        if local_path.endswith('.gz'):
            print(f"ğŸ”¥ [MID: {mid}] .gzãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤–éƒ¨ã®gunzipã§ä¸€æ™‚çš„ã«è§£å‡ã—ã¾ã™ã€‚", file=sys.stdout, flush=True)
            
            temp_txt_path = local_path.replace('.gz', '.txt') 
            
            print(f">>> [MID: {mid}] DECOMPRESS CALL (gunzip) -> {temp_txt_path}", file=sys.stdout, flush=True) 

            try:
                with open(temp_txt_path, 'wb') as f_out:
                    subprocess.run(
                        ['gunzip', '-c', local_path], 
                        stdout=f_out, 
                        check=True, 
                        timeout=300 # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®š
                    )
                print(f"<<< [MID: {mid}] DECOMPRESS SUCCESS ({human_readable_size(os.path.getsize(temp_txt_path))})", file=sys.stdout, flush=True)
                path_to_open = temp_txt_path
            except subprocess.CalledProcessError as e:
                print(f"âŒ [MID: {mid}] gunzipã‚³ãƒãƒ³ãƒ‰å¤±æ•—: {e}.", file=sys.stdout, flush=True)
                return False, 0
            except FileNotFoundError:
                # ğŸš¨ gunzipã‚³ãƒãƒ³ãƒ‰ãŒãªã„å ´åˆã®æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                print(f"âŒ [MID: {mid}] **gunzipã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚** (Dockerã‚³ãƒ³ãƒ†ãƒŠã«`gzip`ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚)", file=sys.stdout, flush=True)
                return False, 0
            except subprocess.TimeoutExpired:
                print(f"âŒ [MID: {mid}] gunzipã‚³ãƒãƒ³ãƒ‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
                return False, 0
            
        # --- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¨æ¸¬ã—ãªãŒã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ãƒ«ãƒ¼ãƒ— ---
        for encoding in encoding_list:
            try:
                processed_rows = 0
                product_data_list: List[Dict[str, Any]] = []
                is_likely_delimiter = False
                advertiser_name = ""
                
                print(f"ğŸ” [DEBUG {mid}] è©¦è¡Œé–‹å§‹ (ENC: {encoding}): OPENING {os.path.basename(path_to_open)}", file=sys.stdout, flush=True)

                # 1. ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿ã¨ãƒ‡ãƒªãƒŸã‚¿/ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œè¨¼ 
                with file_open_func(path_to_open, encoding) as f_text_test:
                    print(f"<<< [MID: {mid}] OPEN SUCCESS (ENC: {encoding})", file=sys.stdout, flush=True) 
                    
                    sample_lines: List[str] = []
                    for _ in range(50):
                        line = f_text_test.readline()
                        if not line: break
                        sample_lines.append(line)
                    
                    if not sample_lines:
                        print(f" âŒ [DEBUG {mid}] æ—©æœŸæ¤œè¨¼å¤±æ•— (ENC: {encoding}): ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚", file=sys.stdout, flush=True)
                        continue

                    # ğŸš¨ ä¿®æ­£: 1è¡Œç›® (HDR) ã‹ã‚‰ Advertiser Name (ãƒ¡ãƒ¼ã‚«ãƒ¼å) ã‚’å–å¾—
                    header_line = sample_lines[0].strip()
                    if header_line.startswith('HDR'): 
                        header_fields = header_line.split(delimiter)
                        # HDR|MID|Advertiser Name|Timestamp
                        if len(header_fields) >= 3:
                            advertiser_name = header_fields[2].strip()
                            print(f"ğŸ’¡ [DEBUG {mid}] HDRãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰Advertiser Nameã‚’å–å¾—ã—ã¾ã—ãŸ: '{advertiser_name}'", file=sys.stdout, flush=True)
                        else:
                            print(f" âŒ [DEBUG {mid}] ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œè¨¼å¤±æ•— (ENC: {encoding}): HDRãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•° ({len(header_fields)}) ãŒä¸æ­£ã§ã™ã€‚", file=sys.stdout, flush=True)
                            continue
                    else:
                        print(f" âŒ [DEBUG {mid}] ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œè¨¼å¤±æ•— (ENC: {encoding}): HDRã§å§‹ã¾ã£ã¦ã„ã¾ã›ã‚“ã€‚", file=sys.stdout, flush=True)
                        continue
                        
                    # 2è¡Œç›® (ã‚«ãƒ©ãƒ åãƒ˜ãƒƒãƒ€ãƒ¼) ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                    if len(sample_lines) < 2:
                        print(f" âŒ [DEBUG {mid}] æ—©æœŸæ¤œè¨¼å¤±æ•— (ENC: {encoding}): ã‚«ãƒ©ãƒ åãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", file=sys.stdout, flush=True)
                        continue

                    # ãƒ‡ãƒ¼ã‚¿è¡Œæ¤œè¨¼ (38ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰) ã¯3è¡Œç›®ã‹ã‚‰è¡Œã†
                    valid_data_lines = 0
                    for line in sample_lines[2:]: # 3è¡Œç›® (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2) ã‹ã‚‰é–‹å§‹
                        fields = line.strip().split(delimiter)
                        if len(fields) != 38: break
                        # C1 (Link ID) ãŒæ•´æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ãƒã‚§ãƒƒã‚¯
                        try:
                            # 1è¡Œã§ã‚‚38ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ§‹æˆã•ã‚Œã¦ã„ã‚Œã°ã€ãã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¡ç”¨
                            int(fields[0].strip()) 
                            is_likely_delimiter = True
                            valid_data_lines += 1
                        except ValueError: break
                        
                        if valid_data_lines >= 5: break # 5è¡Œæ¤œè¨¼ã§ããŸã‚‰ååˆ†
                        
                    if is_likely_delimiter:
                        print(f"ğŸ” [DEBUG {mid}] æ—©æœŸæ¤œè¨¼æˆåŠŸ: ENC: {encoding}, DELIM: {delimiter_name}", file=sys.stdout, flush=True)
                    
                    if not is_likely_delimiter:
                        continue # æ¬¡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ

                print(f" â¡ï¸ [MID: {mid}] è©¦è¡Œ: ENC: {encoding}, DELIM: {delimiter_name}ã€‚æœ¬ç•ªãƒ‘ãƒ¼ã‚¹é–‹å§‹...", file=sys.stdout, flush=True)

                # 2. æœ¬ç•ªãƒ‘ãƒ¼ã‚¹: æˆåŠŸã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åº¦é–‹ãã€CSVãƒªãƒ¼ãƒ€ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿åé›†
                with file_open_func(path_to_open, encoding) as f_main:
                    f_main.readline() # 1è¡Œç›®: HDRãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ— (ã™ã§ã«èª­ã¿è¾¼ã¿æ¸ˆã¿)
                    
                    # 2è¡Œç›®: ã‚«ãƒ©ãƒ åãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    column_header_line = f_main.readline()
                    print(f"ğŸ’¡ [MID: {mid}] ã‚«ãƒ©ãƒ åãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ (ä¾‹: {column_header_line.strip()[:60]}...)ã€‚", file=sys.stdout, flush=True)
                    
                    csv_reader = csv.reader(f_main, delimiter=delimiter, quotechar='"')
                    
                    # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡¨ç¤ºã—ã€ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    first_row: Optional[List[str]] = None
                    try:
                        # æœ€åˆã®è¡Œã‚’èª­ã¿è¾¼ã¿
                        current_row = next(csv_reader) 
                        
                        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒæ¼ã‚Œã¦ã„ã‚‹å ´åˆã‚’è€ƒæ…®ã—ã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã‚‹ã¾ã§ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
                        max_skip_count = 5
                        current_skip_count = 0
                        
                        while current_skip_count < max_skip_count:
                            if not current_row or current_row[0].strip().startswith('TRL'):
                                break # ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ‚ç«¯ã¾ãŸã¯ãƒˆãƒ¬ãƒ¼ãƒ©ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰
                            
                            if len(current_row) != 38:
                                # ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°ãŒä¸æ­£ãªå ´åˆã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã®è¡Œã¸
                                print(f" âš ï¸ [MID: {mid}] è¡Œã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°ãŒä¸æ­£ã§ã™ ({len(current_row)} / 38)ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚", file=sys.stdout, flush=True)
                                current_skip_count += 1
                                current_row = next(csv_reader)
                                continue
                                
                            # C14 (Sale Price, Index 13) ãŒãƒ˜ãƒƒãƒ€ãƒ¼æ–‡å­—åˆ—ã®å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã¨è¦‹ãªã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                            # 'amount', 'price', 'retail_price' ãªã©
                            sale_price_raw = current_row[13].strip().lower()
                            if sale_price_raw and not sale_price_raw.replace(',', '', 1).replace('.', '', 1).isdigit(): 
                                print(f" âš ï¸ [MID: {mid}] C14 (Sale Price) ã®å€¤ãŒãƒ˜ãƒƒãƒ€ãƒ¼æ–‡å­—åˆ—ã®ã‚ˆã†ã§ã™ ('{sale_price_raw[:20]}')ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚", file=sys.stdout, flush=True)
                                current_skip_count += 1
                                current_row = next(csv_reader)
                                continue
                            
                            # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã£ãŸ
                            first_row = current_row
                            break
                        
                        if not first_row:
                             print(f" âŒ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", file=sys.stdout, flush=True)
                             continue

                    except StopIteration:
                        print(f" âŒ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", file=sys.stdout, flush=True)
                        continue
                    except Exception as e:
                        print(f" âŒ [MID: {mid}] æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stdout, flush=True)
                        continue

                    # 1è¡Œç›®ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã¨å€¤ã®è¡¨ç¤º
                    if first_row:
                        try:
                            _display_mapping_for_first_row(first_row) 
                            
                            # 1è¡Œç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                            parsed_data = _parse_single_row(first_row, mid)
                            if parsed_data:
                                product_data_list.append(parsed_data)
                                processed_rows += 1
                        except Exception as e:
                            # ä¿®æ­£ã—ãŸã¯ãšã® unpacking ã‚¨ãƒ©ãƒ¼ãŒã¾ã å‡ºã‚‹å ´åˆã¯ã“ã“ã§æ•æ‰
                            print(f" âŒ [MID: {mid}] æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ‘ãƒ¼ã‚¹/è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stdout, flush=True)
                            traceback.print_exc(file=sys.stdout)
                            continue # æ¬¡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¸

                    # 2è¡Œç›®ä»¥é™ã®å‡¦ç†
                    for row in csv_reader:
                        # ç©ºè¡Œã¾ãŸã¯ãƒˆãƒ¬ãƒ¼ãƒ©ãƒ¼è¡Œ (TRL) ã®å‡¦ç†
                        if not row or (len(row) == 1 and not row[0].strip()):
                            continue 
                        if row[0].strip().startswith('TRL'):
                            continue
                        
                        # ãƒ‡ãƒ¼ã‚¿è¡Œ (38ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰) ã®ã¿ã‚’å‡¦ç†
                        if len(row) == 38:
                            parsed_data = _parse_single_row(row, mid)
                            if parsed_data:
                                product_data_list.append(parsed_data)
                            
                            processed_rows += 1
                        
                        if processed_rows % 10000 == 0 and processed_rows > 0:
                            print(f" ğŸ”„ [MID: {mid}] å‡¦ç†ä¸­... {processed_rows:,} è¡Œãƒ‘ãƒ¼ã‚¹æ¸ˆã¿", file=sys.stdout, flush=True)

                print(f" âœ… [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ã‚¹å®Œäº†ã€‚åé›†ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(product_data_list):,} ä»¶", file=sys.stdout, flush=True)
                
                # 3. ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å®Ÿè¡Œ
                total_saved_rows, created_count, updated_count = _bulk_import_products(mid, product_data_list)
                
                print(f"âœ… [MID: {mid}] DBã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ã€‚ä½œæˆ: {created_count:,} ä»¶, æ›´æ–°: {updated_count:,} ä»¶, åˆè¨ˆ: {total_saved_rows:,} ä»¶", file=sys.stdout, flush=True)

                return True, total_saved_rows

            except Exception as e:
                # ãƒ‘ãƒ¼ã‚¹/ä¿å­˜ä¸­ã®ã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«å‡ºåŠ›ã—ã€æ¬¡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
                print(f" âŒ è©¦è¡Œå¤±æ•— (ENC: {encoding}, DELIM: {delimiter_name}): {type(e).__name__} - {e}", file=sys.stdout, flush=True)
                pass

        print(f" âš ï¸ [MID: {mid}] å…¨ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è©¦è¡Œã§ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
        return False, 0

    except Exception as fatal_e:
        # è‡´å‘½çš„ãªãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¯ãƒ©ãƒƒã‚·ãƒ¥
        print(f"âŒ [MID: {mid}] å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªã‚¯ãƒ©ãƒƒã‚·ãƒ¥ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚åŸå› ä¸æ˜ã®å ´æ‰€ã§ã®ã‚¨ãƒ©ãƒ¼ã€‚", file=sys.stdout, flush=True)
        print("--- FATAL ERROR TRACEBACK START ---", file=sys.stdout, flush=True)
        traceback.print_exc(file=sys.stdout)
        print("--- FATAL ERROR TRACEBACK END ---", file=sys.stdout, flush=True)
        return False, 0

    finally:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å‡¦ç†
        try:
            if os.path.exists(local_path):
                os.remove(local_path)
                print(f"ğŸ—‘ï¸ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ƒãƒ•ã‚¡ã‚¤ãƒ« ({os.path.basename(local_path)}) ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
        except Exception as e:
            print(f"âš ï¸ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {os.path.basename(local_path)} ({e})", file=sys.stdout, flush=True)

        try:
            if temp_txt_path and os.path.exists(temp_txt_path):
                os.remove(temp_txt_path)
                print(f"ğŸ—‘ï¸ [MID: {mid}] ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ« ({os.path.basename(temp_txt_path)}) ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
        except Exception as e:
            print(f"âš ï¸ [MID: {mid}] ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {os.path.basename(temp_txt_path)} ({e})", file=sys.stdout, flush=True)
        
# ==============================================================================
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
# ==============================================================================

class DownloadProgress:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—è¡¨ç¤ºç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""
    def __init__(self, total_size: int, file_pointer: Any):
        self.total_size = total_size
        self.downloaded = 0
        self.start_time = time.time()
        self.last_print_len = 0
        self.file_pointer = file_pointer
    
    def update(self, data: bytes):
        """FTPã®retrbinaryã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã€‚ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ã¨é€²æ—è¡¨ç¤ºã‚’è¡Œã†ã€‚"""
        self.file_pointer.write(data)
        self.downloaded += len(data)
        # 1MBã”ã¨ã€ã¾ãŸã¯å®Œäº†æ™‚ã«é€²æ—ã‚’æ›´æ–°
        if self.downloaded % (1024 * 1024) == 0 or self.downloaded == self.total_size:
            percent = (self.downloaded / self.total_size) * 100 if self.total_size > 0 else 0
            elapsed = time.time() - self.start_time
            speed = (self.downloaded / elapsed) if elapsed > 0 else 0
            progress_str = (
                f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {human_readable_size(self.downloaded)} "
                f"/ {human_readable_size(self.total_size)} "
                f"({percent:3.1f}%) "
                f"é€Ÿåº¦: {human_readable_size(speed).replace(' B', 'B/s')}"
            )
            # ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ã§ä¸Šæ›¸ãè¡¨ç¤º
            print('\r' + progress_str, end='', flush=True)
            self.last_print_len = len(progress_str)

def download_file(filename: str, local_path: str, file_size: int, mid: str) -> Tuple[bool, int]:
    """FTPã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã«æ¸¡ã™"""
    print(f"\nğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {filename} -> {local_path} ({human_readable_size(file_size)})", file=sys.stdout, flush=True)
    print(f"ğŸ’¡ [DEBUG {mid}] download_file å‘¼ã³å‡ºã—é–‹å§‹ã€‚", file=sys.stdout, flush=True)

    ftp = _get_ftp_client()
    if not ftp:
        print("âŒ [DEBUG] download_file çµ‚äº† (FTPæ¥ç¶šå¤±æ•—)ã€‚", file=sys.stderr, flush=True)
        return False, 0

    download_success = False
    downloaded_size = 0
    saved_rows = 0

    try:
        print(f"ğŸ’¡ [DEBUG {mid}] open('{local_path}', 'wb') é–‹å§‹ã€‚", file=sys.stdout, flush=True)
        with open(local_path, 'wb') as fp:
            print(f"ğŸ’¡ [DEBUG {mid}] open æˆåŠŸã€‚ftp.retrbinary å‘¼ã³å‡ºã—é–‹å§‹ã€‚", file=sys.stdout, flush=True)
            progress = DownloadProgress(file_size, fp)
            # ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            ftp.retrbinary(f'RETR {filename}', progress.update)
            downloaded_size = progress.downloaded
        print(f"ğŸ’¡ [DEBUG {mid}] open/ftp.retrbinary ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†ã€‚", file=sys.stdout, flush=True)

        # é€²æ—è¡¨ç¤ºã®ã‚¯ãƒªã‚¢ã¨å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\r" + " " * progress.last_print_len, end='', flush=True)
        print("\râœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚", file=sys.stdout, flush=True)
        download_success = True

    except ftplib.all_errors as e:
        print(f"\nâŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«FTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (MID: {mid}): {type(e).__name__} - {e}", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (MID: {mid}): {type(e).__name__} - {e}", file=sys.stderr, flush=True)
    finally:
        try:
            ftp.quit()
        except Exception:
            pass
        print(f"ğŸ’¡ [DEBUG {mid}] finallyãƒ–ãƒ­ãƒƒã‚¯çµ‚äº† (ftp.quitå®Ÿè¡Œæ¸ˆã¿)ã€‚", file=sys.stdout, flush=True)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã®æ¤œè¨¼ã¨ãƒ‘ãƒ¼ã‚¹
    print(f"ğŸ“ [DEBUG {mid}] æ¤œè¨¼: æœŸå¾…ã‚µã‚¤ã‚º={file_size}, å®Ÿã‚µã‚¤ã‚º={downloaded_size}", file=sys.stdout, flush=True)

    if download_success and os.path.exists(local_path): 
        
        if downloaded_size != file_size:
            # ã‚µã‚¤ã‚ºä¸ä¸€è‡´ã®å ´åˆã€è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¤ã¤å‡¦ç†ã‚’ç¶šè¡Œ (Linkshare FTPã§ã¯ã‚µã‚¤ã‚ºãŒå¤‰å‹•ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚)
            print(f"âš ï¸ [DEBUG {mid}] ã‚µã‚¤ã‚ºä¸ä¸€è‡´ã‚’æ¤œå‡º (æœŸå¾…: {file_size}, å®Ÿæ¸¬: {downloaded_size})ã€‚ãƒ‘ãƒ¼ã‚¹ã‚’ç¶šè¡Œã—ã¾ã™ã€‚", file=sys.stdout, flush=True)
        else:
            print(f"ğŸ’¡ [DEBUG {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ¡ä»¶ãƒã‚§ãƒƒã‚¯é€šéã€‚ã‚µã‚¤ã‚º: {downloaded_size} ãƒã‚¤ãƒˆã€‚", file=sys.stdout, flush=True)

        # ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜å‡¦ç†ã‚’å®Ÿè¡Œ
        try:
            print(f"ğŸ‘‰ [DEBUG {mid}] PARSE_FUNC_CALL TRYãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ã€‚", file=sys.stdout, flush=True) 

            # æœ¬æ¥ã®å‡¦ç†: parse_and_process_file ã‚’å®Ÿè¡Œ
            success, saved_rows = parse_and_process_file(local_path, mid)
            
            print(f"ğŸ‘ˆ [DEBUG {mid}] PARSE_FUNC_CALL END (Success: {success})", file=sys.stdout, flush=True)

            if success:
                print(f"ğŸ’¡ [DEBUG {mid}] download_file æ­£å¸¸çµ‚äº† (True, {saved_rows})", file=sys.stdout, flush=True)
                return success, saved_rows

        except Exception as e:
            # parse_and_process_fileå†…ã§ã®ã‚¨ãƒ©ãƒ¼ã‚’æ•æ‰
            print(f"\nâŒ [MID: {mid}] parse_and_process_file ã®å‘¼ã³å‡ºã—ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", file=sys.stdout, flush=True)
            print(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}", file=sys.stdout, flush=True)
            print("--- CALL ERROR TRACEBACK START ---", file=sys.stdout, flush=True)
            traceback.print_exc(file=sys.stdout)
            print("--- CALL ERROR TRACEBACK END ---", file=sys.stdout, flush=True)

    print(f"ğŸ’¡ [DEBUG {mid}] download_file çµ‚äº† (False, 0) ã¾ãŸã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã€‚", file=sys.stdout, flush=True)
    return False, 0


# ==============================================================================
# Django Management Command ã®å®šç¾©
# ==============================================================================

class Command(BaseCommand):
    """LinkShare FTPã‹ã‚‰ãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€DBã«ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹Djangoã‚³ãƒãƒ³ãƒ‰"""
    help = 'LinkShare FTPã‹ã‚‰ãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€LinkshareProductãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚'

    def add_arguments(self, parser):
        parser.add_argument(
            '--mid',
            type=str,
            help='å‡¦ç†ã™ã‚‹ç‰¹å®šã®ãƒãƒ¼ãƒãƒ£ãƒ³ãƒˆID (MID) ã‚’æŒ‡å®šã—ã¾ã™ã€‚',
            default=None
        )
        parser.add_argument(
            '--limit',
            type=int,
            default=5,
            help='ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®ãŸã‚ã€å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§æ•°ã‚’æŒ‡å®šã—ã¾ã™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)ã€‚'
        )

    def handle(self, *args, **options):
        self.stdout.write(self.style.SUCCESS("--- LinkShare ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰é–‹å§‹ (ãƒãƒ«ã‚¯å‡¦ç†) ---"))
        print(f"ğŸ’¡ [DEBUG] Command.handle å®Ÿè¡Œé–‹å§‹ã€‚", file=sys.stdout, flush=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è‡ªå‹•ä½œæˆ
        if not os.path.exists(DOWNLOAD_DIR):
            try:
                os.makedirs(DOWNLOAD_DIR)
                self.stdout.write(self.style.SUCCESS(f"ğŸ“‚ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {DOWNLOAD_DIR} ã‚’ä½œæˆã—ã¾ã—ãŸã€‚"))
            except Exception as e:
                self.stderr.write(self.style.ERROR(f"ğŸš¨ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {DOWNLOAD_DIR} ({e})"))
                return
        
        target_mid = options['mid']
        limit = options['limit']

        # 1. FTPãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—
        self.stdout.write("ğŸ” FTPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­... (1GBæœªæº€ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«é™å®š)")
        DATA_PATH = os.getenv("LINKSHARE_BS_DATA_PATH", "")

        try:
            mid_list = get_ftp_mid_list(DATA_PATH)
        except Exception as e:
            self.stderr.write(self.style.ERROR(f"FTPãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}"))
            print("--- get_ftp_mid_list ERROR TRACEBACK START ---", file=sys.stdout, flush=True)
            traceback.print_exc(file=sys.stdout)
            print("--- get_ftp_mid_list ERROR TRACEBACK END ---", file=sys.stdout, flush=True)
            return

        if not mid_list:
            self.stdout.write(self.style.WARNING("å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‹ã€FTPæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚"))
            return

        self.stdout.write(f"\nâœ… FTPã‹ã‚‰ä»¥ä¸‹ã® **{len(mid_list)}** ä»¶ã®å‡¦ç†å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ (1GBæœªæº€):")
        # ãƒªã‚¹ãƒˆè¡¨ç¤º
        self.stdout.write("{:<10} {:<10} {:<60}".format("MID", "SIZE", "FILENAME"))
        self.stdout.write("-" * 80)
        for mid, filename, file_type, mtime_dt, file_size in mid_list:
            size_hr = human_readable_size(file_size)
            self.stdout.write("{:<10} {:<10} {:<60}".format(mid, size_hr, filename))
        self.stdout.write("-" * 80)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨åˆ¶é™
        if target_mid:
            mid_list = [item for item in mid_list if item[0] == target_mid]
            if not mid_list:
                self.stdout.write(self.style.WARNING(f"æŒ‡å®šã•ã‚ŒãŸMID ({target_mid}) ã«è©²å½“ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"))
                return
        
        files_to_process = mid_list[:limit]
        if not files_to_process:
            self.stdout.write(self.style.WARNING(f"åˆ¶é™æ•° ({limit}) ã«ã‚ˆã‚Šã€å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"))
            return

        total_processed_files = 0
        total_saved_rows = 0

        self.stdout.write(f"\nğŸš€ ä¸Šä½ {len(files_to_process)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")

        # 2. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ‘ãƒ¼ã‚¹ã®å®Ÿè¡Œ (ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†)
        for mid, filename, file_type, _, file_size in files_to_process:
            local_file_path = os.path.join(DOWNLOAD_DIR, filename)
            print(f"â­ [DEBUG {mid}] download_file å‘¼ã³å‡ºã—å‰ã€‚", file=sys.stdout, flush=True)

            current_saved_rows = 0
            success = False

            try: 
                # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å…¨ä½“ã‚’ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§å›²ã‚€
                with transaction.atomic():
                    success, current_saved_rows = download_file(filename, local_file_path, file_size, mid)
                print(f"â­ [DEBUG {mid}] download_file å‘¼ã³å‡ºã—å¾Œã€‚çµæœ: ({success}, {current_saved_rows})", file=sys.stdout, flush=True)
            
            except Exception as e:
                # download_fileå†…ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ã‚„ã€DBæ“ä½œä¸­ã®IntegrityErrorãªã©ã‚’æ•æ‰
                self.stderr.write(self.style.ERROR(f"\n[MID: {mid}] å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚"))
                self.stderr.write(self.style.ERROR(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}"))
                self.stderr.write(self.style.ERROR("--- download_file CALL ERROR TRACEBACK START ---"))
                self.stderr.write(traceback.format_exc()) 
                self.stderr.write(self.style.ERROR("--- download_file CALL ERROR TRACEBACK END ---"))
                success = False
                current_saved_rows = 0 # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã‚Œã‚‹ãŸã‚ 0 ã«ãƒªã‚»ãƒƒãƒˆ
            
            print(f"â­ [DEBUG {mid}] çµæœå‡¦ç†ä¸­ã€‚Success: {success}", file=sys.stdout, flush=True)

            if success:
                total_processed_files += 1
                total_saved_rows += current_saved_rows
                self.stdout.write(self.style.SUCCESS(f"\n[MID: {mid}] å‡¦ç†å®Œäº†ã€‚DBä¿å­˜ä»¶æ•°: {current_saved_rows:,} ä»¶"))
            else:
                self.stdout.write(self.style.ERROR(f"\n[MID: {mid}] å‡¦ç†å¤±æ•— (ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯)ã€‚"))

        self.stdout.write(self.style.SUCCESS(f"\n--- ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰å®Œäº†: {total_processed_files} / {len(files_to_process)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«å‡¦ç†ã•ã‚Œã¾ã—ãŸ (åˆè¨ˆ {total_saved_rows:,} è¡Œä¿å­˜) ---"))
        print(f"ğŸ’¡ [DEBUG] Command.handle å®Ÿè¡Œçµ‚äº†ã€‚", file=sys.stdout, flush=True)