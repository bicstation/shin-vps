import time
import csv
import random
import os
from playwright.sync_api import sync_playwright

def scrape_acer_to_csv_realtime():
    # 1. ä¿å­˜ãƒ‘ã‚¹ã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«å›ºå®š
    base_dir = os.path.dirname(os.path.abspath(__file__))
    output_csv = os.path.join(base_dir, "acer_detailed_final.csv")
    
    base_url = "https://store.acer.com/ja-jp/notebooks?p="
    last_page_data = set()
    total_count = 0

    # CSVã‚’æ–°è¦ä½œæˆã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã‚€
    with open(output_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['name', 'price', 'url', 'image_url', 'description'])

    with sync_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹è§£é™¤ + é€šä¿¡ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
        browser = p.chromium.launch(
            headless=False, 
            args=['--disable-http2', '--disable-blink-features=AutomationControlled']
        )
        
        context = browser.new_context(
            viewport={'width': 1280, 'height': 900},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = context.new_page()

        page_num = 1
        while True:
            current_url = f"{base_url}{page_num}"
            print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            
            try:
                # ãƒšãƒ¼ã‚¸é·ç§»ã€‚domcontentloadedã‚ˆã‚Šç¢ºå®Ÿãªcommitã‚’ä½¿ç”¨
                page.goto(current_url, wait_until="commit", timeout=90000)
                
                # ğŸ’¡ ã‚µã‚¤ãƒˆãŒé‡ã„ãŸã‚ã€è¦ç´ ã‚’æ¢ã™å‰ã«æ•°ç§’é–“å¾…æ©Ÿï¼ˆã“ã‚Œé‡è¦ã§ã™ï¼‰
                page.wait_for_timeout(5000) 

                # å•†å“ãƒªã‚¹ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§æœ€å¤§20ç§’ç²˜ã‚‹
                try:
                    page.wait_for_selector(".product-item-info", state="attached", timeout=20000)
                except:
                    print(f"ğŸ ã“ã‚Œä»¥ä¸Šå•†å“ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                    break

                # ğŸ’¡ LazyLoadå¯¾ç­–ï¼šå°åˆ»ã¿ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ç”»åƒã‚’ç¢ºå®šã•ã›ã‚‹
                for _ in range(8):
                    page.evaluate("window.scrollBy(0, 700)")
                    time.sleep(0.4)
                
                # å–å¾—å‰ã®æœ€çµ‚å¾…æ©Ÿ
                page.wait_for_timeout(2000)

                items = page.query_selector_all(".product-item-info")
                current_page_names = []

                # 2. ãƒ¡ãƒ¢ãƒªå¯¾ç­–ï¼šãƒšãƒ¼ã‚¸å†…ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’1ä»¶ãšã¤å‡¦ç†ã—ã¦å³ä¿å­˜
                for item in items:
                    try:
                        name_el = item.query_selector(".product-item-name a")
                        price_el = item.query_selector("span.price")
                        
                        if name_el and price_el:
                            name = name_el.inner_text().strip()
                            item_url = name_el.get_attribute("href") or ""
                            
                            # ç”»åƒå–å¾—ï¼ˆpixel.jpgã‚’å¾¹åº•æ’é™¤ï¼‰
                            img_el = item.query_selector(".product-image-photo")
                            image_url = ""
                            if img_el:
                                # è¦ç´ ã‚’è¦–ç•Œã«å…¥ã‚Œã¦ç¢ºå®Ÿã«URLã‚’ç”Ÿæˆã•ã›ã‚‹
                                img_el.scroll_into_view_if_needed()
                                for attr in ["data-src", "src", "data-original"]:
                                    candidate = img_el.get_attribute(attr)
                                    if candidate and "pixel.jpg" not in candidate and candidate.startswith("http"):
                                        image_url = candidate
                                        break
                            
                            # âš ï¸ ç”»åƒãŒå–ã‚Œãªã„ï¼ˆpixelã®ã¾ã¾ï¼‰ãªã‚‰ä¿å­˜ã—ãªã„
                            if not image_url:
                                print(f"âš ï¸ ç”»åƒå–å¾—å¾…ã¡ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {name[:20]}...")
                                continue

                            # ä¾¡æ ¼ã¨è©³ç´°èª¬æ˜
                            price_text = price_el.inner_text()
                            price = int(price_text.replace('Â¥', '').replace(',', '').replace(' ', ''))
                            desc_el = item.query_selector(".product-item-details .description") or item.query_selector(".product-item-details")
                            description = desc_el.inner_text().replace('\n', ' / ').strip() if desc_el else ""

                            current_page_names.append(name)

                            # ğŸ’¡ 1ä»¶ãšã¤CSVã¸è¿½è¨˜ï¼ˆãƒ¡ãƒ¢ãƒªã‚’é£Ÿã‚ãšã€ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã«ã‚‚å¼·ã„ï¼‰
                            with open(output_csv, 'a', newline='', encoding='utf-8') as f:
                                writer = csv.writer(f)
                                writer.writerow([name, price, item_url, image_url, description])
                            
                            total_count += 1
                    except:
                        continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå‰ã®ãƒšãƒ¼ã‚¸ã¨åŒã˜å†…å®¹ãªã‚‰çµ‚äº†ï¼‰
                current_page_set = set(current_page_names)
                if not current_page_names or (last_page_data and current_page_set.issubset(last_page_data)):
                    print("ğŸ å…¨ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                    break

                last_page_data = current_page_set
                print(f"âœ… ãƒšãƒ¼ã‚¸ {page_num} å®Œäº†ï¼ˆç´¯è¨ˆ: {total_count}ä»¶ï¼‰")
                
                page_num += 1
                # ã‚µãƒ¼ãƒãƒ¼ã¸ã®ãƒãƒŠãƒ¼ã¨ã—ã¦å¾…æ©Ÿ
                time.sleep(random.uniform(3.0, 5.0))

            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                break

        browser.close()
    
    print(f"\nâœ¨ å®Œäº†ï¼CSVã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚Šã¾ã™: {output_csv}")

if __name__ == "__main__":
    scrape_acer_to_csv_realtime()