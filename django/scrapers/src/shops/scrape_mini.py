import os
import django
import re
import hashlib
import time
import random
import json
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
# Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã®å®Ÿè¡Œã‚’æƒ³å®šã—ã€Djangoç’°å¢ƒã‚’åˆæœŸåŒ–
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

# ==========================================
# ğŸ”‘ 1. è¨­å®šæƒ…å ±
# ==========================================
AFFILIATE_ID = "389"
MAKER_NAME = "MINISFORUM"
BASE_DOMAIN = "www.minisforum.jp"

# ==========================================
# ğŸ› ï¸ 2. è§£æã‚¨ãƒ³ã‚¸ãƒ³
# ==========================================

def extract_detailed_specs(soup, product_name):
    """
    ã‚¿ã‚¤ãƒˆãƒ«ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€ãŠã‚ˆã³ãƒšãƒ¼ã‚¸å†…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰
    CPU / GPU / RAM / SSD ã‚’æŠ½å‡ºã—ã¦ã‚¹ãƒ©ãƒƒã‚·ãƒ¥åŒºåˆ‡ã‚Šã§è¿”ã™
    """
    full_text = soup.get_text()
    desc_meta = soup.select_one('meta[name="description"]')
    meta_text = desc_meta['content'] if desc_meta else ""
    search_target = f"{product_name} {meta_text} {full_text}"

    specs = []

    # 1. CPUæŠ½å‡º
    cpu_pattern = r'((?:AMD\s?)?Ryzenâ„¢?\s?\d\s\d{4}[A-Z]{1,2}|(?:Intel\s?)?Coreâ„¢?\s?i\d-\d+[A-Z]?|(?:Intel\s?)?Ultra\s?\d\s\d{3}[A-Z]?)'
    cpu_match = re.search(cpu_pattern, search_target, re.I)
    if cpu_match:
        specs.append(cpu_match.group(1).replace('â„¢', '').strip())
    else:
        specs.append("CPUæœªç¢ºèª")

    # 2. GPUæŠ½å‡º
    gpu_pattern = r'(RTX\s?\d{4}(?:\s?Ti)?|Radeon\s?\d{2,3}[A-Z]?)'
    gpu_match = re.search(gpu_pattern, search_target, re.I)
    if gpu_match:
        specs.append(gpu_match.group(1).strip())
    elif "G1" in product_name or "ã‚²ãƒ¼ãƒŸãƒ³ã‚°" in search_target:
        specs.append("å¤–éƒ¨GPUå¯¾å¿œå¯")

    # 3. ãƒ¡ãƒ¢ãƒª(RAM)æŠ½å‡º
    ram_pattern = r'(\d{1,3}GB\s?(?:DDR\d|LPDDR\d|çµ±åˆãƒ¡ãƒ¢ãƒª|RAM))'
    ram_match = re.search(ram_pattern, search_target, re.I)
    if ram_match:
        specs.append(ram_match.group(1).strip())

    # 4. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸(SSD)æŠ½å‡º
    ssd_pattern = r'(\d{1,3}(?:GB|TB)\s?(?:SSD|NVMe|PCIe|ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸))'
    ssd_match = re.search(ssd_pattern, search_target, re.I)
    if ssd_match:
        specs.append(ssd_match.group(1).strip())

    return " / ".join(specs)

def extract_correct_price(soup, product_data):
    """HTMLä¸Šã®æ—¥æœ¬å††è¡¨è¨˜ã‚’å„ªå…ˆã—ã¦æŠ½å‡º"""
    price_selectors = [
        '.price-item--sale', 
        '.price-item--regular', 
        'sale-price', 
        '.product-form__price',
        '.price__last .price-item'
    ]
    
    for selector in price_selectors:
        tag = soup.select_one(selector)
        if tag:
            digits = re.sub(r'[^\d]', '', tag.get_text())
            if digits and int(digits) > 1000:
                return int(digits)

    offers = product_data.get('offers', {})
    if isinstance(offers, list): offers = offers[0]
    raw_price = int(float(offers.get('price', 0)))
    return raw_price if raw_price > 1000 else 0

def extract_best_image(soup, product_data):
    """
    ç”»åƒURLã‚’æŠ½å‡ºã—ã€æ­£ã—ã„ Shopify CDN ãƒ‘ã‚¹ã‚’è£œæ­£ã—ã¦ç”Ÿæˆã™ã‚‹
    """
    img = product_data.get('image')
    img_url = img[0] if isinstance(img, list) and img else img
    
    if not img_url:
        meta_img = soup.select_one('meta[property="og:image"]')
        if meta_img:
            img_url = meta_img.get('content')

    if not img_url:
        selectors = ['.product__media img', '.product-gallery__image', '.product-main-image', '[data-zoom]']
        for s in selectors:
            target = soup.select_one(s)
            if target:
                img_url = target.get('src') or target.get('data-src') or target.get('srcset')
                if img_url: break

    if not img_url:
        return ""

    # URLè£œæ­£
    if img_url.startswith('//'):
        img_url = "https:" + img_url
    elif "files/" in img_url and "cdn/shop" not in img_url:
        path_part = img_url.split('files/')[-1]
        img_url = f"https://{BASE_DOMAIN}/cdn/shop/files/{path_part}"
    elif img_url.startswith('/') and not img_url.startswith('//'):
        img_url = f"https://{BASE_DOMAIN}{img_url}"
    
    return img_url.split('?')[0]

def scrape_minis_page(page, url, current_index, total_count):
    url_clean = url.split('?')[0].split('#')[0].rstrip('/')
    
    print(f"ğŸ” [{current_index + 1}/{total_count}] è§£æä¸­: {url_clean}")
    
    try:
        page.goto(url_clean, wait_until="domcontentloaded", timeout=60000)
        page.evaluate("window.scrollTo(0, 500)")
        page.wait_for_timeout(2000) 
        
        html_content = page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # JSON-LD (æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿) ã®èª­ã¿è¾¼ã¿
        product_data = {}
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and data.get('@type') == 'Product':
                    product_data = data
                    break
            except: continue

        # 1. è£½å“å
        name = product_data.get('name') or (soup.select_one('h1').get_text().strip() if soup.select_one('h1') else "ä¸æ˜ãªè£½å“")
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        blacklist = ["é…é€ä¿è­·", "ä¿é™º", "ã‚µãƒ¼ãƒ“ã‚¹", "å»¶é•·ä¿è¨¼", "ã‚¯ãƒ¼ãƒãƒ³", "é€æ–™"]
        if any(word in name for word in blacklist):
            print(f" â© ã‚¹ã‚­ãƒƒãƒ—: {name}")
            return False

        # 2. ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š (mini-pc ã¾ãŸã¯ motherboard ã«åˆ†é¡)
        if any(kw in name for kw in ["ãƒã‚¶ãƒ¼ãƒœãƒ¼ãƒ‰", "Motherboard", "BD790", "BD770"]):
            raw_genre = "motherboard"
            unified_genre = "motherboard"
        else:
            raw_genre = "mini-pc"
            unified_genre = "mini-pc"

        # 3. ä¾¡æ ¼ / ç”»åƒ / ã‚¹ãƒšãƒƒã‚¯
        price = extract_correct_price(soup, product_data)
        image_url = extract_best_image(soup, product_data)
        description = extract_detailed_specs(soup, name)
        
        # 4. ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆURLã®ç”Ÿæˆ (æ—¢å­˜ã®ã‚«ãƒ©ãƒ  'affiliate_url' ã«æ ¼ç´)
        final_affiliate_url = f"{url_clean}?aff={AFFILIATE_ID}"

        # 5. åœ¨åº«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        offers = product_data.get('offers', {})
        if isinstance(offers, list): offers = offers[0]
        is_instock = offers.get('availability') == 'http://schema.org/InStock'
        stock_status = 'åœ¨åº«ã‚ã‚Š' if price > 0 and is_instock else 'æœªç™ºå£²ãƒ»äºˆç´„å—ä»˜ä¸­'

        print(f" ğŸ“¦ è£½å“å : {name}")
        print(f" ğŸ’° ä¾¡  æ ¼ : Â¥{price:,}" if price > 0 else " ğŸ’° ä¾¡  æ ¼ : ä¾¡æ ¼æœªå®š")
        print(f" ğŸ·ï¸ ï½¼ï¾ï½¬ï¾ï¾™ : {unified_genre}")
        print("-" * 50)

        # 6. Djangoãƒ¢ãƒ‡ãƒ«ã¸ä¿å­˜
        unique_id = "minis-" + hashlib.md5(url_clean.encode()).hexdigest()[:12]
        PCProduct.objects.update_or_create(
            unique_id=unique_id,
            defaults={
                'site_prefix': 'MINIS',
                'maker': MAKER_NAME,
                'name': name,
                'price': price,
                'url': url_clean,
                'affiliate_url': final_affiliate_url,
                'image_url': image_url,
                'description': description,
                'is_active': True,
                'stock_status': stock_status,
                'raw_genre': raw_genre,
                'unified_genre': unified_genre,
            }
        )
        return True

    except Exception as e:
        print(f" âŒ è§£æå¤±æ•—: {e}")
        return False

# ==========================================
# ğŸš€ 3. ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
# ==========================================

def run_minis_crawler():
    list_url = "https://www.minisforum.jp/collections/all-product?page=1"

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        print(f"ğŸ“‚ MINISFORUM å…¨è£½å“åŒæœŸé–‹å§‹ (ã‚«ãƒ©ãƒ æ§‹æˆé©æ­£åŒ–ç‰ˆ)")
        try:
            page.goto(list_url, wait_until="domcontentloaded", timeout=60000)
            page.wait_for_timeout(3000) 
            
            # å…¨aã‚¿ã‚°ã‹ã‚‰å•†å“ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡º
            hrefs = page.evaluate('''() => {
                const links = Array.from(document.querySelectorAll('a'));
                return links.map(a => a.href);
            }''')
            
            product_urls = sorted(list(set([h.split('?')[0] for h in hrefs if "/products/" in h])))
            print(f"ğŸ“Š è§£æå¯¾è±¡URL: {len(product_urls)}ä»¶")

            for i, url in enumerate(product_urls):
                scrape_minis_page(page, url, i, len(product_urls))
                time.sleep(random.uniform(2.0, 4.0))

        except Exception as e:
            print(f" âš ï¸ ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {e}")
        
        browser.close()
        print(f"\nâœ¨ ã™ã¹ã¦ã®è£½å“ãƒ‡ãƒ¼ã‚¿ã®åŒæœŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    run_minis_crawler()