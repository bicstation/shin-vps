import os
import re
import ftplib
import gzip
import csv
import logging
import sys
from decimal import Decimal
from datetime import datetime
from typing import List, Dict, Any, Optional

from django.core.management.base import BaseCommand
from django.db import transaction
from django.utils import timezone
from django.conf import settings

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿéš›ã®ãƒ‘ã‚¹ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„
from api.models import PCProduct

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = 'Dell FTPã‹ã‚‰è£½å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚¹ãƒšãƒƒã‚¯ã‚’è§£æã—ã¦PCProductãƒ¢ãƒ‡ãƒ«ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚'

    # --- è¨­å®šå®šæ•° ---
    FTP_HOST = "aftp.linksynergy.com"
    # .envã‹ã‚‰å–å¾—ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Bicstation(bc_)ã®è¨­å®š
    FTP_USER = os.getenv("LINKSHARE_BC_USER", "rkp_3273700")
    FTP_PASS = os.getenv("LINKSHARE_BC_PASS", "5OqF1NfuruvJlmuJXKQDRuzh")
    
    DOWNLOAD_DIR = "/tmp/dell_ftp_import"
    DELL_MID = "37509" # ãƒ‡ãƒ«ã®ãƒãƒ¼ãƒãƒ£ãƒ³ãƒˆID
    SID = "3273700"    # Bicstationã®SID
    EXPECTED_COLUMNS_COUNT = 38

    def add_arguments(self, parser):
        parser.add_argument('--limit', type=int, default=None, help='å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°')

    def handle(self, *args, **options):
        self.stdout.write(self.style.SUCCESS(f"--- Dell FTP Import Start ({datetime.now()}) ---"))

        # 1. æº–å‚™
        if not os.path.exists(self.DOWNLOAD_DIR):
            os.makedirs(self.DOWNLOAD_DIR)

        # 2. FTPæ¥ç¶š
        ftp = self._connect_ftp()
        if not ftp:
            return

        try:
            # 3. ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®š (ãƒ•ãƒ«ãƒ•ã‚£ãƒ¼ãƒ‰)
            target_filename = f"{self.DELL_MID}_{self.SID}_mp.txt.gz"
            local_gz_path = os.path.join(self.DOWNLOAD_DIR, target_filename)
            local_txt_path = local_gz_path.replace('.gz', '.txt')

            # 4. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            self.stdout.write(f"ğŸ“¡ Downloading: {target_filename}")
            with open(local_gz_path, 'wb') as f:
                ftp.retrbinary(f'RETR {target_filename}', f.write)

            # 5. è§£å‡
            self.stdout.write("ğŸ”“ Decompressing...")
            with gzip.open(local_gz_path, 'rb') as f_in:
                with open(local_txt_path, 'wb') as f_out:
                    f_out.write(f_in.read())

            # 6. ãƒ‘ãƒ¼ã‚¹ã¨DBä¿å­˜
            self._parse_and_import(local_txt_path, options['limit'])

            self.stdout.write(self.style.SUCCESS("âœ… Import Completed successfully."))

        except Exception as e:
            self.stderr.write(self.style.ERROR(f"âŒ Critical Error: {str(e)}"))
            import traceback
            self.stderr.write(traceback.format_exc())

        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if os.path.exists(local_gz_path): os.remove(local_gz_path)
            if os.path.exists(local_txt_path): os.remove(local_txt_path)
            ftp.quit()

    def _connect_ftp(self) -> Optional[ftplib.FTP]:
        try:
            ftp = ftplib.FTP(self.FTP_HOST, timeout=60)
            ftp.login(self.FTP_USER, self.FTP_PASS)
            ftp.set_pasv(True)
            return ftp
        except Exception as e:
            self.stderr.write(self.style.ERROR(f"FTP Connection Failed: {e}"))
            return None

    def _parse_and_import(self, file_path: str, limit: Optional[int]):
        batch = []
        count = 0

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            # ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã¯ãƒ‘ã‚¤ãƒ—åŒºåˆ‡ã‚Š
            reader = csv.reader(f, delimiter='|')
            
            # HDRè¡Œã®ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
            try:
                first_row = next(reader)
                if first_row[0] != 'HDR':
                    f.seek(0) # HDRãŒãªã„å ´åˆã¯æœ€åˆã‹ã‚‰
            except StopIteration:
                return

            for row in reader:
                if not row or row[0] == 'TRL': continue # ãƒ•ãƒƒã‚¿ãƒ¼è¡Œã‚¹ã‚­ãƒƒãƒ—
                if len(row) < self.EXPECTED_COLUMNS_COUNT: continue

                # ãƒãƒƒãƒ”ãƒ³ã‚°
                sku = row[2].strip()          # C3: SKU
                product_name = row[1].strip()  # C2: Name
                aff_url = row[5].strip()      # C6: Affiliate URL
                prod_url = row[8].strip()     # C9: Product URL
                price_raw = row[12].strip()    # C13: Price
                raw_desc = row[3].strip()     # C4: Description
                category = row[17].strip()    # C18: Category Path

                # ã‚¹ãƒšãƒƒã‚¯è§£æ
                specs = self._extract_specs(product_name, raw_desc)
                
                # descriptionã‚«ãƒ©ãƒ ç”¨ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥åŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
                # ä¾‹: "Core i7 / RTX 4060 / 16GB RAM / 1TB SSD"
                desc_parts = []
                if specs['cpu']: desc_parts.append(specs['cpu'])
                if specs['gpu']: desc_parts.append(specs['gpu'])
                if specs['ram']: desc_parts.append(f"{specs['ram']}GB RAM")
                if specs['ssd']: 
                    cap = specs['ssd']
                    ssd_str = f"{cap/1024}TB" if cap >= 1024 else f"{cap}GB"
                    desc_parts.append(f"{ssd_str} SSD")
                
                # æœ€çµ‚çš„ãª description
                formatted_desc = " / ".join(desc_parts) if desc_parts else raw_desc[:500]

                # PCProductã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æº–å‚™
                product = PCProduct(
                    unique_id=f"dell_{sku}",
                    site_prefix="dell",
                    maker="Dell",
                    name=product_name,
                    price=self._clean_price(price_raw),
                    url=prod_url,
                    affiliate_url=aff_url,
                    description=formatted_desc,
                    raw_genre=category,
                    stock_status="åœ¨åº«ã‚ã‚Š",
                    is_active=True,
                    updated_at=timezone.now()
                )

                batch.append(product)
                count += 1

                if len(batch) >= 500:
                    self._bulk_upsert(batch)
                    self.stdout.write(f"  Processed {count} items...")
                    batch = []

                if limit and count >= limit:
                    break

            if batch:
                self._bulk_upsert(batch)

    def _extract_specs(self, name: str, desc: str) -> Dict[str, Any]:
        """å•†å“åã¨èª¬æ˜æ–‡ã‹ã‚‰ã‚¹ãƒšãƒƒã‚¯ã‚’æŠœãå‡ºã™"""
        text = f"{name} {desc}"
        
        # CPU: Core i/Ryzen/Core Ultra
        cpu_pattern = r'(Core\s?i[3579]|Ryzen\s?[3579]|Core\s?Ultra\s?\d|Celeron|Pentium)'
        # GPU: RTX/GTX/Radeon/Intel Graphics
        gpu_pattern = r'(RTX\s?\d{4}|GTX\s?\d{4}|Radeon\s?\d{3,4}[M|S]?|Intel\s?Iris\s?Xe|Intel\s?Graphics)'
        # RAM: 8GB, 16GB...
        ram_pattern = r'(\d+)GB\s?(?:RAM|ãƒ¡ãƒ¢ãƒª|DDR)'
        # SSD/Storage: 256GB, 1TB...
        ssd_pattern = r'(\d+)(GB|TB)\s?(?:SSD|NVMe|ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸|HDD)'

        cpu_match = re.search(cpu_pattern, text, re.I)
        gpu_match = re.search(gpu_pattern, text, re.I)
        ram_match = re.search(ram_pattern, text, re.I)
        ssd_match = re.search(ssd_pattern, text, re.I)

        ssd_val = 0
        if ssd_match:
            val = int(ssd_match.group(1))
            unit = ssd_match.group(2).upper()
            ssd_val = val * 1024 if unit == 'TB' else val

        return {
            'cpu': cpu_match.group(1) if cpu_match else None,
            'gpu': gpu_match.group(1) if gpu_match else None,
            'ram': int(ram_match.group(1)) if ram_match else 0,
            'ssd': ssd_val
        }

    def _clean_price(self, price_str: str) -> int:
        """ä¾¡æ ¼æ–‡å­—åˆ—ã‚’æ•´æ•°ã«å¤‰æ›"""
        try:
            # æ•°å­—ã¨ãƒ‰ãƒƒãƒˆä»¥å¤–ã‚’å‰Šé™¤
            nums = re.sub(r'[^\d.]', '', price_str)
            return int(float(nums))
        except (ValueError, TypeError):
            return 0

    def _bulk_upsert(self, batch: List[PCProduct]):
        """unique_idã‚’ã‚­ãƒ¼ã«æ›´æ–°ã¾ãŸã¯ä½œæˆ"""
        with transaction.atomic():
            for item in batch:
                PCProduct.objects.update_or_create(
                    unique_id=item.unique_id,
                    defaults={
                        'site_prefix': item.site_prefix,
                        'maker': item.maker,
                        'name': item.name,
                        'price': item.price,
                        'url': item.url,
                        'affiliate_url': item.affiliate_url,
                        'description': item.description,
                        'raw_genre': item.raw_genre,
                        'stock_status': item.stock_status,
                        'is_active': item.is_active,
                        'updated_at': item.updated_at,
                    }
                )