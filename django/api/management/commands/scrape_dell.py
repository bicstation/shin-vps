import os
import re
import time
from django.core.management.base import BaseCommand
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from api.models.pc_products import PCProduct

os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"

class Command(BaseCommand):
    help = 'Dellå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã‚»ãƒ¼ãƒ«ãƒ»è£½å“æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦DBã«ä¿å­˜ã—ã¾ã™'

    def handle(self, *args, **options):
        self.stdout.write(self.style.SUCCESS('--- Dell Scraper é–‹å§‹ ---'))
        self.run_crawler()
        self.stdout.write(self.style.SUCCESS('--- Dell Scraper å®Œäº† ---'))

    def get_genre_from_url(self, url):
        """URLã‚„æ–‡å­—åˆ—ã‹ã‚‰è£½å“ã‚¸ãƒ£ãƒ³ãƒ«ã‚’å³å¯†ã«åˆ¤å®š"""
        url_lower = url.lower()
        if "monitor" in url_lower:
            return "monitor"
        if "accessory" in url_lower or "mouse" in url_lower or "keyboard" in url_lower:
            return "accessory"
        if "alienware" in url_lower or "g-series" in url_lower or "gaming" in url_lower:
            return "gaming_pc"
        if "inspiron" in url_lower or "xps" in url_lower:
            return "laptop"
        if "vostro" in url_lower or "latitude" in url_lower:
            return "business_laptop"
        if "optiplex" in url_lower or "precision" in url_lower:
            return "desktop"
        return "pc"

    def extract_specs(self, page):
        """ãƒ‡ãƒ«ã®å‹•çš„ãªã‚¹ãƒšãƒƒã‚¯è¡¨ã‚’Playwrightã§æŠ½å‡º"""
        specs_list = []
        try:
            # ã‚¹ãƒšãƒƒã‚¯ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿã™ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å€™è£œ
            selectors = [
                'div[data-testid="shared-spec-list"]',
                '.technical-specifications',
                '#specs-section',
                '.spec-column'
            ]
            
            for sel in selectors:
                if page.query_selector(sel):
                    # ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ä¸€æ‹¬å–å¾—
                    specs = page.eval_on_selector_all(f'{sel} .spec-item, {sel} dt, {sel} dd, {sel} li', 
                        'elements => elements.map(e => e.innerText)')
                    if specs:
                        # é‡è¤‡å‰Šé™¤ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                        unique_specs = list(dict.fromkeys([s.strip() for s in specs if s.strip()]))
                        specs_list = unique_specs
                        break
        except:
            pass
        
        return " / ".join(specs_list) if specs_list else "è©³ç´°ã‚¹ãƒšãƒƒã‚¯ã¯å…¬å¼ã‚µã‚¤ãƒˆã‚’ã”ç¢ºèªãã ã•ã„"

    def extract_price(self, soup):
        """å‰²å¼•å¾Œä¾¡æ ¼ã‚’å„ªå…ˆçš„ã«æŠ½å‡º"""
        # 1. Dellç‰¹æœ‰ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å±æ€§ã‚’ç‹™ã†
        price_element = soup.select_one('[data-testid="shared-ps-dell-price"], .ps-dell-price, .dell-price, .ps-title-price')
        if price_element:
            text = price_element.get_text()
            digits = re.sub(r'[^\d]', '', text)
            if digits:
                return int(digits)
        
        # 2. é€šè²¨è¨˜å·ã‹ã‚‰ã®æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        for element in soup.find_all(['span', 'div', 'p']):
            text = element.get_text()
            if 'ï¿¥' in text or 'Â¥' in text:
                # ã€Œç¨è¾¼ã€ç­‰ã®æ–‡å­—ã‚’é™¤å»ã—ã¦æ•°å­—ã ã‘æŠ½å‡º
                digits = re.sub(r'[^\d]', '', text)
                if digits and 5000 < int(digits) < 4000000:
                    return int(digits)
        return 0

    def scrape_detail_page(self, page, url):
        self.stdout.write(f"ğŸ” Dellè©³ç´°ãƒšãƒ¼ã‚¸å·¡å›ä¸­... {url}")
        try:
            # URLã‹ã‚‰ä¸€æ„ã®IDã‚’ç”Ÿæˆ
            unique_id = "dell-" + url.split('/')[-1].split('?')[0]
            
            # ãƒšãƒ¼ã‚¸é·ç§»ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒè½ã¡ç€ãã¾ã§å¾…æ©Ÿï¼‰
            page.goto(url, wait_until="networkidle", timeout=60000)
            
            # é‚ªé­”ãªãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚„ãƒãƒŠãƒ¼ãŒã‚ã‚Œã°é–‰ã˜ã‚‹
            try:
                close_btn = page.query_selector('#net-banner-close, .close-modal, .soft-cookie-close')
                if close_btn:
                    close_btn.click()
            except: pass

            # å‹•çš„è¦ç´ ã®ãƒ­ãƒ¼ãƒ‰å¾…ã¡
            page.evaluate("window.scrollBy(0, 800)")
            page.wait_for_timeout(3000)
            
            soup = BeautifulSoup(page.content(), 'html.parser')
            
            genre = self.get_genre_from_url(url)
            price = self.extract_price(soup)
            
            # ç”»åƒURLã®å–å¾—
            image_url = ""
            img_sel = 'img[data-testid="shared-ps-image"], .product-image img, .main-image img'
            img_handle = page.query_selector(img_sel)
            if img_handle:
                image_url = img_handle.get_attribute("src")
                if image_url and image_url.startswith('//'):
                    image_url = "https:" + image_url

            specs_text = self.extract_specs(page)
            
            save_data = {
                'unique_id': unique_id,
                'site_prefix': 'DELL',
                'maker': 'Dell',
                'raw_genre': genre,
                'unified_genre': genre,
                'name': page.title().split('|')[0].replace('Dell æ—¥æœ¬', '').strip(),
                'price': price,
                'url': url,
                'image_url': image_url,
                'description': specs_text,
                'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'è©³ç´°ç¢ºèª',
                'is_active': True,
            }

            PCProduct.objects.update_or_create(unique_id=unique_id, defaults=save_data)
            self.stdout.write(self.style.SUCCESS(f"âœ… ä¿å­˜å®Œäº†: {save_data['name']} (ä¾¡æ ¼: {price}å††)"))
            return True
        except Exception as e:
            self.stdout.write(self.style.ERROR(f" âŒ Dellè©³ç´°ã‚¨ãƒ©ãƒ¼: {url} -> {e}"))
            return False

    def run_crawler(self):
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆã‚»ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‚’å«ã‚€æˆ¦ç•¥çš„ãªãƒªã‚¹ãƒˆï¼‰
        target_categories = [
            "https://www.dell.com/ja-jp/shop/deals/top-pc-deals",        # PCã‚»ãƒ¼ãƒ«
            "https://www.dell.com/ja-jp/shop/scc/sc/laptops",           # ãƒãƒ¼ãƒˆPC
            # "https://www.dell.com/ja-jp/shop/scc/sc/desktops",          # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—
            # "https://www.dell.com/ja-jp/shop/deals/gaming-deals",       # ã‚²ãƒ¼ãƒŸãƒ³ã‚°
            # "https://www.dell.com/ja-jp/shop/deals/business-pc-deals",  # ãƒ“ã‚¸ãƒã‚¹ãƒ‘ã‚½ã‚³ãƒ³
            # "https://www.dell.com/ja-jp/shop/deals/monitors-deals",     # ãƒ¢ãƒ‹ã‚¿ãƒ¼
            # "https://www.dell.com/ja-jp/shop/deals/pc-accessories-deals",# å‘¨è¾ºæ©Ÿå™¨
            # "https://www.dell.com/ja-jp/shop/deals/clearance-pc-deals", # ã‚¯ãƒªã‚¢ãƒ©ãƒ³ã‚¹ã‚»ãƒ¼ãƒ«
        ]

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={'width': 1280, 'height': 800}
            )
            page = context.new_page()
            
            all_product_urls = set()
            for cat_url in target_categories:
                self.stdout.write(f"ğŸ“‚ Dellã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚¹ã‚­ãƒ£ãƒ³: {cat_url}")
                try:
                    page.goto(cat_url, wait_until="networkidle", timeout=60000)
                    
                    # ãƒ‡ãƒ«ã®ã‚·ãƒ§ãƒƒãƒ—URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ãƒªãƒ³ã‚¯ã‚’ã™ã¹ã¦æŠ½å‡º
                    hrefs = page.eval_on_selector_all('a[href*="/ja-jp/shop/"]', 
                        'elements => elements.map(e => e.href)')
                    
                    # è©³ç´°ãƒšãƒ¼ã‚¸ï¼ˆcp=æ§‹æˆã€pd=è£½å“è©³ç´°ï¼‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    for url in hrefs:
                        if "/cp/" in url or "/pd/" in url:
                            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦é‡è¤‡é˜²æ­¢
                            clean_url = url.split('?')[0].rstrip('/')
                            all_product_urls.add(clean_url)
                            
                except Exception as e:
                    self.stdout.write(self.style.WARNING(f" âŒ ã‚«ãƒ†ã‚´ãƒªå–å¾—å¤±æ•—: {e}"))
            
            self.stdout.write(f"ğŸš€ åˆè¨ˆ {len(all_product_urls)}ä»¶ã®å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            
            # ã‚µãƒ¼ãƒãƒ¼è² è·ã¨å®Ÿè¡Œæ™‚é–“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒãˆã€ã¾ãšã¯ãƒªã‚¹ãƒˆåŒ–ã•ã‚ŒãŸé †ã«å‡¦ç†
            # ãƒ†ã‚¹ãƒˆæ™‚ã¯ list(all_product_urls)[:5] ãªã©ã§åˆ¶é™ã—ã¦ãã ã•ã„
            # for url in list(all_product_urls): 
            for url in list(all_product_urls)[:5]:
                self.scrape_detail_page(page, url)
                time.sleep(2) # ãƒ‡ãƒ«ã®ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
            
            browser.close()