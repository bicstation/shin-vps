import time
import csv
import os
import sys
import re
from playwright.sync_api import sync_playwright

# ãƒ‘ã‚¹è§£æ±º
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../"))
if project_root not in sys.path:
    sys.path.append(project_root)

def scrape_arkhive():
    output_dir = os.path.join(project_root, "data", "raw")
    os.makedirs(output_dir, exist_ok=True)
    output_csv = os.path.join(output_dir, "ark_products.csv")
    
    # æ•™ãˆã¦ã„ãŸã ã„ãŸ arkhive ã®ä¸€è¦§ãƒšãƒ¼ã‚¸
    target_url = "https://www.ark-pc.co.jp/bto/special/arkhive/"
    total_count = 0

    print(f"ğŸ”— ã‚¢ãƒ¼ã‚¯ï¼ˆarkhiveï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        # ç”»é¢ã‚µã‚¤ã‚ºã‚’å°‘ã—å¤§ãã‚ã«è¨­å®š
        context = browser.new_context(viewport={'width': 1280, 'height': 1080})
        page = context.new_page()

        try:
            # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿
            page.goto(target_url, wait_until="domcontentloaded", timeout=60000)
            
            # ã‚¢ãƒ¼ã‚¯ã®BTOãƒšãƒ¼ã‚¸ã¯å•†å“ãŒ '.bto-item' ãªã©ã®ã‚¯ãƒ©ã‚¹ã§ä¸¦ã‚“ã§ã„ã¾ã™
            # ãƒšãƒ¼ã‚¸å†…ã®ã€Œå•†å“ãƒ–ãƒ­ãƒƒã‚¯ã€ã‚’ç‰¹å®š
            items = page.query_selector_all(".bto-item, .item")

            if not items:
                # ã‚¯ãƒ©ã‚¹åãŒé•ã£ãŸå ´åˆã®äºˆå‚™ç­–ï¼šè©³ç´°ãƒœã‚¿ãƒ³ãŒã‚ã‚‹æ ã‚’æ¢ã™
                items = page.query_selector_all(".product-card, .list-product")

            with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['category', 'name', 'price', 'url', 'image_url', 'description'])

                for item in items:
                    try:
                        # å•†å“å
                        name_el = item.query_selector(".title, .product-title, h3")
                        if not name_el: continue
                        name = name_el.inner_text().strip()

                        # ä¾¡æ ¼
                        price_el = item.query_selector(".price, .amount")
                        if not price_el: continue
                        price_text = price_el.inner_text()
                        # æ•°å­—ã ã‘ã‚’æŠ½å‡º
                        price = int(re.sub(r'\D', '', price_text))

                        # URL (aã‚¿ã‚°ã®hrefã‚’å–å¾—)
                        link_el = item.query_selector("a")
                        href = link_el.get_attribute("href") if link_el else ""
                        url = "https://www.ark-pc.co.jp" + href if href.startswith('/') else href

                        # èª¬æ˜/ã‚¹ãƒšãƒƒã‚¯
                        spec_el = item.query_selector(".spec, .description")
                        description = spec_el.inner_text().replace('\n', ' ').strip() if spec_el else ""

                        # CSVã«ä¿å­˜
                        writer.writerow(['arkhive', name, price, url, "", description])
                        total_count += 1
                        print(f"âœ… å–å¾—æˆåŠŸ: {name[:25]}... ({price}å††)")

                    except Exception as e:
                        continue

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        finally:
            browser.close()
    
    print(f"\nâœ¨ å®Œäº†ï¼ã‚¢ãƒ¼ã‚¯ã‹ã‚‰ {total_count} ä»¶å–å¾—ã—ã¾ã—ãŸã€‚")
    print(f"ğŸ“‚ ä¿å­˜å…ˆ: {output_csv}")

if __name__ == "__main__":
    scrape_arkhive()