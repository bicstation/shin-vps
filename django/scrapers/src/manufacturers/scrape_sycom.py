# ã‚µã‚¤ã‚³ãƒ ã¯HTMLã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒšã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«è²¼ã‚Šä»˜ã‘ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚²ãƒƒãƒˆã—ã¾ã™


import csv
import os
import subprocess
from bs4 import BeautifulSoup

def run_docker_import_sycom(csv_path):
    container_name = "api_django_v2"
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    print(f"ğŸš€ Dockerã‚³ãƒ³ãƒ†ãƒŠã¸ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ãƒ»DBæ›´æ–°ä¸­...")
    try:
        subprocess.run(["docker", "cp", csv_path, f"{container_name}:/usr/src/app/scrapers/sycom_products.csv"], check=True)
        import_cmd = [
            "docker", "compose", "-f", "docker-compose.stg.yml", 
            "exec", "django-v2", 
            "python", "manage.py", "import_sycom"
        ]
        subprocess.run(import_cmd, check=True, cwd=project_root)
        print(f"âœ… Djangoã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼")
    except Exception as e:
        print(f"âŒ Dockeré€£æºã‚¨ãƒ©ãƒ¼: {e}")

def scrape_sycom_from_html_source():
    input_file = os.path.join(os.path.dirname(__file__), "sycom_data.txt")
    output_csv = os.path.join(os.path.dirname(__file__), "sycom_products.csv")
    
    if not os.path.exists(input_file):
        print(f"âŒ {input_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"ğŸ“– {input_file} ã‚’è§£æä¸­...")
    with open(input_file, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")

    all_data = []
    items = soup.find_all("div", class_="item")
    
    for item in items:
        # 1. å•†å“å
        name_tag = item.find("p", class_="name01")
        # 2. ä¾¡æ ¼
        price_tag = item.find("span", id=lambda x: x and x.startswith('model_'))
        # 3. URL
        link_tag = item.find("a", href=True)
        # 4. ç”»åƒURL (imgã‚¿ã‚°ã®srcã‚’å–å¾—)
        img_tag = item.find("img")
        # 5. ã‚¹ãƒšãƒƒã‚¯è©³ç´° (p.spec ã‚„ div.spec_box ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†)
        # ã‚µã‚¤ã‚³ãƒ ã®æ§‹é€ ã«åˆã‚ã›ã€è¤‡æ•°ã®ã‚¹ãƒšãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¦ã€Œ / ã€ã§çµåˆ
        spec_tags = item.find_all("p", class_="spec") # ã‚‚ã— spec ã‚¯ãƒ©ã‚¹ã«è©³ç´°ãŒã‚ã‚‹å ´åˆ
        if not spec_tags:
            # specã‚¯ãƒ©ã‚¹ãŒãªã„å ´åˆã¯ã€itemå†…ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æ¢ã‚‹
            spec_text = item.get_text(" / ", strip=True) 
        else:
            spec_text = " / ".join([s.get_text(strip=True) for s in spec_tags])

        if name_tag and price_tag:
            name = name_tag.get_text(strip=True)
            price_text = price_tag.get_text(strip=True).replace(",", "")
            price = int(price_text)
            
            url = link_tag["href"]
            if not url.startswith("http"):
                url = "https://www.sycom.co.jp" + url

            image_url = ""
            if img_tag and img_tag.get("src"):
                image_url = img_tag["src"]
                if not image_url.startswith("http"):
                    image_url = "https://www.sycom.co.jp/" + image_url.lstrip("/")

            # descriptionã«ã‚¹ãƒšãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹
            description = spec_text

            all_data.append(["Desktop", f"[Sycom] {name}", price, url, image_url, description])
            print(f"   âœ… æŠ½å‡ºæˆåŠŸ: {name} | {price}å††")

    if all_data:
        # åå‰ã‚’ã‚­ãƒ¼ã«ã—ã¦é‡è¤‡å‰Šé™¤
        unique_data = {d[1]: d for d in all_data}.values()
        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['category', 'name', 'price', 'url', 'image_url', 'description'])
            writer.writerows(unique_data)
        
        print(f"âœ¨ åˆè¨ˆ {len(unique_data)} ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        run_docker_import_sycom(output_csv)
    else:
        print("âŒ å•†å“æƒ…å ±ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    scrape_sycom_from_html_source()