import os
import django
import re
import time
import hashlib
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

def extract_price(text):
    """HPã®ä¾¡æ ¼ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    if not text: return 0
    nums = re.sub(r'[^\d]', '', text)
    return int(nums) if nums else 0

def scrape_individual_page(page, product_url, genre):
    """å€‹åˆ¥ãƒšãƒ¼ã‚¸ï¼ˆPDPï¼‰ã‚’è©³ç´°ã«è§£æã—ã¦DBã«ä¿å­˜"""
    print(f"  ğŸ“– å€‹åˆ¥ãƒšãƒ¼ã‚¸è§£æä¸­: {product_url}")
    try:
        page.goto(product_url, wait_until="networkidle", timeout=60000)
        soup = BeautifulSoup(page.content(), 'html.parser')

        # 1. è£½å“å (titleã‚¿ã‚°ã‹ã‚‰å–å¾—ã™ã‚‹ã®ãŒHPã®PDPã§ã¯æœ€ã‚‚ç¢ºå®Ÿ)
        name = soup.title.string.split('|')[0].strip() if soup.title else "HPè£½å“"
        
        # 2. ä¾¡æ ¼
        price_tag = soup.select_one(".price-amount, #price-amount, .product-price")
        price = extract_price(price_tag.get_text()) if price_tag else 0

        # 3. ç”»åƒ
        img_tag = soup.select_one(".product-image img, #pdp-main-image, .hero-image img")
        image_url = ""
        if img_tag:
            image_url = img_tag.get('src') or img_tag.get('data-src') or ""
            if image_url.startswith('//'): image_url = "https:" + image_url
            elif image_url.startswith('/'): image_url = "https://jp.ext.hp.com" + image_url

        # 4. ã‚¹ãƒšãƒƒã‚¯è©³ç´° (ddã‚¿ã‚°ã‚„ã‚¹ãƒšãƒƒã‚¯ãƒªã‚¹ãƒˆã‹ã‚‰å–å¾—)
        spec_list = []
        for spec_item in soup.select(".model_spec_text, .d_model_spec_modal dd, .spec-item"):
            txt = spec_item.get_text(strip=True)
            if txt: spec_list.append(txt)
        specs = " / ".join(spec_list[:10]) # é•·ã™ããªã„ã‚ˆã†ã«åˆ¶é™

        # 5. Unique ID
        raw_id_part = product_url.split('/')[-1] or product_url.split('/')[-2]
        safe_id = re.sub(r'[^a-zA-Z0-9-]', '', raw_id_part)
        if len(safe_id) < 3:
            safe_id = hashlib.md5(product_url.encode()).hexdigest()[:10]
        unique_id = f"hp-{safe_id}"

        # DBä¿å­˜/æ›´æ–°
        PCProduct.objects.update_or_create(
            unique_id=unique_id,
            defaults={
                'site_prefix': 'HP',
                'maker': 'HP',
                'raw_genre': genre,
                'unified_genre': genre,
                'name': name,
                'price': price,
                'url': product_url,
                'image_url': image_url,
                'description': specs if specs else f"HPå…¬å¼ {genre} - {name}",
                'is_active': True,
                'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'åœ¨åº«ç¢ºèªä¸­',
            }
        )
        print(f"    âœ… ä¿å­˜å®Œäº†: {name[:30]}")

    except Exception as e:
        print(f"    âŒ å€‹åˆ¥ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼: {e}")

def scrape_hp_search_results(page, start_url, genre):
    """æ¤œç´¢çµæœä¸€è¦§ã‹ã‚‰å…¨URLã‚’æŠ½å‡ºã—ã€å€‹åˆ¥ãƒšãƒ¼ã‚¸ã¸èª˜å°ã™ã‚‹"""
    print(f"ğŸ” HPæ¤œç´¢ä¸€è¦§è§£æé–‹å§‹: {start_url}")
    page.goto(start_url, wait_until="networkidle", timeout=90000)
    
    all_product_urls = set()
    page_num = 1
    
    # --- STEP 1: å…¨è£½å“ã®URLã‚’åé›† ---
    while True:
        print(f"ğŸ“‘ æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ {page_num} ã‹ã‚‰URLã‚’åé›†ã—ã¦ã„ã¾ã™...")
        try:
            page.wait_for_selector(".hawk-results-item, .hawk-item", timeout=20000)
        except:
            break

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…¨è¦ç´ ã‚’ãƒ­ãƒ¼ãƒ‰
        for _ in range(3):
            page.mouse.wheel(0, 1000)
            page.wait_for_timeout(500)

        soup = BeautifulSoup(page.content(), 'html.parser')
        items = soup.select(".hawk-results-item, .hawk-item")
        
        for item in items:
            link_tag = item.select_one("a")
            if link_tag and link_tag.has_attr('href'):
                href = link_tag['href']
                full_url = href if href.startswith('http') else "https://jp.ext.hp.com" + href
                all_product_urls.add(full_url)

        # æ¬¡ã¸ãƒœã‚¿ãƒ³ã®å‡¦ç†
        next_button = page.query_selector(".hawk-pagination-next, .hawk-page-next")
        if next_button and next_button.is_visible() and next_button.is_enabled():
            next_button.click()
            page.wait_for_timeout(3000)
            page_num += 1
            if page_num > 30: break # å®‰å…¨è£…ç½®
        else:
            break

    print(f"ğŸ“¦ åˆè¨ˆ {len(all_product_urls)} ä»¶ã®URLã‚’åé›†ã—ã¾ã—ãŸã€‚è©³ç´°è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    # --- STEP 2: å„URLã‚’å·¡å›ã—ã¦è©³ç´°æƒ…å ±ã‚’å–å¾— ---
    for i, url in enumerate(all_product_urls):
        print(f"[{i+1}/{len(all_product_urls)}]")
        scrape_individual_page(page, url, genre)
        time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

def run_crawler():
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURL
    start_url = "https://jp.ext.hp.com/search/?orderBy=score&type=Product"
    genre = "laptop"

    with sync_playwright() as p:
        # headless=Falseã«ã™ã‚‹ã¨ã€è‡ªå®…PCã§å‹•ä½œãŒç›®è¦–ç¢ºèªã§ãã¾ã™
        browser = p.chromium.launch(headless=True) 
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()

        try:
            scrape_hp_search_results(page, start_url, genre)
        finally:
            browser.close()
            print("âœ¨ å…¨å·¥ç¨‹ãŒå®Œäº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    run_crawler()