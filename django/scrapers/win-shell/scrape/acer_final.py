import re
import json
import time
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

def run_acer():
    results = []
    targets = [
        {"url": "https://store.acer.com/ja-jp/notebooks", "genre": "laptop"},
        {"url": "https://store.acer.com/ja-jp/monitors", "genre": "monitor"}
    ]

    with sync_playwright() as p:
        # HTTP2ã‚¨ãƒ©ãƒ¼å›é¿ & å®‰å®šæ€§ã®ãŸã‚ã®å¼•æ•°
        browser = p.chromium.launch(headless=False, args=["--disable-http2"])
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        # ã‚µã‚¤ãƒˆå…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’å°‘ã—é•·ã‚ã«
        page = context.new_page()
        page.set_default_timeout(90000) 

        for target in targets:
            current_page = 1
            while True:
                url = f"{target['url']}?p={current_page}"
                print(f"ğŸ“‚ å·¡å›ä¸­: {url}")
                
                success = False
                # ğŸ’¡ å¤±æ•—ã—ã¦ã‚‚æœ€å¤§3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ãƒ«ãƒ¼ãƒ—
                for attempt in range(3):
                    try:
                        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’90ç§’ã«è¨­å®šï¼‰
                        page.goto(url, wait_until="domcontentloaded", timeout=90000)
                        success = True
                        break 
                    except Exception as e:
                        print(f"   âš ï¸ èª­ã¿è¾¼ã¿å¤±æ•— (è©¦è¡Œ {attempt + 1}/3): {e}")
                        time.sleep(5) # 5ç§’å¾…ã£ã¦ã‹ã‚‰ãƒªãƒˆãƒ©ã‚¤
                
                if not success:
                    print(f"   âŒ 3å›ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã—ãŸãŒå¤±æ•—ã—ã¾ã—ãŸã€‚æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸é€²ã¿ã¾ã™ã€‚")
                    # æ¬¡ã®ã‚«ãƒ†ã‚´ãƒªã¸è¡Œãã®ã§ã¯ãªãã€ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã¦æ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¸
                    break

                try:
                    # ğŸ’¡ LazyLoadå¯¾ç­–ï¼šç”»é¢ã‚’å°‘ã—ãšã¤ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ç”»åƒã‚’èª­ã¿è¾¼ã¾ã›ã‚‹
                    print("   âŒ› ç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­...")
                    for _ in range(10):  # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã‚’å°‘ã—å¢—ã‚„ã—ã¦ç¢ºå®Ÿã«
                        page.mouse.wheel(0, 800)
                        time.sleep(0.6)
                    
                    # ãƒšãƒ¼ã‚¸ã®ä¸€ç•ªä¸Šã«æˆ»ã‚‹ï¼ˆè¦ç´ å–å¾—ã®å®‰å®šåŒ–ï¼‰
                    page.evaluate("window.scrollTo(0, 0)")
                    time.sleep(1)

                    # BeautifulSoupã§è§£æ
                    soup = BeautifulSoup(page.content(), 'html.parser')
                    products = soup.select(".item.product.product-item")
                    
                    if not products:
                        print("   âœ… ã“ã®ã‚«ãƒ†ã‚´ãƒªã®å…¨ãƒšãƒ¼ã‚¸ã‚’å®Œäº†ã—ã¾ã—ãŸ")
                        break

                    for product in products:
                        link_tag = product.select_one("a.product-item-link")
                        if not link_tag: continue
                        
                        name = link_tag.get_text(strip=True)
                        desc_tag = product.select_one(".description")
                        description = desc_tag.get_text(" / ", strip=True) if desc_tag else ""
                        
                        # ç”»åƒURLã®å–å¾—
                        img_tag = product.select_one("img.product-image-photo")
                        img_url = ""
                        if img_tag:
                            img_url = img_tag.get('src')
                            # ã‚‚ã—ã¾ã pixel.jpgãªã‚‰data-originalå±æ€§ãªã©ã‚’ãƒã‚§ãƒƒã‚¯
                            if img_url and ("pixel.jpg" in img_url or "data:image" in img_url):
                                img_url = img_tag.get('data-original') or img_tag.get('data-src') or img_url

                        price_tag = product.select_one('.price')
                        price_val = int(re.sub(r'[^\d]', '', price_tag.get_text())) if price_tag else 0
                        
                        results.append({
                            "name": name,
                            "description": description,
                            "url": link_tag['href'].split('?')[0],
                            "image_url": img_url,
                            "price": price_val,
                            "genre": target['genre'],
                            "maker": "Acer"
                        })
                    
                    print(f"   âœ… {current_page}ãƒšãƒ¼ã‚¸ç›®å–å¾—å®Œäº† ({len(products)}ä»¶ / ç´¯è¨ˆ {len(results)}ä»¶)")
                    
                    # ã€Œæ¬¡ã¸ã€ãƒœã‚¿ãƒ³ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if not soup.select_one(".pages a.next"):
                        break
                    current_page += 1

                except Exception as e:
                    print(f"   âŒ ãƒ‡ãƒ¼ã‚¿è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                    break

        # çµæœã‚’JSONä¿å­˜
        with open("acer_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        
        print(f"\nâœ¨ ã™ã¹ã¦å®Œäº†ï¼åˆè¨ˆ {len(results)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
        browser.close()

if __name__ == "__main__":
    run_acer()