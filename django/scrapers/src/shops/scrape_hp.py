import os
import django
import re
import time
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'tiper_api.settings')
django.setup()

from api.models import PCProduct

def extract_price(text):
    """HPã®ä¾¡æ ¼ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡ºï¼ˆä¾‹: ï¿¥124,800ï¼ˆç¨è¾¼ï¼‰ï½ -> 124800ï¼‰"""
    if not text: return 0
    nums = re.sub(r'[^\d]', '', text)
    return int(nums) if nums else 0

def scrape_hp_category(page, url, genre):
    """ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ï¼ˆãƒãƒ¼ãƒˆPCä¸€è¦§ãªã©ï¼‰ã‚’è§£æã—ã¦ä¿å­˜"""
    print(f"ğŸ” HPã‚«ãƒ†ã‚´ãƒªè§£æä¸­... {url}")
    try:
        page.goto(url, wait_until="networkidle", timeout=60000)
        
        # HPã®è£½å“ã‚«ãƒ¼ãƒ‰ï¼ˆ.product-itemï¼‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…ã¤
        page.wait_for_selector(".product-item", timeout=10000)
        
        # é…å»¶èª­ã¿è¾¼ã¿ç”»åƒï¼ˆLazy Loadï¼‰ã‚’å®Ÿä½“åŒ–ã•ã›ã‚‹ãŸã‚ã«ä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        for _ in range(3):
            page.evaluate("window.scrollBy(0, 1000)")
            page.wait_for_timeout(500)

        soup = BeautifulSoup(page.content(), 'html.parser')
        items = soup.select(".product-item")
        
        print(f"ğŸ“¦ ã“ã®ãƒšãƒ¼ã‚¸ã§ {len(items)} ä»¶ã®è£½å“ã‚’è¦‹ã¤ã‘ã¾ã—ãŸ")

        for item in items:
            try:
                # 1. è£½å“å
                name_tag = item.select_one(".name, .product-name")
                if not name_tag: continue
                name = name_tag.get_text(strip=True)

                # 2. å›ºæœ‰ID (URLã‚„å‹ç•ªã‹ã‚‰æŠ½å‡º)
                link_tag = item.select_one("a[href*='/directplus/']")
                if not link_tag: continue
                product_url = "https://jp.ext.hp.com" + link_tag['href']
                unique_id = product_url.split('/')[-2] if product_url.endswith('/') else product_url.split('/')[-1]

                # 3. ä¾¡æ ¼
                price_tag = item.select_one(".price-amount, .price")
                price = extract_price(price_tag.get_text()) if price_tag else 0

                # 4. ç”»åƒ (HPã¯ lazy load ã®ãŸã‚ data-original ã‚„ src ã‚’ä½¿ã„åˆ†ã‘)
                img_tag = item.select_one("img")
                image_url = ""
                if img_tag:
                    image_url = img_tag.get('data-original') or img_tag.get('src') or ""
                    if image_url.startswith('//'): image_url = "https:" + image_url
                    elif image_url.startswith('/'): image_url = "https://jp.ext.hp.com" + image_url

                # 5. ã‚¹ãƒšãƒƒã‚¯ (HPã®ä¸€è¦§ã«ã¯ã‚¹ãƒšãƒƒã‚¯ãŒç®‡æ¡æ›¸ãã•ã‚Œã¦ã„ã‚‹)
                spec_tags = item.select(".spec-list li, .summary-spec li")
                specs = " / ".join([s.get_text(strip=True) for s in spec_tags])

                # ä¿å­˜
                save_data = {
                    'unique_id': f"HP_{unique_id}",
                    'site_prefix': 'HP',
                    'maker': 'HP',
                    'raw_genre': genre,
                    'unified_genre': genre,
                    'name': name,
                    'price': price,
                    'url': product_url,
                    'image_url': image_url,
                    'description': specs,
                    'is_active': True,
                    'stock_status': 'åœ¨åº«ã‚ã‚Š' if price > 0 else 'ç¢ºèªä¸­',
                }

                PCProduct.objects.update_or_create(
                    unique_id=save_data['unique_id'],
                    defaults=save_data
                )
                print(f"  âœ… ä¿å­˜: {name[:30]}... ({price}å††)")

            except Exception as e:
                print(f"  âš ï¸ å€‹åˆ¥è£½å“ã‚¨ãƒ©ãƒ¼: {e}")
                continue

    except Exception as e:
        print(f"  âŒ ã‚«ãƒ†ã‚´ãƒªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

def run_crawler():
    # HPã®ä¸»è¦ã‚«ãƒ†ã‚´ãƒªURL
    categories = [
        ("https://jp.ext.hp.com/notebooks/personal/omnibook_ultra/", "laptop"),      # å€‹äººå‘ã‘ãƒãƒ¼ãƒˆ
        ("https://jp.ext.hp.com/notebooks/personal/omnibook_x/", "laptop"),      # å€‹äººå‘ã‘ãƒãƒ¼ãƒˆ
        ("https://jp.ext.hp.com/desktops/personal/", "desktop"),      # å€‹äººå‘ã‘ãƒ‡ã‚¹ã‚¯
        ("https://www.hp.com/jp-ja/gaming-pc.html", "desktop"),       # ã‚²ãƒ¼ãƒŸãƒ³ã‚°(OMEN)
        ("https://jp.ext.hp.com/notebooks/business/", "laptop"),     # æ³•äººå‘ã‘ãƒãƒ¼ãƒˆ
        ("https://jp.ext.hp.com/desktops/business/", "desktop"),     # æ³•äººå‘ã‘ãƒ‡ã‚¹ã‚¯
        ("https://jp.ext.hp.com/workstations/", "workstation") # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        page = context.new_page()

        for url, genre in categories:
            scrape_hp_category(page, url, genre)
            time.sleep(3)

        browser.close()

if __name__ == "__main__":
    run_crawler()