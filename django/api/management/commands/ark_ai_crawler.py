# -*- coding: utf-8 -*-
import os
import django
import re
import json
import time
import urllib.parse
import requests
import hashlib
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand

# --- Djangoè¨­å®š ---
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"

class Command(BaseCommand):
    help = 'ã‚¢ãƒ¼ã‚¯ã®è£½å“ã‚’Playwrightã¨Ollama AIã§è§£æã—ã€JSONå‡ºåŠ›ã¨DBä¿å­˜ã‚’è¡Œã†ãƒ•ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ'

    def add_arguments(self, parser):
        parser.add_argument('--limit', type=int, default=100)

    def handle(self, *args, **options):
        # --- è¨­å®š ---
        OLLAMA_API_URL = "http://ollama-v2:11434/api/generate"
        REASONING_MODEL = "gemma3:4b"
        MAKER_NAME = "ark"
        SITE_PREFIX = "Ark"
        # æä¾›ã•ã‚ŒãŸæœ€æ–°ã®ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒ™ãƒ¼ã‚¹URL (sid=3697471, pid=892466351)
        AFFILIATE_BASE_URL = "https://ck.jp.ap.valuecommerce.com/servlet/referral?sid=3697471&pid=892466351&vc_url="
        
        # --- ã€ä¿®æ­£ã€‘Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã®ãƒ‘ã‚¹ã«å¤‰æ›´ ---
        # docker-compose.yml ã§ ./django ãŒ /usr/src/app ã«ãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹ãŸã‚
        # ã“ã®ãƒ‘ã‚¹ã«æ›¸ãè¾¼ã‚€ã“ã¨ã§ãƒ›ã‚¹ãƒˆå´ã® django/scrapers/src/json/ ã«åæ˜ ã•ã‚Œã¾ã™
        JSON_OUTPUT_FILE = "/usr/src/app/scrapers/src/json/ark_results.json"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        output_dir = os.path.dirname(JSON_OUTPUT_FILE)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, mode=0o775, exist_ok=True)
        
        from api.models.pc_products import PCProduct

        results_list = [] # JSONä¿å­˜ç”¨ã®ãƒªã‚¹ãƒˆ

        def call_ollama_simple(prompt):
            """ Ollama APIã‚’å‘¼ã³å‡ºã—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ """
            payload = {
                "model": REASONING_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.0, "num_predict": 1000}
            }
            try:
                print(f"ğŸ¤– AI Requesting ({REASONING_MODEL})...")
                response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
                if response.status_code != 200:
                    print(f"âŒ API Error: {response.status_code}")
                    return ""
                res_text = response.json().get("response", "").strip()
                print(f"--- [AI RAW RESPONSE] ---\n{res_text}\n-------------------------")
                return res_text
            except Exception as e:
                print(f"âŒ Ollama Connection Error: {e}")
                return ""

        def ask_ai_about_spec_detailed(raw_text, web_price=None):
            """ ã‚¹ãƒšãƒƒã‚¯è¡¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ """
            clean_text = raw_text[:4000] # æ–‡å­—æ•°åˆ¶é™å¯¾ç­–
            prompt = f"""Extract technical specs as JSON.
STRICT RULES for 'npu_exists':
- Set TRUE ONLY IF: "Intel Core Ultra", "AMD Ryzen AI", or "Snapdragon X".
- Set FALSE FOR: "Intel Core i3/i5/i7/i9" (14th gen or older).
TEXT:
{clean_text}
JSON TEMPLATE:
{{
  "product_name": "string",
  "genre": "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯" | "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—" | "ãƒ¢ãƒ‹ã‚¿ãƒ¼" | "ãƒ‘ãƒ¼ãƒ„",
  "price": number,
  "cpu": "string or null",
  "gpu": "string or null",
  "ram": "string or null",
  "storage": "string or null",
  "npu_exists": boolean
}}
"""
            raw_res = call_ollama_simple(prompt)
            try:
                # JSONéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
                match = re.search(r'\{.*\}', raw_res, re.DOTALL)
                if match:
                    data = json.loads(match.group(0))
                    # ä¾¡æ ¼è£œå®Œ
                    if (not data.get("price")) and web_price:
                        data["price"] = web_price
                    return data
            except Exception as e:
                print(f"âš ï¸ AI Parse Error: {e}")
            return {"error": "parse_failed", "price": web_price}

        # --- å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º ---
        with sync_playwright() as p:
            print("ğŸš€ Playwrightã‚’èµ·å‹•ä¸­...")
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()
            
            target_urls = []
            limit = options['limit']

            # 1. ãƒªãƒ³ã‚¯åé›†
            self.stdout.write(self.style.HTTP_INFO("ğŸ” ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‚’åé›†ä¸­..."))
            for offset in range(0, limit + 50, 50):
                search_url = f"https://www.ark-pc.co.jp/search/?key=ark&limit=50&offset={offset}"
                try:
                    page.goto(search_url, wait_until="networkidle", timeout=60000)
                    links = page.query_selector_all('a[href*="/i/"]')
                    found_before = len(target_urls)
                    for l in links:
                        href = l.get_attribute('href')
                        if href:
                            full_url = urllib.parse.urljoin("https://www.ark-pc.co.jp", href).split('?')[0]
                            if full_url not in target_urls:
                                target_urls.append(full_url)
                    
                    print(f"ğŸ“„ Offset {offset}: {len(target_urls) - found_before}ä»¶æ–°è¦ç™ºè¦‹")
                    if len(target_urls) >= limit or (len(target_urls) - found_before) == 0:
                        break
                except:
                    break

            target_urls = target_urls[:limit]

            # 2. è©³ç´°è§£æ & ä¿å­˜
            self.stdout.write(self.style.HTTP_INFO(f"ğŸš€ {len(target_urls)}ä»¶ã®è§£æã‚’é–‹å§‹ã—ã¾ã™..."))
            for i, url in enumerate(target_urls):
                print(f"ğŸ”„ [{i+1}/{len(target_urls)}] Accessing: {url}")
                try:
                    # ç”»åƒãªã©ã®ãƒªã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ã‚’å¾…ã¤ãŸã‚ networkidle ã‚’ä½¿ç”¨
                    page.goto(url, wait_until="networkidle", timeout=60000)
                    soup = BeautifulSoup(page.content(), 'html.parser')

                    # --- â‘  ä¾¡æ ¼ã®æŠ½å‡º ---
                    price_val = 0
                    price_tag = soup.select_one('.item_price, .total_price, .price, #item_price_area')
                    if price_tag:
                        price_digits = re.sub(r'\D', '', price_tag.get_text())
                        price_val = int(price_digits) if price_digits else 0
                    
                    # --- â‘¡ ç”»åƒã®æŠ½å‡º (å¼·åŒ–ç‰ˆ) ---
                    image_url = ""
                    # è¤‡æ•°ã®å€™è£œã‚»ãƒ¬ã‚¯ã‚¿ã§ãƒã‚§ãƒƒã‚¯
                    img_selectors = [
                        '#item_main_image img', 
                        '#main_image_container img',
                        '.item_photo img', 
                        '.product-image img',
                        'img[src*="/images/item/"]' # ã‚¢ãƒ¼ã‚¯ã®ç”»åƒãƒ‘ã‚¹ã®ç‰¹å¾´
                    ]
                    
                    for selector in img_selectors:
                        img_tag = soup.select_one(selector)
                        if img_tag:
                            # å„ªå…ˆé †ä½: data-src (é…å»¶èª­ã¿è¾¼ã¿) > src
                            src = img_tag.get('data-src') or img_tag.get('src')
                            if src and 'spacer.gif' not in src:
                                image_url = urllib.parse.urljoin("https://www.ark-pc.co.jp", src)
                                break

                    # JSON-LDã‹ã‚‰ã®è£œå®Œ (ç”»åƒãŒHTMLã‹ã‚‰å–ã‚Œãªã‹ã£ãŸå ´åˆ)
                    if not image_url:
                        json_ld_tags = soup.find_all("script", type="application/ld+json")
                        for tag in json_ld_tags:
                            try:
                                ld = json.loads(tag.string)
                                if isinstance(ld, list): ld = ld[0]
                                if "image" in ld:
                                    img_field = ld["image"]
                                    image_url = img_field[0] if isinstance(img_field, list) else img_field
                                    break
                            except: pass

                    # --- â‘¢ AIè§£æ ---
                    spec_element = soup.select_one('.spec_table, .item_spec_table, #item_spec_area')
                    raw_text = spec_element.get_text("\n") if spec_element else page.inner_text("body")
                    
                    ai_data = ask_ai_about_spec_detailed(raw_text, web_price=price_val)
                    
                    # --- â‘£ ãƒ‡ãƒ¼ã‚¿ã®çµ„ã¿ç«‹ã¦ ---
                    uid = f"ark-ai-{hashlib.md5(url.encode()).hexdigest()[:12]}"
                    
                    cpu = ai_data.get('cpu') or 'N/A'
                    gpu = ai_data.get('gpu') or 'N/A'
                    ram = ai_data.get('ram') or 'N/A'
                    storage = ai_data.get('storage') or 'N/A'
                    npu = ai_data.get('npu_exists', False)

                    description = f"{cpu} / {gpu} / {ram} / {storage} / NPU:{npu}"

                    save_data = {
                        "unique_id": uid,
                        "name": ai_data.get("product_name") or soup.title.string,
                        "maker": MAKER_NAME,
                        "price": ai_data.get("price") or price_val,
                        "description": description,
                        "url": url,
                        "image_url": image_url,
                        "genre": ai_data.get("genre", "ä¸æ˜"),
                        "last_update": time.strftime("%Y-%m-%d %H:%M:%S")
                    }

                    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
                    img_status = "âœ… Found" if image_url else "âŒ Not Found"
                    print(f"ğŸ“Š [DATA] {save_data['name'][:30]} | Image: {img_status}")

                    # DBæ›´æ–°
                    PCProduct.objects.update_or_create(
                        unique_id=uid,
                        defaults={
                            'site_prefix': SITE_PREFIX,
                            'maker': MAKER_NAME,
                            'name': save_data["name"],
                            'price': save_data["price"],
                            'url': url,
                            'affiliate_url': f"{AFFILIATE_BASE_URL}{urllib.parse.quote(url, safe='')}",
                            'image_url': image_url,
                            'description': description,
                            'is_active': True,
                            'stock_status': "åœ¨åº«ã‚ã‚Š",
                            'raw_genre': 'bto-pc',
                        }
                    )

                    # JSONãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
                    results_list.append(save_data)
                    with open(JSON_OUTPUT_FILE, "w", encoding="utf-8") as f:
                        json.dump(results_list, f, ensure_ascii=False, indent=4)

                    time.sleep(1.0)

                except Exception as e:
                    print(f"âŒ Error in {url}: {e}")

            browser.close()
            self.stdout.write(self.style.SUCCESS(f"\nâœ¨ ã‚¢ãƒ¼ã‚¯è£½å“ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼"))