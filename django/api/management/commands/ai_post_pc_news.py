# -*- coding: utf-8 -*-
# /usr/src/app/api/management/commands/ai_post_pc_news.py

import os
import re
import requests
import feedparser
import urllib.parse
import time
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from requests.auth import HTTPBasicAuth
from django.core.files.temp import NamedTemporaryFile

class Command(BaseCommand):
    help = 'å¤–éƒ¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦å°‚é–€ãƒ©ã‚¤ã‚¿ãƒ¼é¢¨ã«è¨˜äº‹ã‚’ç”Ÿæˆã—ã€ã‚«ãƒ†ã‚´ãƒªãƒ»ç”»åƒã‚’è‡ªå‹•åæ˜ ã™ã‚‹'

    def add_arguments(self, parser):
        parser.add_argument('--url', type=str, help='ç‰¹å®šã®è¨˜äº‹URLã‚’ç›´æ¥æŒ‡å®š')
        parser.add_argument('--image', type=str, help='ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒURLã‚’ç›´æ¥æŒ‡å®š')

    def handle(self, *args, **options):
        # --- 1. åŸºæœ¬è¨­å®š ---
        WP_USER = "bicstation"
        WP_APP_PASSWORD = "9re0 t3de WCe1 u1IL MudX 31IY"
        W_DOM = "blog.tiper.live"
        GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        AUTH = HTTPBasicAuth(WP_USER, WP_APP_PASSWORD)
        WP_API_BASE = f"https://{W_DOM}/wp-json/wp/v2"

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
        current_dir = os.path.dirname(os.path.abspath(__file__))
        MODELS_FILE = os.path.join(current_dir, "ai_models.txt")
        # æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        PROMPT_FILE = "/mnt/c/dev/SHIN-VPS/django/api/management/commands/ai_prompt_news.txt"
        HISTORY_FILE = os.path.join(current_dir, "post_history.txt")

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        if not os.path.exists(PROMPT_FILE):
            self.stdout.write(self.style.ERROR(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {PROMPT_FILE}"))
            return

        with open(MODELS_FILE, "r", encoding='utf-8') as f:
            MODELS = [line.strip() for line in f if line.strip()]
        with open(PROMPT_FILE, "r", encoding='utf-8') as f:
            PROMPT_TEMPLATE = f.read()

        posted_links = set()
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, "r", encoding='utf-8') as f:
                posted_links = set(line.strip() for line in f if line.strip())

        # --- 2. è¨˜äº‹å€™è£œã®å–å¾— ---
        target_url = options.get('url')
        target_image_url = options.get('image')
        candidates = []

        if target_url:
            candidates.append({"url": target_url, "source": "ç›´æ¥æŒ‡å®š"})
        else:
            RSS_SOURCES = [
                {"name": "PC Watch", "url": "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf"},
                {"name": "ASCII.jp", "url": "https://ascii.jp/pc/rss.xml"},
                {"name": "ITmedia", "url": "https://rss.itmedia.co.jp/rss/2.0/pcuser.xml"}
            ]
            for source in RSS_SOURCES:
                feed = feedparser.parse(source['url'])
                for entry in feed.entries:
                    if entry.link not in posted_links:
                        candidates.append({"url": entry.link, "source": source['name']})

        # --- 3. æŠ•ç¨¿ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— ---
        success = False
        for item in candidates:
            current_url = item['url']
            self.stdout.write(f"ğŸŒ è§£æé–‹å§‹: {current_url}")
            
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
            try:
                res = requests.get(current_url, timeout=15, headers=headers)
                res.encoding = res.apparent_encoding
                soup = BeautifulSoup(res.text, 'html.parser')
                raw_title = soup.title.string.split('|')[0].strip() if soup.title else "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                
                for s in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'ins']):
                    s.decompose()
                
                main_area = soup.find('article') or soup.find('main') or soup.body
                page_content = main_area.get_text(separator=' ', strip=True) if main_area else ""
                if len(page_content) < 300: continue
            except Exception as e:
                self.stdout.write(f"è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue

            # --- 4. AIè¨˜äº‹ç”Ÿæˆ ---
            self.stdout.write(f"ğŸ¤– AIåŸ·ç­†ä¸­ (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨)...")
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®å¤‰æ•°ã‚’ç½®æ›
            prompt = PROMPT_TEMPLATE.format(
                raw_title=raw_title,
                page_content=page_content[:3500],
                current_url=current_url
            )

            ai_response = ""
            for model in MODELS:
                api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_API_KEY}"
                try:
                    r = requests.post(api_url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=60)
                    if r.status_code == 200:
                        ai_response = r.json()['candidates'][0]['content']['parts'][0]['text']
                        break
                except: continue
            
            if not ai_response: continue

            # --- 5. AIå¿œç­”ã®è§£æ & HTMLæˆå½¢ ---
            lines = ai_response.strip().split('\n')
            # 1è¡Œç›®ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦å–å¾—ï¼ˆè£…é£¾ã‚’é™¤å»ï¼‰
            final_title = re.sub(r'^[#*\s]+|[#*\s]+$', '', lines[0])

            # ã‚«ãƒ†ã‚´ãƒªã¨ã‚¿ã‚°ã®æŠ½å‡º
            cat_name = "PCãƒ‘ãƒ¼ãƒ„"
            tag_names = []
            cat_m = re.search(r'\[CAT\]\s*(.*?)\s*\[/CAT\]', ai_response, re.IGNORECASE)
            if cat_m: cat_name = cat_m.group(1).strip()
            
            tag_m = re.search(r'\[TAG\]\s*(.*?)\s*\[/TAG\]', ai_response, re.IGNORECASE)
            if tag_m: tag_names = [t.strip() for t in tag_m.group(1).split(',') if t.strip()]

            # ãƒ¡ã‚¿æƒ…å ±ã®é™¤å»
            body_only = re.sub(r'\[CAT\].*?\[/CAT\]|\[TAG\].*?\[/TAG\]', '', ai_response, flags=re.DOTALL | re.IGNORECASE)

            # SUMMARYã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è£…é£¾
            html_body = ""
            sum_m = re.search(r'\[SUMMARY\](.*?)\[/SUMMARY\]', body_only, re.DOTALL | re.IGNORECASE)
            if sum_m:
                html_body += '<div style="background:#f8fafc;border:1px solid #e2e8f0;border-radius:12px;padding:20px;margin-bottom:20px;">'
                html_body += '<h4 style="margin-top:0;">ğŸ“ å°‚é–€ãƒ©ã‚¤ã‚¿ãƒ¼ã®è¦ç´„ãƒã‚¤ãƒ³ãƒˆ</h4><ul>'
                for line in sum_m.group(1).strip().split('\n'):
                    p = line.strip().lstrip('*-ãƒ»â€¢ ')
                    if p: html_body += f"<li>{p}</li>"
                html_body += '</ul></div>'
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã¨HTMLå¤‰æ›
            main_text = re.sub(r'\[SUMMARY\].*?\[/SUMMARY\]', '', body_only, flags=re.DOTALL | re.IGNORECASE)
            for line in main_text.split('\n'):
                l = line.strip()
                if not l or l == final_title: continue
                if l.startswith('##'): html_body += f'<h2 class="wp-block-heading">{l.replace("##","").strip()}</h2>'
                elif l.startswith('###'): html_body += f'<h3 class="wp-block-heading">{l.replace("###","").strip()}</h3>'
                else: html_body += f'<p>{l}</p>'
            
            html_body += f'<p style="font-size:0.8em;margin-top:20px;color:#666;">å‡ºå…¸: <a href="{current_url}" target="_blank">{raw_title}</a></p>'

            # --- 6. ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒã®å‡¦ç† ---
            featured_media_id = 0
            img_query = urllib.parse.quote(final_title[:15])
            img_url = target_image_url or f"https://images.unsplash.com/featured/?{img_query}"
            
            try:
                img_res = requests.get(img_url, timeout=20)
                if img_res.status_code == 200:
                    with NamedTemporaryFile(suffix=".jpg", delete=False) as tmp:
                        tmp.write(img_res.content)
                        tmp_path = tmp.name
                    with open(tmp_path, 'rb') as f:
                        m_res = requests.post(f"{WP_API_BASE}/media", auth=AUTH, files={'file': ('eyecatch.jpg', f, 'image/jpeg')}, data={'title': final_title})
                        featured_media_id = m_res.json().get('id', 0)
                    if os.path.exists(tmp_path): os.remove(tmp_path)
            except Exception as e:
                self.stdout.write(f"ç”»åƒå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

            # --- 7. ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¿ã‚°ã®IDå–å¾—ï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°ä½œæˆï¼‰ ---
            def get_or_create_wp_id(path, name):
                try:
                    search_res = requests.get(f"{WP_API_BASE}/{path}?search={urllib.parse.quote(name)}", auth=AUTH).json()
                    for item in search_res:
                        if item['name'] == name: return item['id']
                    create_res = requests.post(f"{WP_API_BASE}/{path}", json={"name": name}, auth=AUTH).json()
                    return create_res.get('id')
                except: return None

            cid = get_or_create_wp_id("categories", cat_name)
            tids = [get_or_create_wp_id("tags", tn) for tn in tag_names if tn]

            # --- 8. WordPressã¸æŠ•ç¨¿ ---
            post_payload = {
                "title": final_title,
                "content": html_body,
                "status": "publish",
                "categories": [cid] if cid else [],
                "tags": [tid for tid in tids if tid],
                "featured_media": featured_media_id
            }
            
            final_res = requests.post(f"{WP_API_BASE}/posts", json=post_payload, auth=AUTH)
            if final_res.status_code == 201:
                self.stdout.write(self.style.SUCCESS(f"ğŸš€ æŠ•ç¨¿æˆåŠŸ: [{cat_name}] {final_title}"))
                with open(HISTORY_FILE, "a", encoding='utf-8') as f:
                    f.write(current_url + "\n")
                success = True
                break
            else:
                self.stdout.write(self.style.ERROR(f"âŒ æŠ•ç¨¿å¤±æ•—: {final_res.status_code} - {final_res.text[:100]}"))

        if not success:
            self.stdout.write("æ–°ç€è¨˜äº‹ã®æŠ•ç¨¿ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")