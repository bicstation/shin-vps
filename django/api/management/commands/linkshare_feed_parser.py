import os
import re
import ftplib
import time
import gzip
import csv
import traceback
import sys
import subprocess
from datetime import datetime, timezone
from typing import List, Tuple, Dict, Any, Optional, Set
from decimal import Decimal, InvalidOperation
import math 
import logging 
from logging import Logger

# â­ tqdmã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from tqdm import tqdm 

# Djangoã®ã‚³ã‚¢æ©Ÿèƒ½ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, IntegrityError 
from django.utils import timezone
from django.conf import settings 

# ==============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã¨åˆæœŸåŒ–
# ==============================================================================

# ğŸš¨ ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ãƒŸãƒ¼å®šç¾© (handleå®Ÿè¡Œå‰ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§ã®NameErrorã‚’å›é¿ã™ã‚‹ãŸã‚)
class DummyModel:
    """handleå®Ÿè¡Œå‰ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã§ã®NameErrorã‚’å›é¿ã™ã‚‹ãŸã‚ã®ãƒ€ãƒŸãƒ¼å®šç¾©"""
    objects = None
    def __init__(self, **kwargs): pass
    # LinkshareProduct ãŒæŒã¤ã“ã¨ãŒæƒ³å®šã•ã‚Œã‚‹å±æ€§ã‚’è¿½åŠ 
    id = None
    merchant_id = None
    created_at = None
    updated_at = None
    price = None
    in_stock = None
    is_active = None
    affiliate_url = None 
    sku = None
    product_url = None
    raw_csv_data = None 
    merchant_name = None 
    product_name = None 

LinkshareProduct = DummyModel

# ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ– (handleå†…ã§è¨­å®šã‚’ä¸Šæ›¸ã)
logger: Logger = logging.getLogger(__name__)

# ==============================================================================
# æ¥ç¶šãƒ»ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š (å®šæ•°)
# ==============================================================================
# ç’°å¢ƒå¤‰æ•°ãŒãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
FTP_HOST = os.getenv("LINKSHARE_FTP_HOST", "aftp.linksynergy.com")
FTP_USER = os.getenv("LINKSHARE_BS_USER", "rkp_3750988") 
FTP_PASS = os.getenv("LINKSHARE_BS_PASS", "u5NetPVZEAhABD7HuW2VRymP") 
FTP_PORT = 21
FTP_TIMEOUT = 180

MAX_SIZE_BYTES = 1073741824 # 1 GB ã®ãƒã‚¤ãƒˆå€¤
DOWNLOAD_DIR = "/tmp/ftp_downloads"

# LinkShareãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
FULL_DATA_PATTERN = re.compile(r"(\d+)_3750988_mp\.txt\.gz$")
DELTA_DATA_PATTERN = re.compile(r"(\d+)_3750988_delta\.txt\.gz$")

FIXED_DELIMITER = '|'
FIXED_DELIMITER_NAME = 'PIPE'

# ğŸš€ ä¿®æ­£: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å ±å‘Šé »åº¦ã‚’å‰Šæ¸›
BATCH_SIZE = 1000 # DBã¸ã®ãƒãƒ«ã‚¯å‡¦ç†ä»¶æ•°
ROWS_PER_REPORT = 10000 # é€²è¡ŒçŠ¶æ³ã‚’å ±å‘Šã™ã‚‹é »åº¦ (ç¾åœ¨ã¯ä¸»ã«ãƒ‡ãƒãƒƒã‚°ç”¨é€”)
FIELD_MAPPING = {
    # C3: SKU -> sku (æ–°ã—ã„ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã®ä¸€éƒ¨)
    'C3': {'DB_FIELD': 'sku', 'TYPE': 'str', 'PK': True, 'DESCRIPTION': 'å•†å“ã‚³ãƒ¼ãƒ‰ (SKUã€æ–°ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼)'},
    # ğŸ’¡ ä¿®æ­£: C2 ã‚’ product_name ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    'C2': {'DB_FIELD': 'product_name', 'TYPE': 'str', 'DESCRIPTION': 'å•†å“å'},
    'C13': {'DB_FIELD': 'price', 'TYPE': 'Decimal', 'DESCRIPTION': 'ä¾¡æ ¼ (æ—§å®šä¾¡)'},
    # C6: affiliate_url ã‚’ DBãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦å¾©å¸°
    'C6': {'DB_FIELD': 'affiliate_url', 'TYPE': 'str', 'DESCRIPTION': 'ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆURL'}, 
    # 'C9': {'DB_FIELD': 'product_url', 'TYPE': 'str', 'DESCRIPTION': 'è£½å“URL'}, 
    # è¿½åŠ : ãƒ‡ãƒãƒƒã‚°å¼·åŒ–ã®ãŸã‚ã€å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»¥å¤–ã‚‚ãƒãƒƒãƒ”ãƒ³ã‚°ã«è¿½åŠ ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„
    'C4': {'DB_FIELD': 'description', 'TYPE': 'str', 'DESCRIPTION': 'èª¬æ˜/è£œè¶³ä¾¡æ ¼'},
    'C9': {'DB_FIELD': 'product_url', 'TYPE': 'str', 'DESCRIPTION': 'è£½å“URL'},
}

EXPECTED_COLUMNS_COUNT = 38 
DATE_FORMAT = '%Y%m%d %H:%M:%S'
DATE_REGEX = re.compile(r'^\d{8} \d{2}:\d{2}:\d{2}$')


# ==============================================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤
# ==============================================================================
def human_readable_size(size_bytes: int) -> str:
    size_name = ("B", "KB", "MB", "GB", "TB")
    if size_bytes == 0:
        return "0B"
    try:
        i = int(math.floor(math.log(size_bytes, 1024)))
        p = pow(1024, i) 
        s = round(size_bytes / p, 2)
        
        if i >= len(size_name):
            return f"{size_bytes} B"
            
        return f"{s:,.2f} {size_name[i]}"
    except Exception:
        return f"{size_bytes} B"

def _get_ftp_client() -> Optional[ftplib.FTP]:
    try:
        ftp_client = ftplib.FTP()
        ftp_client.set_pasv(True) 
        ftp_client.set_debuglevel(0)
        
        ftp_client.connect(FTP_HOST, FTP_PORT, FTP_TIMEOUT)
        ftp_client.login(FTP_USER, FTP_PASS)
        
        return ftp_client
        
    except ftplib.all_errors as e:
        logger.error(f"âŒ [ERROR] FTPæ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—: {e}", exc_info=False)
        return None
        
    except Exception as e:
        logger.error(f"âŒ [ERROR] FTPæ¥ç¶šå‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=False)
        return None

def get_ftp_mid_list(ftp_client: ftplib.FTP) -> List[Tuple[str, str, str, Optional[datetime], int]]:
    file_list = []
    
    try:
        logger.info("ğŸ“¡ [FTP] ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ (MLSD) ã‚’å–å¾—ä¸­...")
        # MLSDã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        for filename, facts in ftp_client.mlsd():
            
            if facts.get('type') != 'file' or 'size' not in facts:
                continue
                
            is_full_data = FULL_DATA_PATTERN.match(filename)
            is_delta_data = DELTA_DATA_PATTERN.match(filename)
            
            if is_full_data or is_delta_data:
                mid = is_full_data.group(1) if is_full_data else is_delta_data.group(1)
                file_type = 'full' if is_full_data else 'delta'
                
                try:
                    file_size = int(facts.get('size', 0))
                except ValueError:
                    file_size = 0
                
                mtime_str = facts.get('modify')
                mtime_dt = None
                if mtime_str:
                    try:
                        mtime_dt = datetime.strptime(mtime_str, '%Y%m%d%H%M%S').replace(tzinfo=timezone.utc)
                    except ValueError:
                        pass
                        
                file_list.append((mid, filename, file_type, mtime_dt, file_size))
                        
        logger.info(f"âœ… [FTP] {len(file_list)} ä»¶ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                
    except ftplib.all_errors as e:
        logger.error(f"âŒ [ERROR] FTPãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {e}", exc_info=False)
        return []
        
    return file_list

def safe_cast(value: str, target_type: str, field_name: str) -> Optional[Any]:
    if not value or value.strip().lower() in ('null', 'none', 'n/a', ''):
        return None
        
    stripped_value = value.strip()
    
    # ğŸ’¡ ä¿®æ­£: éç ´å£Šçš„ãªæ–¹æ³•ã§ä¸å¯è¦–æ–‡å­— (U+00A0ãªã©) ã‚’æ¨™æº–ã®ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®ãæ›ãˆã‚‹
    stripped_value = stripped_value.replace('\u00a0', ' ')
    
    if target_type == 'Decimal':
        try:
            # ã‚«ãƒ³ãƒã€é€šè²¨è¨˜å·ã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’å–ã‚Šé™¤ã
            temp_value = stripped_value.replace(',', '').replace('$', '').replace('Â¥', '').strip()
            if not temp_value:
                return None
            return Decimal(temp_value)
        except InvalidOperation:
            # logger.debug(f"âš ï¸ [SAFE_CAST] Decimalå¤‰æ›å¤±æ•— ({field_name}): '{stripped_value}'")
            return None
            
    elif target_type == 'datetime':
        if not DATE_REGEX.match(stripped_value):
            return None
        try:
            dt = datetime.strptime(stripped_value, DATE_FORMAT)
            return dt.replace(tzinfo=timezone.utc)
        except ValueError:
            return None
            
    return stripped_value


# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯
# ==============================================================================

def _display_mapping_for_first_row(row_list: List[str]):
    """æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ‘ãƒ¼ã‚¹çµæœã‚’è¡¨ç¤ºã—ã€ã‚«ãƒ©ãƒ ã‚ºãƒ¬ã®ç¢ºèªã‚’åŠ©ã‘ã‚‹ã€‚ï¼ˆãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼‰"""
    
    # ğŸ’¡ ä¿®æ­£: ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã¨ã—ã¦å‡ºåŠ›
    logger.debug("\n--- æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ãƒ‘ãƒ¼ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚° (ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: LinkShare Column -> DB Field -> Raw Value) ---")
    logger.debug(f"ç·ã‚«ãƒ©ãƒ æ•°: {len(row_list)} / æœŸå¾…å€¤: {EXPECTED_COLUMNS_COUNT}")
    
    # æ¨™æº–å‡ºåŠ›ã«ç›´æ¥æ›¸ãï¼ˆlogging.debugã§ã¯è¡¨å½¢å¼ã®æ•´åˆ—ãŒé›£ã—ã„å ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰
    sys.stdout.write(f"{'LS-COL':<7} | {'DB FIELD':<35} | {'RAW VALUE (å…ˆé ­50æ–‡å­—)':<50}\n")
    sys.stdout.write("-" * 98 + "\n")
    
    # ã‚³ã‚¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‘ãƒ¼ã‚¹è¡¨ç¤º
    for i in range(EXPECTED_COLUMNS_COUNT):
        col_name = f'C{i+1}'
        mapping = FIELD_MAPPING.get(col_name)
        
        if mapping:
            db_field = mapping.get('DB_FIELD', 'N/A')
        else:
            continue
            
        raw_value = row_list[i] if i < len(row_list) else ""
        
        display_value = raw_value.replace('\n', '\\n').replace('\r', '\\r')
        if len(display_value) > 50:
            display_value = display_value[:47] + "..."
            
        sys.stdout.write(f"{col_name:<7} | {db_field:<35} | '{display_value}'\n")
            
    sys.stdout.write(f"{'ALL':<7} | {'raw_csv_data':<35} | 'å…¨ã‚«ãƒ©ãƒ ã®ç”Ÿãƒ‡ãƒ¼ã‚¿'\n")
    sys.stdout.write("--------------------------------------------------------------------------------------------------\n")
    sys.stdout.flush()


def _parse_single_row(row_list: List[str], mid: str, advertiser_name: str) -> Optional[Dict[str, Any]]:
    """å˜ä¸€è¡Œã®CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€DBä¿å­˜ç”¨ã®è¾æ›¸å½¢å¼ã«å¤‰æ›ã™ã‚‹ã€‚"""
    if len(row_list) != EXPECTED_COLUMNS_COUNT:
        # ã“ã®å‡¦ç†ã¯ parse_and_process_file ã§æ—¢ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹ãŒã€å¿µã®ãŸã‚
        return None 

    data: Dict[str, Any] = {
        'merchant_id': mid, 
        'created_at': timezone.now(), 
        'updated_at': timezone.now(),
        'is_active': True,
        'in_stock': True, 
        'sku': None, 
        'product_url': '', 
        'product_name': None, 
        'affiliate_url': '', 
        'merchant_name': advertiser_name, 
        'price': None, 
    }
    
    data['raw_csv_data'] = FIXED_DELIMITER.join(row_list)

    # ã‚³ã‚¢ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‘ãƒ¼ã‚¹ (priceä»¥å¤–)
    for i in range(EXPECTED_COLUMNS_COUNT):
        col_name = f'C{i+1}'
        raw_value = row_list[i] if i < len(row_list) else '' # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²å¤–ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—
        
        mapping = FIELD_MAPPING.get(col_name)
        if not mapping:
            continue
            
        db_field = mapping.get('DB_FIELD')
        data_type = mapping.get('TYPE')

        # C13 (price) ã¯å¤šé‡ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ã€ã“ã“ã§ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
        if db_field == 'price':
             continue

        if not db_field:
            continue
        
        parsed_value = safe_cast(raw_value, data_type, db_field)
        if parsed_value is not None:
            data[db_field] = parsed_value

    # --- å¿…é ˆãƒã‚§ãƒƒã‚¯: sku, product_name ã¯å¿…é ˆ ---
    
    # 1. SKUãƒã‚§ãƒƒã‚¯ (C3)
    if not data.get('sku'):
        raw_c3 = row_list[2] if len(row_list) > 2 else 'N/A'
        raw_c3_clean = raw_c3.replace('\n', '\\n').replace('\r', '\\r')
        logger.debug(f"[MID: {mid}] ã‚¹ã‚­ãƒƒãƒ—: SKU (C3) ãŒNoneã€‚Raw C3: '{raw_c3_clean}'")
        return None
        
    # 2. Product Nameãƒã‚§ãƒƒã‚¯ (C2)
    if not data.get('product_name'):
        raw_c2 = row_list[1] if len(row_list) > 1 else 'N/A'
        raw_c2_clean = raw_c2.replace('\n', '\\n').replace('\r', '\\r')
        logger.debug(f"[MID: {mid}] ã‚¹ã‚­ãƒƒãƒ—: Product Name (C2) ãŒNoneã€‚Raw C2: '{raw_c2_clean}'")
        return None
        
    # 3. Priceãƒã‚§ãƒƒã‚¯ (C13, C14) - å„ªå…ˆé †ä½ã«å¾“ã£ã¦ãƒã‚§ãƒƒã‚¯ã—ã€ãƒ‡ãƒ¼ã‚¿ã«æ ¼ç´
    
    # ä¾¡æ ¼ãŒæ ¼ç´ã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚«ãƒ©ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã€ãƒã‚§ãƒƒã‚¯å„ªå…ˆé †ä½ (C1 = index 0)
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã«åŸºã¥ã„ã¦ C14 ã‚’å„ªå…ˆ
    price_check_indices = {
        'C14 (Price Candidate)': 13,   # â¬…ï¸ 1. C14ã‚’æœ€åˆã«ç¢ºèª (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ 13)
        'C13 (Original Price)': 12,    # â¬…ï¸ 2. C13ã‚’æ¬¡ã«ç¢ºèª (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ 12)
        'C12': 11,
        'C4 (Description/Price?)': 3,
        'C5 (Category/Price?)': 4,
    }
    
    # å„ªå…ˆåº¦é †ã«ã‚«ãƒ©ãƒ ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æœ€åˆã«è¦‹ã¤ã‹ã£ãŸæœ‰åŠ¹ãªä¾¡æ ¼ã‚’æ¡ç”¨
    for name, index in price_check_indices.items():
        if index < len(row_list):
            raw_value = row_list[index]
            # Priceã«ã¯ Decimalå‹ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ãŸã‚ã€safe_castã‚’ä½¿ç”¨
            parsed_price = safe_cast(raw_value, 'Decimal', name)
            
            # ä¾¡æ ¼ãŒæœ‰åŠ¹ãª Decimalå€¤ã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸã‚‰æ¡ç”¨
            if parsed_price is not None:
                data['price'] = parsed_price
                break # ä¾¡æ ¼ãŒè¦‹ã¤ã‹ã£ãŸã®ã§ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†

    # 4. Priceã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ (å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦)
    if data.get('price') is None:
        
        # ãƒ­ã‚°å‡ºåŠ›ç”¨ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        debug_info = {}
        for name, index in price_check_indices.items():
            if index < len(row_list):
                # ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã€é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                raw_value = row_list[index].replace('\n', '\\n').replace('\r', '\\r')
                debug_info[name] = raw_value[:30] + ('...' if len(raw_value) > 30 else '')
            else:
                debug_info[name] = 'INDEX_OUT_OF_BOUNDS'
        
        debug_log_str = " | ".join([f"{k}: '{v}'" for k, v in debug_info.items()])
        
        # ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿®æ­£: C13ã ã‘ã§ãªãã€ä¾¡æ ¼å…¨ä½“ãŒNoneã§ã‚ã‚‹ã“ã¨ç¤ºã™
        logger.debug(f"[MID: {mid}] ã‚¹ã‚­ãƒƒãƒ—: Price ãŒNoneã€‚ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯: {debug_log_str}")
        return None
    
    return data


def _bulk_import_products(mid: str, product_data_list: List[Dict[str, Any]]) -> Tuple[int, int, int]:
    
    if not product_data_list or LinkshareProduct == DummyModel:
        # ğŸ’¡ ä¿®æ­£: ãƒ­ã‚®ãƒ³ã‚°ã«ç½®ãæ›ãˆ
        logger.warning(f"âš ï¸ [BULK] LinkshareProductãƒ¢ãƒ‡ãƒ«æœªå®šç¾©ã®ãŸã‚ã€DBä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚å‡¦ç†ä»¶æ•°: {len(product_data_list)}")
        return len(product_data_list), len(product_data_list), 0 

    # è¾æ›¸ã®ã‚­ãƒ¼ã¯ 'sku'
    incoming_sku_map = {data['sku']: data for data in product_data_list if data.get('sku')} 
    skus_to_check = list(incoming_sku_map.keys())
    
    to_create_linkshare: List[LinkshareProduct] = []
    to_update_linkshare: List[LinkshareProduct] = []

    # 1. LinkshareProduct ã® Upsert æº–å‚™
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ 'sku__in'
    existing_products = LinkshareProduct.objects.filter(
        merchant_id=mid,
        sku__in=skus_to_check
    ).only(
        'sku', 
        'id', 
        'price', 
        'in_stock', 
        'is_active', 
        'product_url',
        'affiliate_url', 
        'raw_csv_data',
        'merchant_id',
        'created_at',
        'updated_at',
        'merchant_name',
        'product_name',
    )
    
    existing_sku_map = {p.sku: p for p in existing_products}
    
    # æ›´æ–°å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    update_fields = [
        'price', 'in_stock', 'is_active', 
        'sku', 'product_url', 'affiliate_url', 
        'raw_csv_data',
        'updated_at',
        'merchant_name',
        'product_name',
    ]
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    required_fields = [
        'price', 'in_stock', 'is_active', 'sku', 'product_url', 
        'raw_csv_data', 'affiliate_url', 'merchant_name', 'product_name'
    ] 
    
    for sku, data in incoming_sku_map.items():
        
        # Djangoãƒ¢ãƒ‡ãƒ«ãŒæŒã¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’å³å¯†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        allowed_fields = set(required_fields + ['merchant_id', 'created_at', 'updated_at', 'sku']) 
        
        clean_data = {
            k: v for k, v in data.items() 
            if hasattr(LinkshareProduct, k) and k in allowed_fields
        }
        
        clean_data['updated_at'] = timezone.now() 

        if 'created_at' not in clean_data:
            clean_data['created_at'] = timezone.now() 
        
        if sku in existing_sku_map:
            product_instance = existing_sku_map[sku]
            is_updated = False
            
            for key in update_fields:
                if key not in clean_data:
                    continue
                
                current_value = getattr(product_instance, key)
                new_value = clean_data[key]
                
                is_diff = True
                if current_value == new_value:
                    is_diff = False
                elif isinstance(current_value, Decimal) and isinstance(new_value, Decimal):
                    if current_value.compare(new_value) == 0:
                        is_diff = False
                elif current_value is None and new_value == '': 
                    is_diff = False
                elif new_value is None and current_value == '':
                    is_diff = False
                        
                if is_diff:
                    setattr(product_instance, key, new_value)
                    is_updated = True
            
            if is_updated:
                to_update_linkshare.append(product_instance)
        else:
            # æ–°è¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            new_instance = LinkshareProduct(**clean_data)
            to_create_linkshare.append(new_instance)
    
    updated_count = 0
    if to_update_linkshare:
        try:
            # bulk_update ã® batch_size ã‚’ BATCH_SIZE (1000) ã«è¨­å®š
            LinkshareProduct.objects.bulk_update(to_update_linkshare, update_fields, batch_size=BATCH_SIZE)
            updated_count = len(to_update_linkshare)
        except Exception as e:
            logger.error(f" âŒ [MID: {mid}] ãƒãƒ«ã‚¯æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
            
    created_count = 0
    if to_create_linkshare:
        try:
            # bulk_create ã® batch_size ã‚’ BATCH_SIZE (1000) ã«è¨­å®š
            LinkshareProduct.objects.bulk_create(
                to_create_linkshare, 
                batch_size=BATCH_SIZE 
            )
        # IntegrityError ã¯ãƒãƒ«ã‚¯ä½œæˆã§æ•æ‰ã•ã‚Œã¾ã™
        except IntegrityError as e:
            logger.error(f" âŒ [MID: {mid}] ãƒãƒ«ã‚¯ä½œæˆä¸­ã«IntegrityErrorãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
            
        created_count = len(to_create_linkshare)
            
    return created_count + updated_count, created_count, updated_count


# ==============================================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜ã‚’çµ±åˆã—ãŸãƒ¡ã‚¤ãƒ³å‡¦ç† (tqdmå¯¾å¿œç‰ˆ)
# ==============================================================================

def parse_and_process_file(local_path: str, mid: str) -> Tuple[bool, int]:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€DBã«ä¿å­˜ã™ã‚‹ã€‚"""
    
    current_batch: List[Dict[str, Any]] = []
    total_saved_rows = 0
    advertiser_name: str = 'N/A'
    first_row_logged = False
    
    delimiter = FIXED_DELIMITER 
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨è¡Œæ•°ã‚’å–å¾— (tqdmã®Totalè¨­å®šã®ãŸã‚)
        # ğŸ’¡ ä¿®æ­£: ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰±ã†éš›ã®ã‚¨ãƒ©ãƒ¼å›é¿
        try:
            with open(local_path, 'r', encoding='utf-8') as f:
                # è¡Œæ•°ã‚’æ­£ç¢ºã«æ•°ãˆã‚‹ (HDRè¡Œã‚„æœ«å°¾ã®ç©ºè¡Œã‚’è€ƒæ…®)
                total_lines = sum(1 for line in f if line.strip())
        except Exception as e:
            logger.error(f"âŒ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False, 0
        
        if total_lines == 0:
            logger.warning(f"âš ï¸ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç©ºã§ã™ã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return True, 0

        data_lines_to_process = total_lines # åˆæœŸå€¤
        
        with open(local_path, 'r', encoding='utf-8', newline='') as f:
            reader = csv.reader(f, delimiter=delimiter)
            
            # 1. HDRè¡Œã®å‡¦ç† (1è¡Œç›®)
            try:
                hdr_row = next(reader)
                
                is_hdr_present = False
                if hdr_row and hdr_row[0].strip() == 'HDR':
                    advertiser_name = hdr_row[2].strip() if len(hdr_row) > 2 else 'N/A'
                    logger.info(f"ğŸ’¡ [MID: {mid}] Advertiser Nameã‚’å–å¾—: '{advertiser_name}'ã€‚æ¬¡ã®è¡Œã‹ã‚‰ãƒ‡ãƒ¼ã‚¿é–‹å§‹ã€‚")
                    is_hdr_present = True
                else:
                    logger.warning(f"âš ï¸ [MID: {mid}] HDRè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
                    f.seek(0) # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                    reader = csv.reader(f, delimiter=delimiter)
                    
                if is_hdr_present:
                    data_lines_to_process -= 1 # ãƒ‡ãƒ¼ã‚¿è¡Œã®ç·æ•°ã‹ã‚‰HDRè¡Œã‚’å¼•ã
                    
            except StopIteration:
                logger.error(f"âŒ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚")
                return False, 0
                
            # â­ 2. ãƒ‡ãƒ¼ã‚¿è¡Œã®å‡¦ç† (tqdmã§ãƒ©ãƒƒãƒ—)
            progress_bar = tqdm(
                reader, # readerã‚’ç›´æ¥æ¸¡ã™
                desc=f"ğŸ“¦ Parsing MID {mid}",
                unit=" lines",
                file=sys.stdout,
                total=data_lines_to_process, # totalã‚’è¨­å®š
                leave=True, 
            )
            
            for row in progress_bar:
                
                # è¡ŒãŒç©ºã ã£ãŸã‚Šã€ã‚«ãƒ©ãƒ æ•°ãŒä¸æ­£ãªå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if not row or not row[0].strip() or len(row) != EXPECTED_COLUMNS_COUNT:
                    continue
                
                # ğŸš¨ ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œã®ã¿ã€å…¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
                # tqdmãƒ«ãƒ¼ãƒ—ã®æœ€åˆã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (progress_bar.n == 1) ã§è¡¨ç¤º
                if not first_row_logged and progress_bar.n == 1 and logger.getEffectiveLevel() <= logging.DEBUG:
                    _display_mapping_for_first_row(row)
                    first_row_logged = True
                
                # 3. å˜ä¸€è¡Œã®ãƒ‘ãƒ¼ã‚¹
                record = _parse_single_row(row, mid, advertiser_name)
                
                # å¿…é ˆãƒã‚§ãƒƒã‚¯ãŒå¤±æ•—ã—ãŸå ´åˆã€_parse_single_rowå†…ã§ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œã€ã“ã“ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹
                # priceã®ãƒã‚§ãƒƒã‚¯ã¯ _parse_single_row ã®ä¸­ã§è¡Œã‚ã‚Œã¦ã„ã‚‹
                if not record or not record.get('sku') or not record.get('merchant_id') or record.get('price') is None:
                    continue

                current_batch.append(record)
                
                # 4. ãƒãƒƒãƒå‡¦ç†
                if len(current_batch) >= BATCH_SIZE:
                    saved, created, updated = _bulk_import_products(mid, current_batch)
                    total_saved_rows += saved
                    
                    # â­ DBæ›¸ãè¾¼ã¿å®Œäº†æƒ…å ±ã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®å³å´ã«è¡¨ç¤º
                    progress_bar.set_postfix_str(f"DB Save: {saved:,} (New:{created:,}, Upd:{updated:,})")
                    
                    current_batch = []

            # 5. æœ€çµ‚ãƒãƒƒãƒã®å‡¦ç†
            if current_batch:
                saved, created, updated = _bulk_import_products(mid, current_batch)
                total_saved_rows += saved
                # æœ€çµ‚ãƒãƒƒãƒã®å‡¦ç†å®Œäº†ãƒ­ã‚°
                logger.info(f"â³ [MID: {mid}] æœ€çµ‚ãƒãƒƒãƒå®Œäº†ã€‚ä¿å­˜: {saved:,} (æ–°è¦:{created:,}, æ›´æ–°:{updated:,})")

            # æœ€çµ‚å®Œäº†ãƒ­ã‚° (tqdmã®ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½¿ç”¨)
            logger.info(f"âœ… [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ã‚¹å®Œäº†ã€‚ç·ãƒ‘ãƒ¼ã‚¹ä»¶æ•°: {progress_bar.n:,} ä»¶")
            return True, total_saved_rows

    except Exception as e:
        logger.error(f"âŒ [MID: {mid}] ãƒ‘ãƒ¼ã‚¹å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return False, 0


def download_file(ftp_client: ftplib.FTP, filename: str, local_path_gz: str, local_path_txt: str, mid: str, file_size: int) -> Tuple[bool, int]:
    logger.info(f"ğŸ“¡ [MID: {mid}] ãƒ•ã‚¡ã‚¤ãƒ« {filename} ({human_readable_size(file_size)}) ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹...")
    
    ENCODING = 'utf-8'
    ERROR_HANDLING = 'ignore' 
    
    try:
        # 1. GZãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with open(local_path_gz, 'wb') as f:
            ftp_client.retrbinary(f'RETR {filename}', f.write)

        logger.info(f"ğŸ“¦ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚è§£å‡ä¸­...")
        
        # 2. GZãƒ•ã‚¡ã‚¤ãƒ«ã®è§£å‡ã¨ãƒ‡ã‚³ãƒ¼ãƒ‰ (ã‚¨ãƒ©ãƒ¼ç„¡è¦–)
        decompressed_size = 0
        
        with gzip.open(local_path_gz, 'rb') as f_in:
            with open(local_path_txt, 'w', encoding='utf-8', newline='') as f_out:
                
                buffer_size = 1024 * 1024 # 1MB chunk
                
                # GZãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã‚’ approximate totalã¨ã—ã¦ä½¿ç”¨ 
                with tqdm(total=file_size, unit='B', unit_scale=True, desc=f"ğŸ”“ Decompressing {mid}", file=sys.stdout, leave=False) as t:
                    while True:
                        chunk = f_in.read(buffer_size)
                        if not chunk:
                            break
                        
                        text_chunk = chunk.decode(ENCODING, errors=ERROR_HANDLING) 
                        f_out.write(text_chunk)
                        
                        decompressed_size += len(text_chunk)
                        t.update(len(chunk)) # åœ§ç¸®ã•ã‚ŒãŸãƒã‚¤ãƒˆæ•°ã§ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
                        
        logger.info(f"âœ… [MID: {mid}] è§£å‡ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {ENCODING}, ã‚¨ãƒ©ãƒ¼å‡¦ç†: {ERROR_HANDLING})ã€‚TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {human_readable_size(decompressed_size)}")
        
        # 3. GZãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        os.remove(local_path_gz)
        
        return True, decompressed_size

    except ftplib.all_errors as e:
        logger.error(f"âŒ [MID: {mid}] FTPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        if os.path.exists(local_path_gz): os.remove(local_path_gz)
        if os.path.exists(local_path_txt): os.remove(local_path_txt)
        return False, 0
    except Exception as e:
        logger.error(f"âŒ [MID: {mid}] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/è§£å‡ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        if os.path.exists(local_path_gz): os.remove(local_path_gz)
        if os.path.exists(local_path_txt): os.remove(local_path_txt)
        return False, 0


# ==============================================================================
# Django Management Command ã®å®šç¾©
# ==============================================================================
class Command(BaseCommand):
    
    help = 'LinkShare FTPã‹ã‚‰ãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€LinkshareProductãƒ¢ãƒ‡ãƒ«ã«ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚'

    def add_arguments(self, parser):
        parser.add_argument(
            '--mid', 
            type=str, 
            help='å‡¦ç†ã™ã‚‹ç‰¹å®šã®ãƒãƒ¼ãƒãƒ£ãƒ³ãƒˆID (MID) ã‚’æŒ‡å®šã—ã¾ã™ã€‚', 
            default=None
        )
        parser.add_argument(
            '--limit',
            type=int,
            help='å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§æ•°ã€‚ãƒ‡ãƒãƒƒã‚°ã‚„ãƒ†ã‚¹ãƒˆæ™‚ã«ä¾¿åˆ©ã§ã™ã€‚',
            default=None
        )
        # verbosityã¯BaseCommandã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ (0=ERROR, 1=INFO, 2=DEBUG, 3=TRACE)

    def handle(self, *args, **options):
        
        # ğŸ’¡ ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒ­ã‚¬ãƒ¼ã®ãƒ¬ãƒ™ãƒ«è¨­å®š
        verbosity = int(options.get('verbosity', 1)) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 1 (INFO)
        
        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®èª¿æ•´ (verbosity=2 ã§ DEBUG ãƒ¬ãƒ™ãƒ«)
        if verbosity >= 2:
            log_level = logging.DEBUG
        elif verbosity == 1:
            log_level = logging.INFO
        else: # verbosity = 0
            log_level = logging.ERROR

        # ãƒ­ã‚¬ãƒ¼ã¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
        logger.setLevel(log_level)
        if not logger.handlers:
            # Djangoã®stdout/stderrã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            class DjangoConsoleHandler(logging.StreamHandler):
                def __init__(self, stdout, stderr):
                    super().__init__(self._get_stream(stdout, stderr))
                    self.stdout = stdout
                    self.stderr = stderr
                
                def _get_stream(self, stdout, stderr):
                    # INFO/DEBUGãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’æ¨™æº–å‡ºåŠ›ã«é€ã‚‹
                    return sys.stdout 
                
                def emit(self, record):
                    # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ™ãƒ«ã®å ´åˆã€BaseCommandã®stderrã‚’åˆ©ç”¨
                    if record.levelno >= logging.ERROR:
                        stream = self.stderr
                    else:
                        stream = self.stdout
                    self.stream = stream
                    super().emit(record)


            handler = DjangoConsoleHandler(self.stdout, self.stderr)
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å¤‰æ›´ã—ã¦ãƒ­ã‚®ãƒ³ã‚°ã‹ã‚‰ print ã¨ä¼¼ãŸå‡ºåŠ›ã‚’å¾—ã‚‹
            formatter = logging.Formatter('%(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        logger.info("--- LinkShare ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰é–‹å§‹ (ãƒãƒ«ã‚¯å‡¦ç†) ---")

        # ğŸš¨ ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ãƒ¼ãƒ—ã®ç½®ãæ›ãˆ
        try:
            # é©åˆ‡ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã«ä¿®æ­£ã—ã¦ãã ã•ã„
            # from api.models.linkshare_products import LinkshareProduct as RealLinkshareProduct
            from api.models import LinkshareProduct as RealLinkshareProduct # ä»®å®š
            
            globals()['LinkshareProduct'] = RealLinkshareProduct
            logger.info("âœ… ãƒ¢ãƒ‡ãƒ« (LinkshareProduct) ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸã€‚")
            
        except ImportError as e:
            logger.error(f"ğŸš¨ CRITICAL: ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚DBã¸ã®ä¿å­˜ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        if not os.path.exists(DOWNLOAD_DIR):
            os.makedirs(DOWNLOAD_DIR)
            logger.info(f"ğŸ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {DOWNLOAD_DIR} ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        # FTPæ¥ç¶š
        ftp_client = _get_ftp_client()

        if not ftp_client:
            logger.error("ğŸš¨ FTPæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return

        total_processed_files = 0
        total_saved_rows = 0
        mid_list: List[Tuple[str, str, str, Optional[datetime], int]] = [] 

        try:
            # 1. FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®å–å¾—
            mid_list = get_ftp_mid_list(ftp_client) 
            
            limit = options['limit']

            if options['mid']:
                mid_list = [item for item in mid_list if item[0] == options['mid']]

            # --limit ã®é©ç”¨
            if limit is not None and limit > 0:
                mid_list = mid_list[:limit]

            if not mid_list:
                logger.warning("âŒ å‡¦ç†å¯¾è±¡ã¨ãªã‚‹LinkShareãƒãƒ¼ãƒãƒ£ãƒ³ãƒ€ã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            logger.info(f"âœ… {len(mid_list)} ä»¶ã®MIDãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

            # FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®è¡¨ç¤º
            self.stdout.write(self.style.NOTICE("\n--- å‡¦ç†å¯¾è±¡ã®LinkShare FTPãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ---"))
            self.stdout.write(f"{'MID':<6} | {'ãƒ•ã‚¡ã‚¤ãƒ«å':<40} | {'ã‚µã‚¤ã‚º':<10} | æœ€çµ‚æ›´æ–° (UTC)")
            self.stdout.write("-" * 75)
            for mid_id, filename, file_type, mtime_dt, file_size in mid_list:
                size_hr = human_readable_size(file_size)
                mtime_str = mtime_dt.strftime('%Y-%m-%d %H:%M:%S') if mtime_dt else 'N/A'
                self.stdout.write(f"{mid_id:<6} | {filename:<40} | {size_hr:<10} | {mtime_str}")
            self.stdout.write("----------------------------------------------\n")
            
            # --- ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹ ---
            for mid, filename, file_type, mtime_dt, file_size in mid_list:
                logger.info(f"\n--- [MID: {mid}] å‡¦ç†é–‹å§‹ ({filename}) ---")
                
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®æ±ºå®š
                local_gz_path = os.path.join(DOWNLOAD_DIR, filename)
                local_txt_path = local_gz_path.replace('.gz', '.txt')

                # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç† (Atomic: å¤±æ•—æ™‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                with transaction.atomic():
                    success = False
                    current_saved_rows = 0
                    try:
                        # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è§£å‡ (tqdmå¯¾å¿œ)
                        is_downloaded, downloaded_size = download_file(
                            ftp_client, 
                            filename, 
                            local_gz_path, 
                            local_txt_path, 
                            mid, 
                            file_size
                        )
                        
                        if is_downloaded:
                            # 2. ãƒ‘ãƒ¼ã‚¹ã¨ä¿å­˜ (tqdmå¯¾å¿œ)
                            success, current_saved_rows = parse_and_process_file(local_txt_path, mid) 
                            
                            # 3. å‡¦ç†æ¸ˆã¿TXTãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                            if os.path.exists(local_txt_path):
                                os.remove(local_txt_path)
                                logger.info(f"ğŸ§¹ [MID: {mid}] å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ« {os.path.basename(local_txt_path)} ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚") 

                        
                    except Exception as e:
                        # å‡¦ç†ä¸­ã®è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã‚’æ•æ‰ã—ã€ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        logger.error(f"\n[MID: {mid}] å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã¯ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚", exc_info=True)
                        logger.error(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}")

                    if success:
                        total_processed_files += 1
                        total_saved_rows += current_saved_rows
                        self.stdout.write(self.style.SUCCESS(f"\n[MID: {mid}] å‡¦ç†å®Œäº†ã€‚DBä¿å­˜ä»¶æ•°: {current_saved_rows:,} ä»¶"))
                    else:
                        self.stdout.write(self.style.ERROR(f"\n[MID: {mid}] å‡¦ç†å¤±æ•— (ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯)ã€‚"))

        finally:
            # FTPæ¥ç¶šã®çµ‚äº†å‡¦ç†
            if ftp_client:
                try:
                    ftp_client.quit()
                    logger.info("\nğŸ“¡ FTPæ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸã€‚")
                except ftplib.all_errors:
                    pass
            
        self.stdout.write(f"\n==================================================================================")
        self.stdout.write(f"--- æœ€çµ‚çµæœ: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚³ãƒãƒ³ãƒ‰å®Œäº† ---")
        self.stdout.write(f"æ­£å¸¸å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_processed_files} / {len(mid_list)} ä»¶")
        self.stdout.write(self.style.SUCCESS(f"åˆè¨ˆä¿å­˜è¡Œæ•°: {total_saved_rows:,} è¡Œ"))
        self.stdout.write("==================================================================================")