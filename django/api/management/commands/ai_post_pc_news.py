# -*- coding: utf-8 -*-
# /home/maya/shin-vps/django/api/management/commands/ai_post_pc_news.py

import os
import re
import requests
import feedparser
import urllib.parse
import time
import difflib
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from requests.auth import HTTPBasicAuth
from api.models import PCProduct

class Command(BaseCommand):
    help = 'ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ç”Ÿæˆã—ã€è¨˜å·é™¤å»ãƒ»ã‚¹ãƒšãƒƒã‚¯è¡¨å®Œå…¨ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ãƒ»è‡ªç¤¾URLæœ€é©åŒ–ãƒ»é‡è¤‡å›é¿æ©Ÿèƒ½ã‚’å‚™ãˆã¦æŠ•ç¨¿ã™ã‚‹'

    def add_arguments(self, parser):
        parser.add_argument('--url', type=str, help='ç‰¹å®šã®è¨˜äº‹URLã‚’ç›´æ¥æŒ‡å®š')
        parser.add_argument('--image', type=str, help='ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒURLã‚’ç›´æ¥æŒ‡å®š')

    def handle(self, *args, **options):
        # --- 1. åŸºæœ¬è¨­å®š ---
        WP_USER = "bicstation"
        WP_APP_PASSWORD = "9re0 t3de WCe1 u1IL MudX 31IY"
        W_DOM = "blog.tiper.live"
        GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        AUTH = HTTPBasicAuth(WP_USER, WP_APP_PASSWORD)
        WP_API_BASE = f"https://{W_DOM}/wp-json/wp/v2"

        current_dir = os.path.dirname(os.path.abspath(__file__))
        MODELS_FILE = os.path.join(current_dir, "ai_models.txt")
        PROMPT_FILE = os.path.join(current_dir, "ai_prompt_news.txt")
        HISTORY_FILE = os.path.join(current_dir, "post_history.txt")

        if not os.path.exists(PROMPT_FILE):
            self.stdout.write(self.style.ERROR(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {PROMPT_FILE}"))
            return

        # å±¥æ­´ã®èª­ã¿è¾¼ã¿ï¼ˆURLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
        posted_links = set()
        posted_titles = []
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, "r", encoding='utf-8') as f:
                for line in f:
                    parts = line.strip().split('\t')
                    if parts:
                        posted_links.add(parts[0].strip())
                        if len(parts) > 1:
                            posted_titles.append(parts[1].strip())

        with open(MODELS_FILE, "r", encoding='utf-8') as f:
            MODELS = [line.strip() for line in f if line.strip()]
        with open(PROMPT_FILE, "r", encoding='utf-8') as f:
            PROMPT_TEMPLATE = f.read()

        # --- 2. è¨˜äº‹å€™è£œã®å–å¾— ---
        target_url = options.get('url')
        target_image_url = options.get('image')
        candidates = []

        if target_url:
            candidates.append({"url": target_url.strip()})
        else:
            RSS_SOURCES = [
                {"name": "PC Watch", "url": "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf"},
                {"name": "ASCII.jp", "url": "https://ascii.jp/pc/rss.xml"},
                {"name": "ITmedia", "url": "https://rss.itmedia.co.jp/rss/2.0/pcuser.xml"}
            ]
            for source in RSS_SOURCES:
                feed = feedparser.parse(source['url'])
                for entry in feed.entries:
                    link = entry.link.strip()
                    if link in posted_links:
                        continue
                    candidates.append({"url": link})

        # --- 3. æŠ•ç¨¿ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— ---
        success = False
        for item in candidates:
            current_url = item['url']
            if current_url in posted_links:
                continue

            self.stdout.write(f"ğŸŒ è§£æé–‹å§‹: {current_url}")
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            try:
                res = requests.get(current_url, timeout=15, headers=headers)
                res.encoding = res.apparent_encoding
                soup = BeautifulSoup(res.text, 'html.parser')
                
                raw_title = soup.title.string.split('|')[0].strip() if soup.title else "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                
                # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ (é‡è¤‡æŠ•ç¨¿é˜²æ­¢)
                if any(difflib.SequenceMatcher(None, raw_title, t).ratio() > 0.8 for t in posted_titles):
                    self.stdout.write(f"â© ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {raw_title}")
                    continue

                # OGPç”»åƒå–å¾—
                og_image_url = None
                og_tag = soup.find("meta", property="og:image")
                if og_tag:
                    og_image_url = og_tag.get("content")

                for s in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'ins']):
                    s.decompose()
                
                main_area = soup.find('article') or soup.find('main') or soup.body
                page_content = main_area.get_text(separator=' ', strip=True) if main_area else ""
                if len(page_content) < 300: continue
            except Exception as e:
                self.stdout.write(f"è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue

            # --- 4. AIè¨˜äº‹ç”Ÿæˆ ---
            self.stdout.write(f"ğŸ¤– AIåŸ·ç­†ä¸­...")
            prompt = PROMPT_TEMPLATE.replace("{raw_title}", raw_title).replace("{page_content[:3500]}", page_content[:3500])

            ai_response = ""
            for model in MODELS:
                api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GEMINI_API_KEY}"
                try:
                    r = requests.post(api_url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=60)
                    if r.status_code == 200:
                        ai_response = r.json()['candidates'][0]['content']['parts'][0]['text']
                        break
                except: continue
            if not ai_response: continue

            # --- 5. æœ¬æ–‡æˆå½¢ã¨HTMLå¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ ---
            lines = ai_response.strip().split('\n')
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¸è¦ãªè¨˜å·ã‚’é™¤å»
            final_title = re.sub(r'^[#*\sãƒ»]+|[#*\sãƒ»]+$', '', lines[0])

            # ç‰¹æ®Šã‚¿ã‚°æŠ½å‡º
            cat_name = "PCãƒ‘ãƒ¼ãƒ„"
            cat_m = re.search(r'\[CAT\]\s*(.*?)\s*\[/CAT\]', ai_response, re.IGNORECASE)
            if cat_m: cat_name = cat_m.group(1).strip()
            
            tag_m = re.search(r'\[TAG\]\s*(.*?)\s*\[/TAG\]', ai_response, re.IGNORECASE)
            tag_names = [t.strip() for t in tag_m.group(1).split(',') if t.strip()] if tag_m else []

            # è¦ç´„æŠ½å‡º
            sum_m = re.search(r'\[SUMMARY\](.*?)\[/SUMMARY\]', ai_response, re.DOTALL | re.IGNORECASE)
            html_body = ""
            if sum_m:
                html_body += '<div style="background:#f1f5f9;border-left:5px solid #0f172a;padding:20px;margin-bottom:30px;border-radius:4px;">'
                html_body += '<h4 style="margin:0 0 10px 0;color:#0f172a;font-size:1.1em;">ğŸ“ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦ç´„ãƒã‚¤ãƒ³ãƒˆ</h4><ul style="margin-bottom:0;padding-left:20px;">'
                for line in sum_m.group(1).strip().split('\n'):
                    p = line.strip().lstrip('*-ãƒ»â€¢ ')
                    if p: html_body += f"<li>{p}</li>"
                html_body += '</ul></div>'
            
            main_text = re.sub(r'\[CAT\].*?\[/CAT\]|\[TAG\].*?\[/TAG\]|\[SUMMARY\].*?\[/SUMMARY\]', '', ai_response, flags=re.DOTALL | re.IGNORECASE)

            in_table = False
            for line in main_text.split('\n'):
                line = line.strip()
                if not line or line == final_title: continue

                # ã€ã‚¹ãƒšãƒƒã‚¯è¡¨ãƒ»ç®‡æ¡æ›¸ãã®æ¤œçŸ¥ã¨ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã€‘
                spec_match = re.match(r'^[*-]\s*(?:\*\*)?(.*?)(?:\*\*)?[:ï¼š]\s*(.*)', line)
                if spec_match:
                    if not in_table:
                        html_body += '<table style="width:100%; border-collapse:collapse; margin:20px 0; border:1px solid #e2e8f0; font-size:0.95em;">'
                        in_table = True
                    key, val = spec_match.groups()
                    html_body += f'<tr style="border-bottom:1px solid #e2e8f0;"><td style="background:#f8fafc; padding:12px; font-weight:bold; width:35%; color:#334155;">{key}</td><td style="padding:12px; color:#1e293b;">{val}</td></tr>'
                    continue
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«çµ‚äº†åˆ¤å®š
                if in_table:
                    html_body += '</table>'
                    in_table = False

                # ã€è¦‹å‡ºã—å‡¦ç†ï¼š# è¨˜å·ã‚’å®Œå…¨ã«é™¤å»ã€‘
                if line.startswith('#'):
                    level = line.count('#')
                    clean_text = line.replace('#', '').strip()
                    if level >= 3:
                        html_body += f'<h3 class="wp-block-heading" style="color:#2563eb;margin-top:30px;font-weight:bold;">{clean_text}</h3>'
                    else:
                        html_body += f'<h2 class="wp-block-heading" style="border-bottom:2px solid #333;padding-bottom:10px;margin-top:40px;font-weight:bold;">{clean_text}</h2>'
                else:
                    html_body += f'<p>{line}</p>'
            
            if in_table: html_body += '</table>'

            # --- 6. å•†å“ã‚«ãƒ¼ãƒ‰ï¼šè‡ªç¤¾URLï¼ˆsite_prefix_unique_idï¼‰ ---
            search_keyword = cat_name if len(cat_name) > 1 else final_title[:10]
            related_products = PCProduct.objects.filter(is_active=True, name__icontains=search_keyword).exclude(stock_status="å—æ³¨åœæ­¢ä¸­").order_by('-created_at')[:3]

            if related_products:
                html_body += '<h2 class="wp-block-heading" style="margin-top:50px;text-align:center;">ğŸ›  é–¢é€£ãŠã™ã™ã‚ãƒ¢ãƒ‡ãƒ«</h2>'
                for prod in related_products:
                    amazon_url = f"https://www.amazon.co.jp/s?k={urllib.parse.quote(prod.name)}"
                    official_url = prod.affiliate_url or prod.url
                    # è‡ªç¤¾URLã®æ­£è¦åŒ–é€£çµ
                    bic_url = f"https://bicstation.com/product/{prod.site_prefix}_{prod.unique_id}/"

                    html_body += f'''
                    <div style="border:1px solid #e2e8f0; border-radius:12px; padding:20px; margin-bottom:30px; background:#fff; box-shadow:0 4px 6px -1px rgba(0,0,0,0.1);">
                        <div style="display:flex; flex-wrap:wrap; align-items:center; gap:20px;">
                            <div style="flex:1; min-width:180px;"><img src="{prod.image_url}" style="width:100%; height:auto; border-radius:8px; object-fit:contain; max-height:200px;"></div>
                            <div style="flex:2; min-width:250px;">
                                <h4 style="margin:0 0 10px 0; color:#1e293b; font-weight:bold;">{prod.name}</h4>
                                <p style="color:#b91c1c; font-weight:bold; font-size:1.4em; margin-bottom:15px;">Â¥{prod.price:,}</p>
                                <div style="display:grid; grid-template-columns: 1fr; gap:10px;">
                                    <a href="{amazon_url}" target="_blank" style="text-align:center; background:#ff9900; color:#fff; padding:10px; text-decoration:none; border-radius:6px; font-weight:bold;">Amazonã§ä¾¡æ ¼ã‚’ç¢ºèª</a>
                                    <a href="{official_url}" target="_blank" style="text-align:center; background:#2563eb; color:#fff; padding:10px; text-decoration:none; border-radius:6px; font-weight:bold;">å…¬å¼ã‚µã‚¤ãƒˆã§è³¼å…¥</a>
                                    <a href="{bic_url}" style="text-align:center; background:#fff; color:#2563eb; border:1px solid #2563eb; padding:10px; text-decoration:none; border-radius:6px; font-weight:bold;">BicStationã§è©³ç´°ã‚’è¦‹ã‚‹</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    '''

            html_body += f'<p style="font-size:0.8em;margin-top:30px;color:#94a3b8;border-top:1px dotted #ccc;padding-top:10px;">å‡ºå…¸: <a href="{current_url}" target="_blank" rel="nofollow">{raw_title}</a></p>'

            # --- 7. ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒç”»åƒã®å‡¦ç† ---
            featured_media_id = 0
            final_img_url = target_image_url or og_image_url or f"https://images.unsplash.com/featured/?{urllib.parse.quote(final_title[:15])}"
            try:
                img_res = requests.get(final_img_url, timeout=20, allow_redirects=True, headers=headers)
                if img_res.status_code == 200:
                    m_headers = {'Content-Disposition': f'attachment; filename="news_{int(time.time())}.jpg"', 'Content-Type': img_res.headers.get('Content-Type', 'image/jpeg')}
                    m_res = requests.post(f"{WP_API_BASE}/media", auth=AUTH, headers=m_headers, data=img_res.content)
                    if m_res.status_code == 201:
                        featured_media_id = m_res.json().get('id', 0)
            except: pass

            # --- 8. WordPressã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¿ã‚°åŒæœŸ ---
            def get_wp_id(path, name):
                try:
                    r = requests.get(f"{WP_API_BASE}/{path}?search={urllib.parse.quote(name)}", auth=AUTH).json()
                    for i in r:
                        if i['name'] == name: return i['id']
                    return requests.post(f"{WP_API_BASE}/{path}", json={"name": name}, auth=AUTH).json().get('id')
                except: return None

            cid = get_wp_id("categories", cat_name)
            tids = [get_wp_id("tags", tn) for tn in tag_names]

            # --- 9. WordPressæŠ•ç¨¿ ---
            post_payload = {
                "title": final_title,
                "content": html_body,
                "status": "publish",
                "categories": [cid] if cid else [],
                "tags": [t for t in tids if t],
                "featured_media": featured_media_id
            }
            
            final_res = requests.post(f"{WP_API_BASE}/posts", json=post_payload, auth=AUTH)
            if final_res.status_code == 201:
                self.stdout.write(self.style.SUCCESS(f"ğŸš€ æŠ•ç¨¿æˆåŠŸ: {final_title}"))
                with open(HISTORY_FILE, "a", encoding='utf-8') as f:
                    f.write(f"{current_url}\t{final_title}\n")
                success = True
                break

        if not success:
            self.stdout.write("æ–°ã—ã„è¨˜äº‹ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")